{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfad6fe",
   "metadata": {},
   "source": [
    "# Anima Anandkumar - Retrospective Role of Tensors in ML\n",
    "\n",
    "## Papers\n",
    "- Tensors\n",
    "  - A. Anandkumar etal, Tensor Decompositions for Learning Latent Variable Models, JLMR 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6ebaa",
   "metadata": {},
   "source": [
    "## Presentation-Summary\n",
    "1. Introduction\n",
    "  - how did the idea came for?\n",
    "1. Probabilistic Graphical Models - 2007/08\n",
    "  - from signal processing\n",
    "  - sensor networks, now called as IoT\n",
    "  - large class of open fields\n",
    "    - how to do learning in high dimensions\n",
    "    - statistically and computationally\n",
    "    - strength of relationships\n",
    "  - interdisciplinary connections\n",
    "  - 2011 - started incorporating hidden variables\n",
    "    - to model such distributions\n",
    "    - some variables are hidden\n",
    "    - image has chair and table\n",
    "      - image is outdoor (hidden variable)\n",
    "    - how to learn hidden variable\n",
    "1. Unsupervised Learning - Topic Models\n",
    "  - wide spread applicability\n",
    "  - text document \n",
    "    - want to extract where did each word come from(topic)\n",
    "    - then calculate the proportion of different topics\n",
    "    - basic clustering cannot be used, as each document can have multiple topics in it\n",
    "      - clustering requires one topic in a document\n",
    "    - proportion of each topic\n",
    "    - simplicity of model comes from exchangability\n",
    "      - order of words dont matter\n",
    "        - first draw topics from proportion vector\n",
    "        - and then draw words from word-topic matrix\n",
    "  - `this is a graphical model and whats hidden are the topics`\n",
    "  - this needs to be learned and is a challenge\n",
    "1. Prior Art\n",
    "  - standard approach to solve the problem \n",
    "    - maximum likelihood (EM or variational inference)\n",
    "    - this is an NP-hard problem\n",
    "    - there is variance like Expectation Maximization\n",
    "      - globally optimal solution not guaranted\n",
    "    - failure in higher dimensions\n",
    "      - cannot be scaled\n",
    "      - neither topics nor word-topic matrix\n",
    "      - not easy to parallelize\n",
    "1. Method of Moments\n",
    "  - Alternative to Maximum Likelihood\n",
    "  - `record the empirical moments of observed variables and then try to fit them`\n",
    "    - first order is the mean\n",
    "    - second order is the pairwise correlation\n",
    "  - first compute and then find a suited distribution\n",
    "  - if you use two orders only \n",
    "    - this is a Gaussian approximation\n",
    "  - higher order moments are required\n",
    "  - for this `Tensors are required`\n",
    "    - `when hidden variables are present`\n",
    "    - `to get good estimate of model, `\n",
    "    - `higher order moments are needed`\n",
    "  - this is even more needed in multivariate higher order moments \n",
    "1. Tensor: Extension of Matrix\n",
    "  - What tensors are\n",
    "  - what is the algebra behind it\n",
    "  - as tensors are generalization of matrices to higher dimensions\n",
    "    - what property of matrix algebra is retained and what is different\n",
    "    - tensors are applied in lot of different areas\n",
    "  - in quantum studies, this is the foundation and in many other areas\n",
    "1. Matrix vs Tensor decomposition\n",
    "  - tensor algebra is very different from matrix algebra\n",
    "  - to decompose tensors into rank 1 components\n",
    "    - can have non-orthogonal components and still have uniqueness\n",
    "  - uniquely identify decomposition\n",
    "    - much less stringent conditions compared to matrix decomposition\n",
    "  - to look at pairwise correlation matrix of topic model\n",
    "    - with orthogonality, you wnat words to appear uniquely \n",
    "      - only then can you guarantee identifiability of the underlying model\n",
    "      - this is not possible\n",
    "    - with tensors, there is a mild condition for uniqueness\n",
    "      - topic vector should be linearly independent\n",
    "      - topics are not representing something redundant in terms of linear independence\n",
    "      - tensors give lot more information than matrices\n",
    "        - because they are modeling higher order moments\n",
    "        - and give deeper relationship between variables\n",
    "1. Tensors for Modeling\n",
    "  - topic detection in text\n",
    "  - co-occurence of word\n",
    "    - look at the average over the entire corpus\n",
    "      - add correction terms and\n",
    "      - decompose that in rank-1 components \n",
    "        - then recover the underlying topic\n",
    "  - tensor decomposition for topic discovery\n",
    "1. Why Not Tensors\n",
    "  - why did others not consider tensors\n",
    "    - they cared about statistical efficiency worse than pairwise correlation\n",
    "    - for small data, tensors are not suited\n",
    "      - loose efficiency\n",
    "    - doing tensor decomposition is NP-hard\n",
    "      - linear algebra has guaranteed solutions\n",
    "1. Why Tensors\n",
    "  - Statistical reasons\n",
    "    - higher order relationships in data can be incorporated\n",
    "    - hidden topics can be found\n",
    "  - Computational reasons\n",
    "    - can be parallelizable like linear algebra\n",
    "    - faster than other algos for LDA\n",
    "    - training and inference can be decomposed\n",
    "    - guaranteed to converge to global optimum under simple assumptions\n",
    "1. Cloud Deployment at AWS\n",
    "  - Mallet - open source topic modeling\n",
    "  - AWS sagemaker platform\n",
    "1. Trinity of AI/ML\n",
    "  - Algorithms, Compute, Data\n",
    "    - earlier focusing on Algorithms only\n",
    "  - in todays world,\n",
    "    - data is multidimensional\n",
    "    - DL uses tensors to calculate moments but in implicit manner\n",
    "    - if models can calculate these moments in explicit manner\n",
    "      - the hidden information is stored there much more efficiently\n",
    "  - compute side\n",
    "    - interesting find\n",
    "1. Tensor Primitives\n",
    "  - Hardware\n",
    "    - BLAS Level 1 - 1969 - Vector-Vector\n",
    "    - BLAS Level 2 - 1972 - Matrix-Vector\n",
    "    - BLAS Level 3 - 1980 - Matrix-Matrix\n",
    "    - BLAS Level 4 - Now? - Tensor-Tensor\n",
    "  - Tensors are less expensive to compute in parallelized manner\n",
    "    - tensoe- much cheaper if you have right primitives\n",
    "    - design these new primitives in hardware\n",
    "      - GPU\n",
    "1. Computation with Tensors - Tensor Contraction Primitive\n",
    "  - extends the notion of matrix product\n",
    "1. New Primitive for Tensor Contractions\n",
    "  - NVidia\n",
    "  - better alternative for tensor contractions\n",
    "    - by doing strides in particular way, performance improves\n",
    "1. Nvidia Cutensor\n",
    "  - high performance cuda-library for tensor primitives\n",
    "    - transpose-free contractions\n",
    "    - arbitrary data layouts\n",
    "    - extensive mixed-precision support\n",
    "      - better primitives for tensor library\n",
    "1. Tensorly: High level API for Tensor algebra\n",
    "  - Jean Kossaifi\n",
    "    - tensor decomposition\n",
    "    - tensor regression\n",
    "    - tensors + deep\n",
    "  - basic tensor operations\n",
    "  - unified backend\n",
    "  - Keras\n",
    "1. Tensors for Models\n",
    "  - Standard CNN use linear algebra\n",
    "- inductive biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37059d02",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
