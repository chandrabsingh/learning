{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f9f3104",
   "metadata": {},
   "source": [
    "# Lecture 1 - Introduction\n",
    "\n",
    "## Topics\n",
    "- Introduction\n",
    "- Examples\n",
    "- Solving Optimization Problems\n",
    "- Least-Squares\n",
    "- Linear Programming\n",
    "- Convex Optimizations\n",
    "- How To Solve? \n",
    "- Course Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753a55d",
   "metadata": {},
   "source": [
    "## Mathematical optimization\n",
    "> minimize $f_{0}(x)$  \n",
    "> subject to $f_{i}(x) \\le b_{i}, \\space \\space i = 1,..,m$  \n",
    "- optimization variables $x = (x_{1},.., x_{n})$\n",
    "- objective function $f_{0}$\n",
    "- constraint functions $f_{i}$\n",
    "- optimal solution - $x^{*}$ has smallest value of $f_{0}$ among all vectors that satisfy the constraints\n",
    "\n",
    "- Examples\n",
    "  - portfolio optimization\n",
    "    - variables - amounts invested in different assets\n",
    "    - constraints - minimum return per asset\n",
    "    - objective - overall risk or return variance\n",
    "  - data fitting\n",
    "    - variables - model parameters\n",
    "    - constraints - prior information, parameter limits\n",
    "    - objective - prediction error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca1f268",
   "metadata": {},
   "source": [
    "## Solving optimization problem\n",
    "- general optimization problem\n",
    "  - generally speaking - global solution is hard to find\n",
    "  - methods involve compromise\n",
    "- type of problems that can be solved\n",
    "  - least square problems\n",
    "  - linear programming problems (most of them)\n",
    "  - convex optimization problems (we can find global solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c302d",
   "metadata": {},
   "source": [
    "## Least-squares\n",
    "> minimize $\\| Ax - b\\|^{2}_{2}$\n",
    "- analytical solution $x^{*} = (A^{T}A)^{-1}A^{T}b$\n",
    "- computation time proportional to $n^{2}k (A \\in R^{kxn})$\n",
    "- n is the number of dimension, which is small\n",
    "- k is the number of rows in A, which is big"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f31d62",
   "metadata": {},
   "source": [
    "## Linear programming\n",
    "\n",
    "> minimize $c^{T}x$  \n",
    "> subject to $a_{i}^{T}x \\le b_{i}, \\space \\space i = 1,..,m$  \n",
    "- computation time proportional to $n^{2}m \\text{( if }m \\ge n)$\n",
    "  - small dimension square times a large dimension\n",
    "- m is the number of constraints \n",
    "- k is the height of A\n",
    "- there are many problems that does not look like linear program but can be transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6ac26",
   "metadata": {},
   "source": [
    "## Convex optimization problem\n",
    "\n",
    "> minimize $f_{0}(x)$  \n",
    "> subject to $f_{i}(x) \\le b_{i}, \\space \\space i = 1,..,m$  \n",
    "- convex means graph should have positive curvature\n",
    "- function $f_{0}(x)$(objective) and $f_{1}(x)$(constraint) have to be convex\n",
    "> $f_{i}(\\alpha x + \\beta y) \\le \\alpha f_{i}(x) + \\beta f_{i}(y) $  \n",
    ">> if $\\alpha + \\beta = 1, \\alpha \\ge 0, \\beta \\ge 0$ \n",
    "  - affine functions make this a equality\n",
    "  - convex functions have non-negative curvature\n",
    "  - affine functions have zero curvature. They are right at the boundary\n",
    "- least squares problem have convex form\n",
    "  - the least square objective plot is a quadratic function\n",
    "  - looks like a bowl\n",
    "  - a slice at a level will be ellipsoid\n",
    "- linear programming is also convex problem\n",
    "  - all of its objective is linear\n",
    "  - linear functions are right on the boundary\n",
    "  - they have zero curvature\n",
    "- one way to say convex is just positive curvature\n",
    "- there are no analytical solutions\n",
    "- but they have efficient algorithms\n",
    "- we get solutions\n",
    "- computation time  (roughly) proportional to max $\\{n^{3}, n^{2}m, F\\}$, where F is cost of evaluating $f_{i}$'s and their first and second derivatives\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/01_objectiveFunction.png\" width=400 height=400>  </p>\n",
    "\n",
    "$\\tiny{\\text{YouTube-Stanford-EE364A-Stephen Boyd}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c865830",
   "metadata": {},
   "source": [
    "- Other form of the convex problem can be:\n",
    "> $f_{i}(\\theta x + (1-\\theta) y) \\le \\theta f_{i}(x) + (1-\\theta) f_{i}(y) $  \n",
    ">> for $\\theta \\in [0,1]$\n",
    "- $\\alpha,\\beta$ problem the function will be called sublinear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed0b43",
   "metadata": {},
   "source": [
    "## Problem\n",
    "- there are bunch of lights illuminating a surface which is piece wise linear\n",
    "- mapping between power and illumination is linear\n",
    "  - the light is incoherent and the power adds up\n",
    "> $I_k = \\sum\\limits_{j=1}^{m}a_{kj}p_{j}$\n",
    "\n",
    "### How to solve\n",
    "1) Use uniform power  \n",
    "2) Use least-squares  \n",
    "3) Use weighted least-squares  \n",
    "4) Use linear programming  \n",
    "5) Use convex optimization  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efebedb3",
   "metadata": {},
   "source": [
    "## Nonlinear optimization/NLP\n",
    "\n",
    "- traditional techniques for general non-convex problems with compromises\n",
    "\n",
    "### Local optimization methods (nonlinear programming)\n",
    "- requires initial guess\n",
    "- finds a point that minimizes among points close by\n",
    "- does not give any information about global optimum\n",
    "\n",
    "### Global optimization methods \n",
    "- worst case complexity grows exponentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5379d023",
   "metadata": {},
   "source": [
    "## Read\n",
    "- NP hard problems\n",
    "- rank two approximation of matrix - is a non-convex problem\n",
    "- problems that have no constrain - unconstrained problems\n",
    "- problems with no objective - feasibility problem or satisfaction problems\n",
    "- combinatorial problems - non-convex problems\n",
    "- Exponential families\n",
    "- Correct parameter to estimate is not covariance. The correct parameter to estimate is actually - Inverse of covariance\n",
    "- The correct parameter to estimate is not mean. Its covariance times inverse the mean\n",
    "- Convex optimization\n",
    "  - theory is called convex analysis\n",
    "  - simplex algorithm for linear programming by Dantzig\n",
    "  - ellipsoid method and other subgradient methods\n",
    "  - polynomial-time interior-point methods for convex optimization - Karmarkar\n",
    "  - large scale convex optimization\n",
    "  - earlier was mostly about operations research\n",
    "  - later came into engineering and finance and signal processing\n",
    "  - into semidefinite and second-order cone programming; robust optimization\n",
    "  - lately into machine learning and statistics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
