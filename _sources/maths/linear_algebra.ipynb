{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b3963e",
   "metadata": {},
   "source": [
    "# Linear Algebra - FAQs\n",
    "\n",
    "## Low rank approximation\n",
    "\n",
    "- data compression in images can be done using matrix approximation\n",
    "- As image is an array of pixels, we can write it as a matrix\n",
    "- find $B \\approx AX$\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/low_rank_appr11.png\" width=400 height=200></p>  \n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n",
    "\n",
    "\n",
    "- column $b_{j}$ can be represented as a linear combination of columns that we picked for A\n",
    "- column $b_{j}$ is approximately sum of some constant $\\chi_{0}a_{0}$, $\\chi_{1}a_{1}$, ..., \n",
    "- we want to pick the best set of $\\chi_{j}$ for this\n",
    "- minimize that over all possible choices of x, which is known as linear least squares problem\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/low_rank_appr2.png\" width=400 height=400></p> \n",
    "    \n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n",
    "\n",
    "\n",
    "- if A has linearly independent columns, then the best such solution is given by $x_{j} = A^{-1}b_{j}$\n",
    "- but because A has more rows than columns, we can't do that\n",
    "- instead we can use **pseudo-inverse**, which is $x_{j} = (A^{T}A)^{-1}A^{T}b_{j}$\n",
    "- this gives us $b_{j} \\approx Ax_{j} = A(A^{T}A)^{-1}A^{T}b_{j} $\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/low_rank_appr3.png\" width=200 height=200> </p> \n",
    "\n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n",
    "\n",
    "\n",
    "- SVD\n",
    "  - say there are optimal columns in this picture which we can choose from \n",
    "  - how to best determine the columns so that we get the best approximation of this image\n",
    "  - SVD allows to find the optimal A and X for this approximation\n",
    "  - it allows us to quantify how many columns are really needed as part of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af121d54",
   "metadata": {},
   "source": [
    "## Orthogonal vectors\n",
    "- Orthogonal vectors have the property $x^{T}y = 0$\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/orthgonalVector1.png\" width=300 height=300>  </p>\n",
    "\n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/orthgonalVector2.png\" width=400 height=400>  </p>\n",
    "\n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52d738",
   "metadata": {},
   "source": [
    "## Components in the direction of vector\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/componentInDirOfVector.png\" width=400 height=400></p>  \n",
    "\n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf715a49",
   "metadata": {},
   "source": [
    "## Orthonormal vectors and matrices\n",
    "\n",
    "- A standard basis are orthonormal vectors\n",
    "  - they are of unit length\n",
    "  - they are mutually orthogonal\n",
    "  \n",
    "- A set ${u_{0}, u_{1}, ..., u_{n-1}}$ in $\\mathbb C^{n}$, is said to be mutually orthonormal if\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/orthonormalVectorMatrix.png\" width=400 height=400></p>  \n",
    "\n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf9d1c",
   "metadata": {},
   "source": [
    "## Unitary matrices\n",
    "\n",
    "- Let $Q \\in \\mathbb R ^{mxn}$ be orthonormal matrix\n",
    "- n < m, because columns of orthonormal matrix are mutually orthogonal therefore they are linearly independent, and so we can atmost have n linearly independent vectors of size m\n",
    "- If n = m, they are called unitary matrices\n",
    "> $Q^{-1} = Q^{T}$ and $QQ^{T} = I$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7955ee",
   "metadata": {},
   "source": [
    "## Unitary matrix - preserve length\n",
    "\n",
    "- If we apply a unitary matrix to a vector, then it preserves length\n",
    "> If $A \\in \\mathbb R^{mxm}$ and unitary, then $\\| Ax \\|_{2} = \\| x \\|_{2}$ for all $\\mathbb R^{m}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daeb9ca",
   "metadata": {},
   "source": [
    "## Rotation preserve length - unitary matrix\n",
    "- Rotation is a linear transformation, because\n",
    "  - scaling a vector first and then rotating will give same results as rotating a vector first and then scaling\n",
    "  - adding two vectors and then rotating will give same results as rotating first and then adding\n",
    "    - so a matrix that represents rotation should be unitary matrix\n",
    "    \n",
    "\n",
    "<p align=\"center\"><img src=\"./images/rotationUnitary.png\" width=400 height=400> </p> \n",
    "\n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fa9da",
   "metadata": {},
   "source": [
    "## Reflection preserve length - unitary matrix\n",
    "- Reflection is a linear transformation, because\n",
    "  - scaling a vector first and then reflecting will give same results as reflecting a vector first and then scaling\n",
    "  - adding two vectors and then reflecting will give same results as reflecting first and then adding\n",
    "    - so a matrix that represents reflection should be unitary matrix\n",
    "    \n",
    "<p align=\"center\"><img src=\"https://www.cs.utexas.edu/users/flame/laff/alaff/images/Chapter02/Mirror6.png\" width=300 height=300></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9165d9fd",
   "metadata": {},
   "source": [
    "## Real numbers vs Complex numbers - transpose representation\n",
    "\n",
    "- In real number form, orthogonal vectors $x,y \\in \\mathbb R^{n}$ is represented as $x^{T}y$, where T - transpose\n",
    "- In complex number form, orthogonal vectors $x,y \\in \\mathbb C^{n}$ is represented as $x^{H}y$, where H - Hermitian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33edff2",
   "metadata": {},
   "source": [
    "## Change of basis using unitary matrices\n",
    "\n",
    "- this is also known as change of orthonormal basis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8c8cc",
   "metadata": {},
   "source": [
    "## Why is it important that a basis be orthonormal?\n",
    "\n",
    "- When a basis is orthonormal, then a vector is merely the sum of its orthogonal projections onto the various members of the basis. That is not true of bases in general.\n",
    "\n",
    "- Often orthogonality represents a form of independence (as in statistics, where it says that there is a lack of covariance, or linear algebra where direct sums of vector spaces have a canonical inner product for which the sum is orthogonal). Orthogonal basis then means the ability to decompose an effect into separate, independent, non-interacting parts that simply add up to form the whole effect. This kind of decomposition is hugely important in situations where it can be done.\n",
    "\n",
    "\n",
    "- Consider a vector:\n",
    "> $ùë£ = ùëéùë•_{1}+ùëèùë•_{2}$  \n",
    "  - where¬†$ùëé,ùëè \\in \\mathbb R$¬†and¬†$ùë•_{1},ùë•_{2}$¬†are basis vectors. \n",
    "  \n",
    "\n",
    "- Let's find the projection of¬†ùë£ onto¬†$x_{1}$\n",
    "\n",
    "1. Non-orthogonal basis: Let¬†$x_{1},x_{2}$¬†be non-orthogonal basis.\n",
    "\n",
    "> $<v,ùë•_{1}>$  \n",
    "> $= <aùë•_{1} ,ùë•_{1}> + <bùë•_{2} ,ùë•_{1}>$  \n",
    "> $= a<ùë•_{1} ,ùë•_{1}> + b<ùë•_{2} ,ùë•_{1}> $  \n",
    "  - Since the basis is non-orthogonal¬†$<ùë•_{2},ùë•_{1}> \\neq 0$. So, to compute the projection of¬†ùë£¬†on¬†$x_{1}$, we have to use two inner products¬†$<ùë•_{1},ùë•_{1}>$ and $<ùë•_{2},ùë•_{1}>$ .\n",
    "\n",
    "2. Orthogonal basis: Let¬†$x_{1},x_{2}$¬†be orthogonal basis.\n",
    "\n",
    "> $<v,ùë•_{1}>$  \n",
    "> $= <aùë•_{1} ,ùë•_{1}> + <bùë•_{2} ,ùë•_{1}>$  \n",
    "> $= a<ùë•_{1} ,ùë•_{1}> $ \n",
    "  - since $ <ùë•_{2} ,ùë•_{1}>=0$¬†by the definition of orthogonality. So, to compute the projection of¬†v on¬†$ùë•_{1}$, we have to use only one inner product¬†$<ùë•_{1},ùë•_{1}>$. Therefore, orthogonal basis allows to compute projections easier than non-orthogonal basis.\n",
    "\n",
    "3. Orthonormal basis: Let¬†$x_{1},x_{2}$¬†be orthogonal basis.\n",
    "  \n",
    "> $<v,ùë•_{1}> $  \n",
    "> $= <aùë•_{1} ,ùë•_{1}> + <bùë•_{2} ,ùë•_{1}>$  \n",
    "> $= a<ùë•_{1} ,ùë•_{1}>$  \n",
    "> $= a $  \n",
    "\n",
    "  - since $<ùë•_{2} ,ùë•_{1}>=0$¬†by the definition of orthogonality and $<ùë•_{1} ,ùë•_{1} \\ge 1$¬†by the definition of normality.  So, we can compute the projection of¬†v on¬†$ùë•_{1}$  instantaneously without inner product:¬†the projections are just coefficients of the corresponding basis components. Since an orthonormal basis doesn't require any computation to find a projection, this is the best basis to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe34af8",
   "metadata": {},
   "source": [
    "## How unitary matrices preserves matrix sensitivity\n",
    "\n",
    "- In conditioning while solving linear system, the relative error in most amplified by the condition number for A into relative error in solution\n",
    "- but this relative error is automatically taken care of if the matrices are unitary\n",
    "\n",
    "<p align=\"center\"><img src=\"images/unitaryMatrixSensitivity.png\" width=300 height=300></p>\n",
    "    \n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e515cb",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "- Here the U and V are unitary matrices\n",
    "- the diagonal matrix of $mxn$ size has all positive diagonal values and are known as singular values. All off-diagonal values are zero\n",
    "- they are always ordered largest to smallest\n",
    "- the rank of matrix is determined by SVD, which is the number of linearly independent columns that matrix A has\n",
    "> $A \\in \\mathbb R ^{mxn}, U \\in \\mathbb R ^{mxm}, V \\in \\mathbb R ^{nxn}, \\Sigma \\in \\mathbb R ^{mxn}, $  \n",
    "> $A = U \\Sigma V^{T}$\n",
    "\n",
    "<p align=\"center\"><img src=\"images/svdTheorem.png\" width=500 height=500></p>\n",
    "\n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61bbd4",
   "metadata": {},
   "source": [
    "## Properties of SVD\n",
    "\n",
    "> $A = U \\Sigma V^{T}$ from SVD\n",
    "- Suppose \n",
    "> $y = Ax$  \n",
    "- Using identity property \n",
    "> $\\Rightarrow UU^{T}y = AVV^{T}x$  \n",
    "> $\\Rightarrow U(U^{T}y) = AV(V^{T}x)$  \n",
    "\n",
    "- $\\hat{y} = (U^{T}y)$ is the vector of coefficients when we view vector y in the orthonormal basis of column of U\n",
    "- $\\hat{x} = (V^{T}x)$ is the vector of coefficients when we view vector x in the orthonormal basis of column of V\n",
    "\n",
    "> $\\Rightarrow U\\hat{y} = AV\\hat{x}$  \n",
    "> $\\Rightarrow \\hat{y} = U^{T}AV\\hat{x}$  \n",
    "> $\\Rightarrow \\hat{y} = \\Sigma\\hat{x}$  \n",
    "\n",
    "- What does this tell us:\n",
    "  - given matrix A, if we view input vector x and output vector y in the correct basis, i.e., use the columns of U as the basis in which we view y and we use the columns of V as the basis in which we view x,\n",
    "  - then the matrix A can be approximated as diagonal matrix $\\Sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21075cb0",
   "metadata": {},
   "source": [
    "## Geometric interpretation of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3c63c",
   "metadata": {},
   "source": [
    "## Reduced SVD\n",
    "\n",
    "> $A = U_{L}\\Sigma_{TL}V_{L}^{H}$ (Reduced SVD)  \n",
    ">> where $A \\in \\mathbb R ^{mxn}, U_{L} \\in \\mathbb R ^{mxr}, V_{L} \\in \\mathbb R ^{nxr}, \\Sigma_{TL} \\in \\mathbb R ^{rxr}, $  \n",
    "\n",
    "> $A = U \\Sigma V^{T}$ (SVD)  \n",
    ">> where $A \\in \\mathbb R ^{mxn}, U \\in \\mathbb R ^{mxm}, V \\in \\mathbb R ^{nxn}, \\Sigma \\in \\mathbb R ^{mxn}, $  \n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/reducedSVD1.png\" width=400 height=400></p>  \n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n",
    "\n",
    "<p align=\"center\"><img src=\"./images/reducedSVD2.png\" width=400 height=400></p>  \n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63fc2af",
   "metadata": {},
   "source": [
    "## SVD of nonsingular matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c0b55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "486c0478",
   "metadata": {},
   "source": [
    "## What is condition number of matrix\n",
    "\n",
    "- A condition number for a matrix measures how sensitive the answer is to perturbations in the input data and to roundoff errors made during the solution process\n",
    "- There will be different condition numbers, when we are solving linear equations, inverting a matrix, finding its eigenvalues, or computing the exponential \n",
    "- A matrix can be poorly conditioned for inversion while the eigenvalue problem is well conditioned\n",
    "- It is given by\n",
    "> $\\kappa(A) = \\|A\\| \\|A^{-1}\\|$\n",
    "- If a matrix is singular, then its condition number is infinite\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/reducedSVD1.png\" width=400 height=400> </p> \n",
    "\n",
    "$\\tiny{\\text{Prof Michael G. Rozman - https://www.phys.uconn.edu/}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42134c95",
   "metadata": {},
   "source": [
    "## What does rank of a matrix tell us?\n",
    "\n",
    "- The rank of a matrix is¬†the maximum number of its linearly independent column vectors (or row vectors). It also can be shown that the columns (rows) of a square matrix are linearly independent only if the matrix is nonsingular. In other words, the rank of any nonsingular matrix of order n is n.\n",
    "\n",
    "- The¬†rank of a matrix¬†is the dimension of the¬†subspace¬†spanned by its rows. The dimension of the column space is equal to the rank. This has important consequences; for instance, if¬†A¬†is an¬†m¬†√ó¬†n¬†matrix and¬†m¬†‚â•¬†n, then rank (A)¬†‚â§¬†n, but if¬†m¬†<¬†n, then rank (A)¬†‚â§¬†m. It follows that if a matrix is not square, either its columns or its rows must be¬†linearly dependent.\n",
    "\n",
    "- For any¬†m¬†√ó¬†n¬†matrix, rank (A)¬†+¬†nullity¬†(A)¬†=¬†n. Thus, if¬†A¬†is¬†n¬†√ó¬†n, then for¬†A¬†to be nonsingular, nullity (A) must be zero.\n",
    "\n",
    "\n",
    "## What is a permutation matrix?\n",
    "\n",
    "- A permutation matrix is the result of repeatedly interchanging the rows and columns of an identity matrix.\n",
    "- Each row of a permutation matrix has one entry equal to¬†and all the other entries equal to¬†0\n",
    "- Each column of a permutation matrix has one entry equal to¬†and all the other entries equal to¬†0\n",
    "- Let¬†P be a¬†permutation matrix. Then, its rows are the¬†standard basis¬†of the space of¬†1 x K vectors, and its columns are the standard basis of the space of¬†vectors K x 1\n",
    "- A permutation matrix is full rank\n",
    "- A permutation matrix is an orthogonal matrix, that is, its transpose is equal to its inverse $P^{-1} = P^{T}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f2b193",
   "metadata": {},
   "source": [
    "## Best rank k-approximation\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/best_rank_kappr1.png\" width=400 height=400></p>  \n",
    "\n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n",
    "\n",
    "<p align=\"center\"><img src=\"./images/best_rank_kappr2.png\" width=400 height=400></p>\n",
    "\n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63922b",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Advanced Linear Algebra: Foundations to Frontiers by Robert van de Geijn, Margaret Myers. Source: url: [http://ulaff.net](http://ulaff.net) notes and YouTube embedded video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco)",
   "language": "python",
   "name": "reco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
