{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Collaborative Filtering (ALS) Deep Dive\n",
    "\n",
    "## Goals\n",
    "\n",
    "Spark MLlib provides a collaborative filtering algorithm that can be used for training a matrix factorization model, which predicts explicit or implicit ratings of users on items for recommendations.\n",
    "\n",
    "This notebook presents a deep dive into the Spark collaborative filtering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Aspects\n",
    "\n",
    "### Matrix factorization algorithm\n",
    "\n",
    "#### Matrix factorization for collaborative filtering problem\n",
    "\n",
    "Matrix factorization is a common technique used in recommendation tasks. Basically, a matrix factorization algorithm tries to find latent factors that represent intrinsic user and item attributes in a lower dimension. That is,\n",
    "\n",
    "$$\\hat r_{u,i} = q_{i}^{T}p_{u}$$\n",
    "\n",
    "where $\\hat r_{u,i}$ is the predicted ratings for user $u$ and item $i$, and $q_{i}^{T}$ and $p_{u}$ are latent factors for item and user, respectively. The challenge to the matrix factorization problem is to find $q_{i}^{T}$ and $p_{u}$. This is achieved by methods such as matrix decomposition. A learning approach is therefore developed to converge the decomposition results close to the observed ratings as much as possible. Furthermore, to avoid overfitting issue, the learning process is regularized. For example, a basic form of such matrix factorization algorithm is represented as below.\n",
    "\n",
    "$$\\min\\sum(r_{u,i} - q_{i}^{T}p_{u})^2 + \\lambda(||q_{i}||^2 + ||p_{u}||^2)$$\n",
    "\n",
    "where $\\lambda$ is a the regularization parameter. \n",
    "\n",
    "In case explict ratings are not available, implicit ratings which are usually derived from users' historical interactions with the items (e.g., clicks, views, purchases, etc.). To account for such implicit ratings, the original matrix factorization algorithm can be formulated as \n",
    "\n",
    "$$\\min\\sum c_{u,i}(p_{u,i} - q_{i}^{T}p_{u})^2 + \\lambda(||q_{i}||^2 + ||p_{u}||^2)$$\n",
    "\n",
    "where $c_{u,i}=1+\\alpha r_{u,i}$ and $p_{u,i}=1$ if $r_{u,i}>0$ and $p_{u,i}=0$ if $r_{u,i}=0$. $r_{u,i}$ is a numerical representation of users' preferences (e.g., number of clicks, etc.). \n",
    "\n",
    "#### Alternating Least Square (ALS)\n",
    "\n",
    "Owing to the term of $q_{i}^{T}p_{u}$ the loss function is non-convex. Gradient descent method can be applied but this will incur expensive computations. An Alternating Least Square (ALS) algorithm was therefore developed to overcome this issue. \n",
    "\n",
    "The basic idea of ALS is to learn one of $q$ and $p$ at a time for optimization while keeping the other as constant. This makes the objective at each iteration convex and solvable. The alternating between $q$ and $p$ stops when there is convergence to the optimal. It is worth noting that this iterative computation can be parallelised and/or distributed, which makes the algorithm desirable for use cases where the dataset is large and thus the user-item rating matrix is super sparse (as is typical in recommendation scenarios). A comprehensive discussion of ALS and its distributed computation can be found [here](http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Mllib implementation\n",
    "\n",
    "The matrix factorization algorithm is available as `ALS` module in [Spark `ml`](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html) for DataFrame or [Spark `mllib`](https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html) for RDD. \n",
    "\n",
    "* The uniqueness of ALS implementation is that it distributes the matrix factorization model training by using \"Alternating Least Square\" method. \n",
    "* In the training method, there are parameters that can be selected to control the model performance.\n",
    "* Both explicit and implicit ratings are supported by Spark ALS model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ALS based MovieLens recommender\n",
    "\n",
    "In the following code, the MovieLens-100K dataset is used to illustrate the ALS algorithm in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This notebook requires a PySpark environment to run properly. Please follow the steps in [SETUP.md](https://github.com/Microsoft/Recommenders/blob/master/SETUP.md#dependencies-setup) to install the PySpark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.8.8 (default, Apr 13 2021, 12:59:45) \n",
      "[Clang 10.0.0 ]\n",
      "Pandas version: 1.4.1\n",
      "PySpark version: 3.2.1\n"
     ]
    }
   ],
   "source": [
    "# set the environment path to find Recommenders\n",
    "import sys\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import FloatType, IntegerType, LongType\n",
    "\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "from recommenders.evaluation.spark_evaluation import SparkRankingEvaluation, SparkRatingEvaluation\n",
    "from recommenders.tuning.parameter_sweep import generate_param_grid\n",
    "from recommenders.datasets.spark_splitters import spark_random_split\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"PySpark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MOVIELENS_DATA_SIZE = \"100k\"\n",
    "\n",
    "COL_USER = \"UserId\"\n",
    "COL_ITEM = \"MovieId\"\n",
    "COL_RATING = \"Rating\"\n",
    "COL_PREDICTION = \"prediction\"\n",
    "COL_TIMESTAMP = \"Timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    (\n",
    "        StructField(COL_USER, IntegerType()),\n",
    "        StructField(COL_ITEM, IntegerType()),\n",
    "        StructField(COL_RATING, FloatType()),\n",
    "        StructField(COL_TIMESTAMP, LongType()),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model hyper parameters - these parameters are selected with reference to the benchmarking results [here](http://mymedialite.net/examples/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK = 10\n",
    "MAX_ITER = 15\n",
    "REG_PARAM = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of recommended items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/04 00:46:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = start_or_get_spark(\"ALS Deep Dive\", memory=\"16g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is read from csv into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4.81k/4.81k [00:01<00:00, 4.01kKB/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfs = movielens.load_spark_df(spark=spark, size=MOVIELENS_DATA_SIZE, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|UserId|MovieId|Rating|Timestamp|\n",
      "+------+-------+------+---------+\n",
      "|   196|    242|   3.0|881250949|\n",
      "|   186|    302|   3.0|891717742|\n",
      "|    22|    377|   1.0|878887116|\n",
      "|   244|     51|   2.0|880606923|\n",
      "|   166|    346|   1.0|886397596|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is then randomly split by 80-20 ratio for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train, dfs_test = spark_random_split(dfs, ratio=0.75, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserId: integer (nullable = true)\n",
      " |-- MovieId: integer (nullable = true)\n",
      " |-- Rating: float (nullable = true)\n",
      " |-- Timestamp: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Rating|\n",
      "+------+\n",
      "|   5.0|\n",
      "|   3.0|\n",
      "|   3.0|\n",
      "|   3.0|\n",
      "|   5.0|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>MovieId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>874965758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>887431973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId  MovieId  Rating  Timestamp\n",
       "0       1        1     5.0  874965758\n",
       "1       1        2     3.0  876893171\n",
       "2       1        4     3.0  876893119\n",
       "3       1        5     3.0  889751712\n",
       "4       1        6     5.0  887431973"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_train[COL_RATING]\n",
    "dfs_train.printSchema()\n",
    "dfs_train.select(COL_RATING).show(5)\n",
    "dfs_train.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a movielens model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that Spark ALS model allows dropping cold users to favor a robust evaluation with the testing data. In case there are cold users, Spark ALS implementation allows users to drop cold users in order to make sure evaluations on the prediction results are sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/04 00:47:10 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/10/04 00:47:10 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/10/04 00:47:11 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "als = ALS(\n",
    "    maxIter=MAX_ITER, \n",
    "    rank=RANK,\n",
    "    regParam=REG_PARAM, \n",
    "    userCol=COL_USER, \n",
    "    itemCol=COL_ITEM, \n",
    "    ratingCol=COL_RATING, \n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "\n",
    "model = als.fit(dfs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction with the model\n",
    "\n",
    "The trained model can be used to predict ratings with a given test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_pred = model.transform(dfs_test).drop(COL_RATING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the prediction results, the model performance can be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrasingh/opt/anaconda3/envs/reco/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score = 0.9683063606464523\n",
      "MAE score = 0.7535616298667033\n",
      "R2 score = 0.2590825012455288\n",
      "Explained variance score = 0.26461688837894515\n"
     ]
    }
   ],
   "source": [
    "evaluations = SparkRatingEvaluation(\n",
    "    dfs_test, \n",
    "    dfs_pred,\n",
    "    col_user=COL_USER,\n",
    "    col_item=COL_ITEM,\n",
    "    col_rating=COL_RATING,\n",
    "    col_prediction=COL_PREDICTION\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"RMSE score = {}\".format(evaluations.rmse()),\n",
    "    \"MAE score = {}\".format(evaluations.mae()),\n",
    "    \"R2 score = {}\".format(evaluations.rsquared()),\n",
    "    \"Explained variance score = {}\".format(evaluations.exp_var()),\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftentimes ranking metrics are also of interest to data scientists. Note usually ranking metrics apply to the scenario of recommending a list of items. In our case, the recommended items should be different from those that have been rated by the users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cross join of all user-item pairs and score them.\n",
    "users = dfs_train.select(COL_USER).distinct()\n",
    "items = dfs_train.select(COL_ITEM).distinct()\n",
    "user_item = users.crossJoin(items)\n",
    "dfs_pred = model.transform(user_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the ambiguous col exception\n",
    "dfs1 = dfs_pred.alias(\"pred\")\n",
    "dfs2 = dfs_train.alias(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 543:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|UserId|MovieId|prediction|\n",
      "+------+-------+----------+\n",
      "|     1|     46|  3.589898|\n",
      "|     1|    255| 2.7095366|\n",
      "|     1|    284|  3.043392|\n",
      "|     1|    285|  4.632741|\n",
      "|     1|    318|  4.467332|\n",
      "|     1|    329| 3.7131171|\n",
      "|     1|    335| 3.3609362|\n",
      "|     1|    353|  2.497454|\n",
      "|     1|    371| 2.7252212|\n",
      "|     1|    372|  4.158202|\n",
      "|     1|    381|  3.781853|\n",
      "|     1|    391| 2.1080215|\n",
      "|     1|    409| 2.6763377|\n",
      "|     1|    413|  3.251512|\n",
      "|     1|    417| 2.5076287|\n",
      "|     1|    440|0.72083133|\n",
      "|     1|    449| 3.2115946|\n",
      "|     1|    463|  4.025364|\n",
      "|     1|    480|  4.459408|\n",
      "|     1|    488| 3.5710163|\n",
      "+------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Remove seen items.\n",
    "# dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "#     dfs_train.alias(\"train\"),\n",
    "#     (dfs_pred[COL_USER] == dfs_train[COL_USER]) & (dfs_pred[COL_ITEM] == dfs_train[COL_ITEM]),\n",
    "#     how='outer'\n",
    "# )\n",
    "dfs_pred_exclude_train = dfs1.join(dfs2,\n",
    "    (dfs1[COL_USER] == dfs2[COL_USER]) & (dfs1[COL_ITEM] == dfs2[COL_ITEM]),\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "dfs_pred_final = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[\"train.Rating\"].isNull()) \\\n",
    "    .select('pred.' + COL_USER, 'pred.' + COL_ITEM, 'pred.' + \"prediction\")\n",
    "\n",
    "dfs_pred_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@k = 0.042629904559915166\n",
      "Recall@k = 0.013265474380586673\n",
      "NDCG@k = 0.03691206373345781\n",
      "Mean average precision = 0.003345435993411571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 654:============================>                           (6 + 6) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluations = SparkRankingEvaluation(\n",
    "    dfs_test, \n",
    "    dfs_pred_final,\n",
    "    col_user=COL_USER,\n",
    "    col_item=COL_ITEM,\n",
    "    col_rating=COL_RATING,\n",
    "    col_prediction=COL_PREDICTION,\n",
    "    k=K\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Precision@k = {}\".format(evaluations.precision_at_k()),\n",
    "    \"Recall@k = {}\".format(evaluations.recall_at_k()),\n",
    "    \"NDCG@k = {}\".format(evaluations.ndcg_at_k()),\n",
    "    \"Mean average precision = {}\".format(evaluations.map_at_k()),\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tune the model\n",
    "\n",
    "Prediction performance of a Spark ALS model is often affected by the parameters\n",
    "\n",
    "|Parameter|Description|Default value|Notes|\n",
    "|-------------|-----------------|------------------|-----------------|\n",
    "|`rank`|Number of latent factors|10|The larger the more intrinsic factors considered in the factorization modeling.|\n",
    "|`regParam`|Regularization parameter|1.0|The value needs to be selected empirically to avoid overfitting.|\n",
    "|`maxIters`|Maximum number of iterations|10|The more iterations the better the model converges to the optimal point.|\n",
    "\n",
    "It is always a good practice to start model building with default parameter values and then sweep the parameter in a range to find the optimal combination of parameters. The following parameter set is used for training ALS models for comparison study purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    \"rank\": [10, 15, 20],\n",
    "    \"regParam\": [0.001, 0.1, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a dictionary for each parameter combination which can then be fed into model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = generate_param_grid(param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train models with parameters specified in the parameter grid. Evaluate the model with, for example, the RMSE metric, and then record the metrics for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rmse_score = []\n",
    "\n",
    "for g in param_grid:\n",
    "    als = ALS(        \n",
    "        userCol=COL_USER, \n",
    "        itemCol=COL_ITEM, \n",
    "        ratingCol=COL_RATING, \n",
    "        coldStartStrategy=\"drop\",\n",
    "        **g\n",
    "    )\n",
    "    \n",
    "    model = als.fit(dfs_train)\n",
    "    \n",
    "    dfs_pred = model.transform(dfs_test).drop(COL_RATING)\n",
    "    \n",
    "    evaluations = SparkRatingEvaluation(\n",
    "        dfs_test, \n",
    "        dfs_pred,\n",
    "        col_user=COL_USER,\n",
    "        col_item=COL_ITEM,\n",
    "        col_rating=COL_RATING,\n",
    "        col_prediction=COL_PREDICTION\n",
    "    )\n",
    "\n",
    "    rmse_score.append(evaluations.rmse())\n",
    "    \n",
    "rmse_score = [float('%.4f' % x) for x in rmse_score]\n",
    "rmse_score_array = np.reshape(rmse_score, (len(param_dict[\"rank\"]), len(param_dict[\"regParam\"]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>reg. parameter</th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.100</th>\n",
       "      <th>1.000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.3260</td>\n",
       "      <td>0.9275</td>\n",
       "      <td>1.3693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.4796</td>\n",
       "      <td>0.9277</td>\n",
       "      <td>1.3693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.5679</td>\n",
       "      <td>0.9277</td>\n",
       "      <td>1.3693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "reg. parameter   0.001   0.100   1.000\n",
       "rank                                  \n",
       "10              1.3260  0.9275  1.3693\n",
       "15              1.4796  0.9277  1.3693\n",
       "20              1.5679  0.9277  1.3693"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_df = pd.DataFrame(data=rmse_score_array, index=pd.Index(param_dict[\"rank\"], name=\"rank\"), \n",
    "                       columns=pd.Index(param_dict[\"regParam\"], name=\"reg. parameter\"))\n",
    "rmse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='reg. parameter', ylabel='rank'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(rmse_df, cbar=False, annot=True, fmt=\".4g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated RMSE scores can be visualized to comparatively study how model performance is affected by different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from this visualization that RMSE first decreases and then increases as rank increases, due to overfitting. When the rank equals 20 and the regularization parameter equals 0.1, the model achieves the lowest RMSE score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top K recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top k for all users (items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrasingh/opt/anaconda3/envs/reco/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dfs_rec = model.recommendForAllUsers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2235:====================================================>(99 + 1) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|UserId|     recommendations|\n",
      "+------+--------------------+\n",
      "|     1|[{1536, 3.8911994...|\n",
      "|     3|[{1536, 3.106384}...|\n",
      "|     6|[{1536, 3.7354307...|\n",
      "|    12|[{1536, 4.4471893...|\n",
      "|    13|[{1536, 3.399742}...|\n",
      "|    16|[{1536, 4.565807}...|\n",
      "|    20|[{1536, 3.3140035...|\n",
      "|    22|[{1536, 3.7392454...|\n",
      "|    26|[{1536, 3.1729918...|\n",
      "|    27|[{1536, 3.4825766...|\n",
      "+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2260:>                                                       (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfs_rec.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top k for a selected set of users (items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = dfs_train.select(als.getUserCol()).distinct().limit(3)\n",
    "\n",
    "dfs_rec_subset = model.recommendForUserSubset(users, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2290:========================================>           (78 + 12) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|UserId|     recommendations|\n",
      "+------+--------------------+\n",
      "|   471|[{1536, 3.3287528...|\n",
      "|   463|[{1536, 3.1125834...|\n",
      "|   148|[{1536, 3.9075737...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2290:==============================================>     (89 + 11) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfs_rec_subset.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run-time considerations for top-k recommendations\n",
    "\n",
    "It is worth noting that usually computing the top-k recommendations for all users is the bottleneck of the whole pipeline (model training and scoring) of an ALS based recommendation system. This is because\n",
    "* Getting the top k from all user-item pairs requires a cross join which is usually very computationally expensive. \n",
    "* Inner products of user-item pairs are calculated individually instead of leveraging matrix block multiplication features which are available in certain contemporary computing acceleration libraries (e.g., BLAS).\n",
    "\n",
    "More details about possible optimizations of the top k recommendations in Spark can be found [here](https://engineeringblog.yelp.com/2018/05/scaling-collaborative-filtering-with-pyspark.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup spark instance\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Yehuda Koren, Robert Bell, and Chris Volinsky, \"Matrix Factorization Techniques for Recommender Systems\n",
    "\", ACM Computer, Vol. 42, Issue 8, pp 30-37, Aug., 2009.\n",
    "2. Yifan Hu, Yehuda Koren, and Chris Volinsky, \"Collaborative Filtering for Implicit Feedback Datasets\n",
    "\", Proc. IEEE ICDM, 2008, Dec, Pisa, Italy.\n",
    "3. Apache Spark. url: https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n",
    "4. Seaborn. url: https://seaborn.pydata.org/\n",
    "5. Scaling collaborative filtering with PySpark. url: https://engineeringblog.yelp.com/2018/05/scaling-collaborative-filtering-with-pyspark.html\n",
    "6. Matrix Completion via Alternating Least Square (ALS). url: http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment\n",
    "- Before starting to learn, working environment needs to be setup on local mac laptop \n",
    "  1. Create virtual environment for setting up kernel for interactive computing using Anaconda Jupyter and not pollute existing environment\n",
    "  1. Setup ability to choose from multiple Java version\n",
    "  1. Setup PySpark  \n",
    "  1. Register environment variables with virtual environment and its associated ipykernel\n",
    "  1. Setup git recommenders locally and build using the local git copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- <div class=\"alert alert-warning\"> Check the active sh - bash or zsh, accordingly update ~/.bash_profile or ~/.zshrc</div>  \n",
    "\n",
    "> `$SHELL --version`  \n",
    "\n",
    "zsh 5.8 (x86_64-apple-darwin21.0)  \n",
    "- Renamed config.json to be able to switch to different venv without authentication hassle\n",
    "> `cd ~/.jupyter/`  \n",
    "> `mv jupyter_notebook_config.json jupyter_notebook_config.json.bkp`  \n",
    "- Create new venv\n",
    "> `python --version`\n",
    "3.8.8  \n",
    "> `conda create --name reco python=3.8.8`   \n",
    "> `conda activate reco`  \n",
    "- As a pre-requisite to installing the dependencies, if using Conda, make sure that Anaconda and the package manager Conda are both up to date\n",
    "> `conda update conda -n root`  \n",
    "> `conda install anaconda` since it is a new venv, otherwise `conda update anaconda`  \n",
    "> `pip install pyspark`  \n",
    "\n",
    "Successfully installed py4j-0.10.9.3 pyspark-3.2.1  \n",
    "- [How to install java on mac](https://stackoverflow.com/questions/24342886/how-to-install-java-8-on-mac)  \n",
    "> `brew tap adoptopenjdk/openjdk`  \n",
    "> `brew install --cask adoptopenjdk8`  \n",
    "- NOTE Spark requires Java version 8 or 11. We support Spark versions 3.0 and 3.1, but versions 2.4+ with Java version 8 may also work.\n",
    "> `brew install --cask adoptopenjdk11`  \n",
    "- If you want to install/manage multiple version then you can use 'jenv':  \n",
    "> `echo 'export PATH=\"$HOME/.jenv/bin:$PATH\"' >> ~/.bash_profile`  \n",
    "> `echo 'eval \"$(jenv init -)\"' >> ~/.bash_profile`  \n",
    "> `source ~/.bash_profile`  \n",
    "\n",
    "- This will change the venv to base. Switch/Activate the environment  \n",
    "> `jenv doctor`  \n",
    "No JAVA_HOME set  \n",
    "- Add the installed java to jenv  \n",
    "> `jenv add /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home`  \n",
    "> `jenv add /Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home`  \n",
    "> `jenv versions`\n",
    "- Configure the java version which you want to use:  \n",
    "> `jenv global 11`  \n",
    "> `jenv versions`  \n",
    "- For v11\n",
    "> `echo 'export JAVA_HOME=$(/usr/libexec/java_home -v11)' >> ~/.bash_profile` \n",
    "- or for v1.8\n",
    "> `echo 'export JAVA_HOME=$(/usr/libexec/java_home -v1.8)' >> ~/.bash_profile` \n",
    "- If `jenv doctor` not set, use  \n",
    "> `echo 'eval \"$(jenv init -)\"' >> ~/.zshrc`   \n",
    "- Test `java --version` or `which java`\n",
    "- Setup PySpark\n",
    "> `RECO_ENV=$(conda env list | grep reco | awk '{print $NF}')`  \n",
    "> `mkdir -p $RECO_ENV/etc/conda/activate.d`  \n",
    "> `mkdir -p $RECO_ENV/etc/conda/deactivate.d`   \n",
    "- Then, create the file `\\$RECO_ENV/etc/conda/activate.d/env_vars.sh` and add:\n",
    "> `#!/bin/sh`  \n",
    "> `RECO_ENV=$(conda env list | grep reco | awk '{print $NF}')`  \n",
    "> `export PYSPARK_PYTHON=$RECO_ENV/bin/python`  \n",
    "> `export PYSPARK_DRIVER_PYTHON=$RECO_ENV/bin/python`  \n",
    "> `unset SPARK_HOME`  \n",
    "- This will export the variables every time we do `conda activate reco`. To unset these variables when we deactivate the environment, create the file `$RECO_ENV/etc/conda/deactivate.d/env_vars.sh` and add:\n",
    "> `#!/bin/sh`  \n",
    "> `unset PYSPARK_PYTHON`  \n",
    "> `unset PYSPARK_DRIVER_PYTHON`  \n",
    "- Register environment as kernel in Jupyter\n",
    "> `python -m ipykernel install --name reco --display-name \"Python (reco)\"`\n",
    "- Clone recommenders repo\n",
    "> `git clone https://github.com/microsoft/recommenders`\n",
    "- Change root directory of recommenders git folder where `setup.py` sits\n",
    "- Install recommenders and additional package setups\n",
    "> `pip install -e .`\n",
    "- To roll back to older java version\n",
    "> `jenv local 1.8`\n",
    "- Reinstate JAVA_HOME environment and relaunch kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "- CF looks for similar users to recommend items \n",
    "- CBF looks for similar contents to recommend items\n",
    "\n",
    "- CF aggregates past behaviour of all users and recommends items to user as per other users likes or dislikes\n",
    "\n",
    "#### Goals of Recommender Systems\n",
    "- Two primary type of models\n",
    "  - Prediction type of problem\n",
    "    - missing (or unobserved) values are predicted using this trained model\n",
    "    - this is also referred as matrix completion problem\n",
    "  - Ranking type of problem\n",
    "    - recommend the top-k items for a particular user\n",
    "    - determine the top-k users to target for a particular item\n",
    "    - this is also referred as top-k recommendation problem\n",
    "  \n",
    "\n",
    "#### Types of collaborative filtering\n",
    "- There are two types of methods that are commonly used\n",
    "  - memory-based methods (neighborhood-based CF algorithms)\n",
    "    - ratings of user-item combinations are predicted on the basis of their neighborhoods\n",
    "      - User-based CF or User-User CF\n",
    "        - determine users who are similar to target user\n",
    "        - recommend ratings for the unobserved ratings of target user\n",
    "        - compute weighted averages of the ratings of this peer group\n",
    "      - Item-based CF or Item-Item CF\n",
    "        - determine set of items most similar to target item\n",
    "        - predict rating for target item based on selected similar items\n",
    "        - weighted average of ratings is computed to predict rating of the item\n",
    "        - Item based similarity measures are the Cosine and the Pearson\n",
    "  - model-based methods (latent factor model)\n",
    "    - assumption is significant portions of the rows and columns of data matrices are highly correlated\n",
    "    - can be well approximated using a low-rank matrix\n",
    "    - other alternative is using expectation-maximization (EM) technique with dimensionality reduction\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*EIBIiW2YiakP1ftxwPF8LA.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS algorithm\n",
    "\n",
    "#### Low-rank approximation\n",
    "- low rank approximation is used for data compression in images \n",
    "- As image is an array of pixels, we can write it as a matrix\n",
    "- find $B \\approx AX$\n",
    "\n",
    "<p align=\"center\"><img src=\"../maths/images/low_rank_appr11.png\" width=400 height=200></p> \n",
    "\n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n",
    "\n",
    "\n",
    "- column $b_{j}$ can be represented as a linear combination of columns that we picked for A\n",
    "- column $b_{j}$ is approximately sum of some constant $\\chi_{0}a_{0}$, $\\chi_{1}a_{1}$, ..., \n",
    "- we want to pick the best set of $\\chi_{j}$ for this\n",
    "- minimize that over all possible choices of x, which is known as linear least squares problem\n",
    "\n",
    "<p align=\"center\"><img src=\"../maths/images/low_rank_appr2.png\" width=400 height=400></p>  \n",
    "\n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n",
    "\n",
    "\n",
    "- if A has linearly independent columns, then the best such solution is given by $x_{j} = A^{-1}b_{j}$\n",
    "- but because A has more rows than columns, we can't do that\n",
    "- instead we can use **pseudo-inverse**, which is $x_{j} = (A^{T}A)^{-1}A^{T}b_{j}$\n",
    "- this gives us $b_{j} \\approx Ax_{j} = A(A^{T}A)^{-1}A^{T}b_{j} $\n",
    "\n",
    "<p align=\"center\"><img src=\"../maths/images/low_rank_appr3.png\" width=200 height=200> </p>\n",
    "\n",
    "$\\tiny{\\text{YouTube - Advanced LAFF - ulaff.net}}$   \n",
    "\n",
    "\n",
    "\n",
    "- To apply low-rank approximation in recommender system, assuming all entries in ratings matrix M are observed\n",
    "- the matrix R is $(m x n)$ of rank $k \\ll \\min{\\{m,n\\}} $  \n",
    "> $M = LR^{T}$  \n",
    "  - where L is $m x k$ matrix and R is $n x k$ matrix, i.e., both L and R are rank-k factors\n",
    "- using one of below factorization methods we can approximately express the product of rank-k factors as \n",
    "> $\\hat{M} \\approx L_{k}R_{k}^{T}$\n",
    "- The error of this approximation represents the noise in the underlying ratings matrix that cannot be modeled\n",
    "> $\\| (M-LR^{T})\\|^{2}$\n",
    "\n",
    "<p align=\"center\"><img src=\"https://dustinstansbury.github.io/theclevermachine/assets/images/svd-data-compression/low-rank-approximation.png\" width=300 height=300></p>  \n",
    "\n",
    "$\\tiny{\\text{Low rank matrix decomposition - https://dustinstansbury.github.io/}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unconstrained Matrix Factorization\n",
    "- How can one determine factors U and V, so that the specified matrix R matches closely with $UV^{T}$  \n",
    "$$  R = UV^{T}  $$  \n",
    "- This can be formulated as an optimization problem wrt matrices U and V  \n",
    "> Minimize $J = \\frac{1}{2}\\| R - UV^{T}\\|^{2}$  \n",
    "  - subject to no constraints on U and V. \n",
    "- The smaller the objective function is, better will be the quality of factorization\n",
    "- This is a quadratic loss function\n",
    "- This can be achieved by any of gradient descent methods\n",
    "- However, in this matrix there are missing entries, so the objective function is not well defined \n",
    "- Let the set of user-item pair (i,j) which are observed in R, be denoted by S\n",
    "$$  S = \\{(i,j) : r_{ij}\\text{ is observed}\\}  $$\n",
    "- The predicted matrix R is \n",
    "$$  \\hat{r}_{ij} = \\sum\\limits_{s=1}^{k}u_{is}v_{js}  $$\n",
    "- The difference between observed and predicted entries will be\n",
    "$$  e_{ij} = (r_{ij} - \\hat{r}_{ij}) = (r_{ij} - \\sum_{s=1}^{k}u_{is}v_{js})  $$\n",
    "- The modified objective function that works over observed entries in S only is\n",
    "> Minimize $J = \\frac{1}{2}\\sum\\limits_{(i,j \\in S)}e_{ij}^{2} = \\frac{1}{2}\\sum\\limits_{(i,j \\in S)}(r_{ij} - \\sum\\limits_{s=1}^{k}u_{is}v_{js})^{2} $\n",
    "- This can be achieved simply with gradient descent methods\n",
    "\n",
    "#### Regularization\n",
    "- Regularization reduces the tendency of model to overfit at the expense of introducing a bias in the model\n",
    "- It discourages large values of the coefficients in U and V in order to encourage stability. \n",
    "- So, a regularization term $\\frac{\\lambda}{2}(\\|U\\|^{2} + \\|U\\|^{2})$ is added to the objective function, where λ > 0 is the regularization parameter\n",
    "> Minimize $\\begin{equation}\\\\\n",
    "\\begin{aligned}\\\\\n",
    "J &= \\frac{1}{2}\\sum\\limits_{(i,j \\in S )}e_{ij}^{2} + \\frac{\\lambda}{2}\\sum\\limits_{i=1}^{m} \\sum\\limits_{s=1}^{k} u_{is}^{2} + \\frac{\\lambda}{2}\\sum\\limits_{j=1}^{n} \\sum\\limits_{s=1}^{k} v_{js}^{2} \\\\  \n",
    " &= \\frac{1}{2}\\sum\\limits_{(i,j \\in S)} \\left(r_{ij} - \\sum\\limits_{s=1}^{k}u_{is}v_{js} \\right)^{2} + \\frac{\\lambda}{2}\\sum\\limits_{i=1}^{m} \\sum\\limits_{s=1}^{k} u_{is}^{2} + \\frac{\\lambda}{2}\\sum\\limits_{j=1}^{n} \\sum\\limits_{s=1}^{k} v_{js}^{2} \\\\\n",
    " \\end{aligned}\\\\\n",
    " \\end{equation} $\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implicit Feedback\n",
    "- Implicit feedback data sets such as web click-streams can help in recommendations about future activity\n",
    "- In explicit feedback matrices, ratings correspond to (highly discriminated) preferences, whereas in implicit feedback matrices, ratings correspond to (less discriminated) confidences\n",
    "- Implicit feedbacks are one that are inferred rather than direct as in explicit feedback matrices\n",
    "- Such feedbacks are easier to obtain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other factorization algorithms\n",
    "The surprise library provides good collection of recommender system matrix factorization algorithms like SVD, SVD++, Non-negative Matrix Factorization(NMF), Probabilistic Matrix Factorization(PMF)\n",
    "\n",
    "#### SVD\n",
    "The method usually referred to as \"SVD\" used in the context of recommendations is not strictly the mathematical Singular Value Decomposition of a matrix but rather an approximate way to compute the [low-rank approximation](#Low-rank-approximation) of the matrix by minimizing the squared error loss. A more accurate, albeit more generic, way to call this would be Matrix Factorization.  The initial version of this approach in the context of the Netflix Prize was presented by Simon Funk in his famous [Try This at Home](https://sifter.org/~simon/journal/20061211.html) blogpost\n",
    "\n",
    "#### SVD++\n",
    "\n",
    "#### NMF\n",
    "\n",
    "#### PMF\n",
    "\n",
    "### Sparsity \n",
    "\n",
    "### Distributed ALS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark \n",
    "\n",
    "#### Spark internals \n",
    "\n",
    "#### Spark Application\n",
    "- At the core of every Spark application is the SparkDriver program\n",
    "    - this creates SparkSession object\n",
    "      - this provides functionality to interact with Spark API\n",
    "    - the SparkDriver converts your Spark application into one or more Spark jobs. \n",
    "      - It transforms each job into a DAG. This is Spark’s execution plan where each node within a DAG could be a single or multiple Spark stages\n",
    "    - A Spark user program consists of driver program and executors on cluster\n",
    "- Job\n",
    "  - A parallel computation gets spawned for Spark action (save(), collect())\n",
    "- Stage\n",
    "  - Each job gets divided into smaller sets of tasks called stages that depend on each other\n",
    "- Task\n",
    "  - it is a single unit of work or execution sent to Spark executor\n",
    "\n",
    "#### Transformation, Actions and Lazy Evaluation\n",
    "  - Transformations transform a Spark DataFrame into a new DataFrame without altering the original data(immutable datastores)\n",
    "  - Operation such as select() or filter() will not change the original DataFrame; instead, will return the transformed results of the operation as a new DataFrame\n",
    "  - All transformations are evaluated lazily, they are recorded and remembered as a lineage \n",
    "  - A recorded lineage allows Spark, at a later time in its execution plan, to rearrange certain transformations, coalesce them, or optimize transformations into stages for more efficient execution. Lazy evaluation is Spark’s strategy for delaying execution until an action is invoked or data is touched\n",
    "  - Nothing in a query plan is executed until an action is invoked \n",
    "\n",
    "\n",
    "#### Narrow transformation\n",
    "  - filter() and contains() represent narrow transformations because they can operate on a single partition and produce the resulting output partition without any exchange of data.\n",
    "  \n",
    "#### Wide transformation\n",
    "  - data from other partitions are read in, combined, and written to disk. groupBy() will shuffle data across executor's partitions across cluster and then orderBy() will require output from other partitions to compute the final aggregation\n",
    "  \n",
    "  \n",
    "#### Spark UI\n",
    "  - GUI for monitoring the various jobs, stages and tasks, information on its execution detail\n",
    "  \n",
    "\n",
    "#### Resilient Distributed Dataset(RDD)\n",
    "  - The main abstraction Spark provides is RDD, it is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel\n",
    "  - Spark can persist an RDD in memory allowing it to be reused efficiently across parallel operations\n",
    "  - RDDs automatically recover from node failures\n",
    "  - two ways to create RDDs: \n",
    "    - parallelizing an existing collection in the driver program \n",
    "    - referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat\n",
    "  - Another abstraction Spark provides is it has shared variables that can be used in parallel operations\n",
    "    - two types of shared variables: \n",
    "      - broadcast variables, which can be used to cache a value in memory on all nodes\n",
    "      - accumulators, which are variables that are only “added” to, such as counters and sums\n",
    "\n",
    "\n",
    "#### Project Tungsten\n",
    "  - focuses on substantially improving the efficiency of memory and CPU for Spark applications, to push performance closer to the limits of modern hardware\n",
    "  - eliminates overhead of JVM object model and garbage collection\n",
    "\n",
    "\n",
    "#### How to launch Spark\n",
    "- four way of using Spark interpreters: \n",
    "  - pyspark api\n",
    "  - spark-shell\n",
    "  - spark-sql\n",
    "  - sparkR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### PySpark \n",
    "- Apache Spark is written in Scala and is integrated with Python using PySpark. It provides interface with RDD using Py4j library\n",
    "\n",
    "#### PySparkSQL\n",
    "- PySparkSQL is a PySpark library to apply SQL-like analysis on a huge amount of structured or semi-structured data\n",
    "  - this can also connect to Apache Hive \n",
    "\n",
    "#### MLLib\n",
    "- MLlib is a wrapper over the PySpark and it is Spark’s machine learning (ML) library. This library uses the data parallelism technique to store and work with data. MLlib supports many machine-learning algorithms for classification, regression, clustering, collaborative filtering, dimensionality reduction, and underlying optimization primitives.\n",
    "\n",
    "### Spark Joins\n",
    "\n",
    "ToRead: https://towardsdatascience.com/strategies-of-spark-join-c0e7b4572bcf\n",
    "\n",
    "#### Broadcast Hash Join\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/282/1*pd-YlCGD9v3W4Lk1tfN72Q.jpeg\"></p>\n",
    "\n",
    "$\\tiny{\\text{towardsdatascience.com - Jyoti Dhiman - Hash Join}}$   \n",
    "\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/1400/1*pO_40cT0UhaiSP0fdT-sWw.jpeg\"></p>\n",
    "\n",
    "$\\tiny{\\text{towardsdatascience.com - Jyoti Dhiman - Broadcast Hash Join}}$  \n",
    "\n",
    "#### Shuffle hash join\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/1400/1*Yjw7V8mh7FipB09ngnBn6A.jpeg\"></p>\n",
    "\n",
    "#### Shuffle sort-merge join\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/1282/1*03nmwDCmVaSDWVHMZTcFjA.jpeg\"></p>\n",
    "\n",
    "$\\tiny{\\text{towardsdatascience.com - Jyoti Dhiman - Sort Merge Join}}$  \n",
    "\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/1400/1*6RA2wnol2zhH4Ps2UNJoxQ.jpeg\"></p>\n",
    "\n",
    "$\\tiny{\\text{towardsdatascience.com - Jyoti Dhiman - Shuffle sort-merge join}}$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.8.8 (default, Apr 13 2021, 12:59:45) \n",
      "[Clang 10.0.0 ]\n",
      "Pandas version: 1.4.1\n",
      "PySpark version: 3.2.1\n"
     ]
    }
   ],
   "source": [
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"PySpark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# help(pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    \"rank\": [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    \"regParam\": [0.001, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "def testModelPerformance(param_dict):\n",
    "    param_grid = generate_param_grid(param_dict)\n",
    "\n",
    "    rmse_score = []\n",
    "\n",
    "    for g in param_grid:\n",
    "        als = ALS(        \n",
    "            userCol=COL_USER, \n",
    "            itemCol=COL_ITEM, \n",
    "            ratingCol=COL_RATING, \n",
    "            coldStartStrategy=\"drop\",\n",
    "            **g\n",
    "        )\n",
    "\n",
    "        model = als.fit(dfs_train)\n",
    "\n",
    "        dfs_pred = model.transform(dfs_test).drop(COL_RATING)\n",
    "\n",
    "        evaluations = SparkRatingEvaluation(\n",
    "            dfs_test, \n",
    "            dfs_pred,\n",
    "            col_user=COL_USER,\n",
    "            col_item=COL_ITEM,\n",
    "            col_rating=COL_RATING,\n",
    "            col_prediction=COL_PREDICTION\n",
    "        )\n",
    "\n",
    "        rmse_score.append(evaluations.rmse())\n",
    "\n",
    "    rmse_score = [float('%.4f' % x) for x in rmse_score]\n",
    "    mae_score.append(evaluations.mae())\n",
    "    rsquared_score.append(evaluations.rsquared())\n",
    "    exp_var_score.append(evaluations.exp_var())\n",
    "    \n",
    "    rmse_score_array = np.reshape(rmse_score, (len(param_dict[\"rank\"]), len(param_dict[\"regParam\"]))) \n",
    "    mae_score_array = np.reshape(mae_score, (len(param_dict[\"rank\"]), len(param_dict[\"regParam\"]))) \n",
    "    rsquared_score_array = np.reshape(rsquared_score, (len(param_dict[\"rank\"]), len(param_dict[\"regParam\"]))) \n",
    "    exp_var_score_array = np.reshape(exp_var_score, (len(param_dict[\"rank\"]), len(param_dict[\"regParam\"]))) \n",
    "\n",
    "    rmse_df = pd.DataFrame(data=rmse_score_array, index=pd.Index(param_dict[\"rank\"], name=\"rank\"), \n",
    "                           columns=pd.Index(param_dict[\"regParam\"], name=\"reg. parameter\"))\n",
    "    return rmse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='reg. parameter', ylabel='rank'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(rmse_df, fmt=\".4g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(dir(pyspark))\n",
    "# help(SparkSession)\n",
    "# help(CrossValidator)\n",
    "# help(StructType)\n",
    "# help(IntegerType)\n",
    "# help(movielens)\n",
    "# help(start_or_get_spark)\n",
    "# help(SparkRankingEvaluation)\n",
    "# help(SparkRatingEvaluation)\n",
    "# help(ALS)\n",
    "\n",
    "# help(model.transform)\n",
    "# dir(pyspark.ml.recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. Recommender Systems by Charu C. Aggarwal\n",
    "1. Learning Spark 2.0 by Jules S. Damji, Brooke Wenig, Tathagata Das & Denny Lee\n",
    "1. Collaborative Filtering for Implicit Feedback Datasets, by Yifan Hu, Yehuda Koren, Chris Volinsky\n",
    "1. [Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model by Yehuda Koren](https://github.com/gpfvic/IRR/blob/master/Factorization%20meets%20the%20neighborhood-%20a%20multifaceted%20collaborative%20filtering%20model.pdf)\n",
    "1. [Spark Programming Guide @ spark.apache.org](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "1. [Spark Join Strategies @ towardsdatascience.com](https://towardsdatascience.com/strategies-of-spark-join-c0e7b4572bcf)\n",
    "1. [Collaborative Filtering @ towardsdatascience.com](https://towardsdatascience.com/build-recommendation-system-with-pyspark-using-alternating-least-squares-als-matrix-factorisation-ebe1ad2e7679)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ec2189bea0434770dca7423a25e631e1cca9c4e2b4ff137a82f4dff32ac9607"
  },
  "kernelspec": {
   "display_name": "Python (reco)",
   "language": "python",
   "name": "reco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
