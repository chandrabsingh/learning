{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d555dbf",
   "metadata": {},
   "source": [
    "# Welcome to ML glossary\n",
    "\n",
    "![ML Mind Map](./images/ml_mind_map.png)\n",
    "\n",
    "$\\tiny{\\text{Jason Brownlee}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f603013",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "\n",
    "### Supervised Learning\n",
    "Supervised Learning is a machine learning approach that uses labeled data to train algorithms into classifying or predicting outcomes accurately.\n",
    "> - Types of Supervised Learning algorithms\n",
    "\n",
    "### Unsupervised Learning\n",
    "Unsupervised Learning is a machine learning approach that uses unlabeled data to analyze and cluster datasets. It makes inference without knowing the label or outcomes. \n",
    "> - Types of Unsupervised Learning algorithms\n",
    ">   - Clustering\n",
    ">   - Association\n",
    "\n",
    "### Reinforcement Learning\n",
    "Reinforcement learning is a machine learning training method based on rewarding desired behaviors and punishing the undesirable ones, thereby learning about the environment by trial and error\n",
    "> - Types of Reinforcement Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd462e06",
   "metadata": {},
   "source": [
    "## Classification algorithms\n",
    "Classification algorithms use input training data to predict the likelihood that subsequent data will fall into one of predetermined categories\n",
    "> - Types of Classification algorithms \n",
    "\n",
    "### Probabilistic \n",
    "Probabilistic classification models classify, given an observation of an input, a probability distribution over a set of classes \n",
    "> - Types of Probabilistic Classification algorithms\n",
    ">   - Naive Bayes\n",
    ">   - Logistic regression \n",
    ">   - Multilayer perceptrons \n",
    "\n",
    "<img src = \"https://www.ismiletechnologies.com/wp-content/uploads/2021/10/image-15.png\">\n",
    "$\\tiny{\\text{www.ismiletechnologies.com}}$   \n",
    "\n",
    "#### Naive Bayes\n",
    "Bayesian classification helps us find the probability of a label given some observed features, using Bayes theorem, which describes the relationship of conditional probabilities of statistical quantities\n",
    "> - READ: https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html\n",
    "> - Types of Bayesian Classification\n",
    ">   - Multinomial Naïve Bayes Classifier\n",
    ">   - Bernoulli Naïve Bayes Classifier\n",
    ">   - Gaussian Naïve Bayes Classifier\n",
    "\n",
    "\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/09/Bayes_rule-300x172-300x172.png\">\n",
    "$\\tiny{\\text{www.analyticsvidhya.com}}$   \n",
    "\n",
    "### Rule based\n",
    "Rule based classification helps in classifying datasets by using a collection of \"if.. else..\" rules. The classifier may contain mutually exclusive rules, exhaustive rules, not mutually exclusive rules, or not exhaustive rules\n",
    "> - READ: http://jcsites.juniata.edu/faculty/rhodes/ml/rulebasedClass.htm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae151c",
   "metadata": {},
   "source": [
    "\n",
    "#### Decision Tree\n",
    "A decision tree is a data mining/machine learning method of predicting/classifying the value of a target variable based on several input variables. In this classification tree, each internal node is labeled with an input feature and each leaf of the tree is labeled with a class or a probability distribution over the classes of either class or into a particular probability distribution\n",
    "\n",
    "> - Decision Tree Types\n",
    ">   - CART - Classification and Regression Trees\n",
    ">   - Ensemble methods, construct more than one decision tree\n",
    ">     - Boosted trees \n",
    ">     - Bootstrap aggregated (or bagged/bagging) decision trees\n",
    "\n",
    "- When a decision tree  \n",
    "  - classifies things into categories\n",
    "    - it's called Classification Tree\n",
    "  - predicts numerical values\n",
    "    - it's called Regression Tree\n",
    "    \n",
    "\n",
    "- In classification tree, features of different datatypes can be mixed together\n",
    "  - exercise < 20 minutes \n",
    "    - classifies as True/False\n",
    "  - eat doughnuts \n",
    "    - classifies as True/False\n",
    "    \n",
    "\n",
    "- numerical thresholds can be different for the same data\n",
    "  - For example\n",
    "    - 40 years or older\n",
    "      - True\n",
    "        - exercise < 20 minutes \n",
    "          - classifies further as True/False\n",
    "      - False\n",
    "        - exercise > 30 minutes \n",
    "          - classifies further as True/False\n",
    "          \n",
    "          \n",
    "- how to choose optimal k\n",
    "  - pick the columns which are least impure\n",
    "  \n",
    "\n",
    "- how to measure quality of split\n",
    "  - based on the lowest impurity factor\n",
    "  - the feature with lowest impurity takes the root position and same iterative process is repeated\n",
    "  \n",
    "> - |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
    "> - |      The function to measure the quality of a split. Supported criteria are\n",
    "> - |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*ZkQXt7mqI7MXuXhHrfvgtQ.png\" width=400 height=400> \n",
    "$\\tiny{\\text{miro.medium.com}}$   \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/25/Cart_tree_kyphosis.png\">\n",
    "$\\tiny{\\text{Wikipedia}}$   \n",
    "\n",
    "\n",
    "#### Gini Impurity\n",
    "- Gini Impurity is the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset. \n",
    "  - If there are $C$ total classes and $p(i)$ is the probability of picking a datapoint with class i, then the Gini Impurity is calculated as\n",
    " \n",
    "$$I_G(p) = \\sum\\limits_{i=1}^{J}\\left(p_i\\sum_{k \\ne i} p_k \\right) = \\sum\\limits_{i=1}^{J}p_i\\left(1 - p_i \\right) = \\sum\\limits_{i=1}^{J}\\left(p_i - p^2_i \\right) = \\sum\\limits_{i=1}^{J}p_i - \\sum\\limits_{i=1}^{J}p^2_i = 1 - \\sum\\limits_{i=1}^{J}p^2_i$$\n",
    "\n",
    "#### Entropy\n",
    "- Entropy is a measure of state of disorder, randomness, or uncertainty. It is used to represent the uncertainty associated with data\n",
    "- If all the data points belong to a single class, then there is no real uncertainty, and will have a low entropy\n",
    "- If all the data points are evenly distributed across the classes, there is lot of uncertainty, and will have a high entropy\n",
    "\n",
    "$$H(Q_m) = -\\sum\\limits_{k}p_{mk}\\log(p_{mk})$$\n",
    "\n",
    "\n",
    "#### Information Gain\n",
    "- measures reduction in entropy or surprise from an additional piece of information\n",
    "- greater the reduction in uncertainty, more is the information gained\n",
    "$$IG(Y,X) = E(Y) - E(Y|X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513e678",
   "metadata": {},
   "source": [
    "## Most common\n",
    "\n",
    "### Linear Regression\n",
    "Linear regression helps us model the relationship between two variables by fitting a linear equation to observed data. The most common method for fitting a regression line is the method of least-squares. This method calculates the best-fitting line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line (if a point lies on the fitted line exactly, then its vertical deviation is 0). Because the deviations are first squared, then summed, there are no cancellations between positive and negative values.\n",
    "\n",
    "We assume here that $y|x; \\theta \\sim \\mathbb N(\\mu, \\sigma^{2})$\n",
    "\n",
    "The closed form solution for the $\\theta$ that minimizes the cost function is\n",
    "> $$\\theta = (X^{T}X)^{-1}X^{T}y$$\n",
    "\n",
    "### Logistic Regression\n",
    "Logistic regression models generate probabilities.\n",
    "\n",
    "The logistic regression is used to model the relationship between a set of independent and dependent variables. The dependent variables are categorical in nature, which is predicted based on the probabilities given some characteristics of class variables. The logistic regression uses sigmoid function to assign class labels.\n",
    "\n",
    "The logit function is defined as the logarithm of the log odds\n",
    "$$ z = \\text{logit(p)} = \\ln\\frac{p}{1-p}$$\n",
    "\n",
    "$z$ is also referred to as the log-odds because the inverse of the sigmoid states that $z$ can be defined as the log of the probability of the $1$ label (e.g., \"dog bark\") divided by the probability of the $0$ label(e.g., \"dog does not bark\") \n",
    "\n",
    "A standard logistic sigmoid function is defined as the \n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "The linear part of the model predicts the log-odds of the dataset example in the form of probability using logistic sigmoid function.\n",
    "\n",
    "It tries to learn a function that approximates P(Y|X), by assuming that P(Y|X) can be approximated as a sigmoid\n",
    "function when applied to the linear combination of input features.  \n",
    "$$ P(Y = 1|X = x) = \\sigma(z) = \\sigma(\\theta^{T}x) $$,\n",
    "where $z = \\theta_{0} + \\sum\\limits_{i=1}^{m}\\theta_{i}x_{i} $\n",
    "\n",
    "Similarly,\n",
    "$$ P(Y = 0|X = x) = 1 - \\sigma(\\theta^{T}x) $$\n",
    "\n",
    "The gradient descent is calculated as the partial derivative of logistic cost function wrt weight, which is used to maximize the logistic cost function.\n",
    "\n",
    "The loss function for logistic regression is Log Loss defined as\n",
    "\n",
    "<img src=\"./images/logisticRegressionCostFun.png\" width=500 height=500>\n",
    "<img src=\"./images/logisticRegressionCostFun2.png\" width=500 height=500>\n",
    "\n",
    "$\\tiny{\\text{towardsdatascience.com - Shuyu Luo}}$\n",
    "\n",
    "### Decision Tree\n",
    "<a href=\"#Decision-Tree\">Link (above)</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e858d",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "#### Links\n",
    "- [StackOverflow - How does SVM work](https://stats.stackexchange.com/questions/23391/how-does-a-support-vector-machine-svm-work)\n",
    "-[SVM Explained - Tristan Fletcher](https://static1.squarespace.com/static/58851af9ebbd1a30e98fb283/t/58902fbae4fcb5398aeb7505/1485844411772/SVM+Explained.pdf)\n",
    "- [Hard vs Soft margin](https://www.baeldung.com/cs/svm-hard-margin-vs-soft-margin)\n",
    "- [Kernel trick](https://datamites.com/blog/support-vector-machine-algorithm-svm-understanding-kernel-trick/#:~:text=A%20Kernel%20Trick%20is%20a,Lagrangian%20formula%20using%20Lagrangian%20multipliers.%20()\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "SVM should maximize the distance between the two decision boundaries. Mathematically, this means we want to __*maximize the distance between the hyperplane*__ defined by $w^{T}x+b=−1$ and the hyperplane defined by $w^{T}x+b = 1$. This distance is equal to $\\frac{2}{\\|w\\|}$. This means we want to solve $\\max\\limits_{w}\\frac{2}{\\|w\\|}$. Equivalently we want $\\min\\limits_{w}\\frac{\\|w\\|}{2}$\n",
    "\n",
    "SVM should also classify all $x^{(i)}$, which means $y^{(i)}(𝐰^{T}x^{(i)}+b \\ge 1)$, which gives us the __*quadratic optimization problem*__\n",
    "  > $$\\min\\limits_{w,b}\\frac{\\|w\\|^{2}}{2}$$  \n",
    "  > s.t., $y^{(i)}(𝐰^{T}x^{(i)}+b \\ge 1)$ for all $i \\in \\{1,..N\\}$   \n",
    "  \n",
    "the above is the __*hard margin SVM*__, which is possible iff data is __*linearly separable*__\n",
    "\n",
    "This optimization is called the __*primal problem*__ and is guaranteed to have a global minimum. We can solve this by introducing __*Lagrange multipliers*__ and converting it to the __*dual problem*__\n",
    "\n",
    "If we allow misclassifications to happen, we call it __*soft margin SVM*__ optimization problem. The loss function that we minimize is the hinge loss\n",
    "  > $\\max\\{0, 1-y^{(i)}(𝐰^{T}x^{(i)}+b \\ge 1)\\}$ \n",
    "  \n",
    "The loss of a misclassified point is called a *__slack variable__* and is added to the primal problem of hard margin SVM. So the primal problem for soft margin becomes\n",
    "  > $$\\min\\limits_{w,b}\\frac{\\|w\\|^{2}}{2} + C\\sum\\limits_{i=0}^{n}\\zeta^{(i)}$$  \n",
    "  > s.t., $y^{(i)}(𝐰^{T}x^{(i)}+b \\ge 1 - \\zeta^{(i)})$ for all $i \\in \\{1,..N\\}$   \n",
    "  > and $\\zeta^{(i)} \\gt 0$ for all $i \\in \\{1,..N\\}$   \n",
    "\n",
    "  \n",
    "####   Questionnaires\n",
    "- What are the goals of SVM?\n",
    "  - maximizes the margin around separating hyperplane\n",
    "  - classify the class labels as {-1,1}\n",
    "- What does linearly separable means?\n",
    "  - Linearly separable data means if graphed in two dimensions, it can be separated by straight line.\n",
    "- What is the difference between hard margin SVM and soft margin SVM?\n",
    "  - if the data is linearly separable, we use hard margin SVM\n",
    "  - if it is impossible to find linear classifier, we allow misclassification of data points, and use appropriate soft margin SVM\n",
    "  - other use case for using soft margin SVM will be \n",
    "    - if data is linearly separable but the margin is small and the model either tends to overfit or is too sensitive to outliers, in that case, we opt for soft margin SVM and get larger margin, thereby generalizing the model better\n",
    "- What is functional margin? \n",
    "- What is geometric margin?\n",
    "- What is a kernel?\n",
    "  - A kernel is a method of placing a two dimensional plane into a higher dimensional space, so that it is curved in the higher dimensional space. (In simple terms, a kernel is a function from the low dimensional space into a higher dimensional space.)\n",
    "- What is kernel trick?\n",
    "  - SVM works better in two dimensional space which is linearly separable, as for non-linear data SVM finds it difficult to classify. So solve this kernel trick is used. A kernel trick is a method of projecting non-linear data onto higher dimensional space so as to easily classify linearly with a hyperplane. This is achieved using Lagrnagian formula/Lagrnagian multipliers.   \n",
    "- What are the different types of kernel?\n",
    "  - Fisher Kernel: It is a kernel function that analyses and measures the similarity of two objects. This is done on the basis of sets of measurements for each object and a statistical model.\n",
    "  - Graph Kernel: It is a kernel function that computes an inner product on graphs.\n",
    "  - Polynomial Kernel: It is a kernel commonly used with support vector machines (SVMs). It is also used with other kernelised models that symbolizes the similarity of vectors in a feature space over polynomials of the original variables, allowing learning of non-linear models.\n",
    "  - Radial Basis Function Kernel (RBF): It is a real-valued kernel function whose value depends only on the distance from the origin, or distance from some other point called a centre.\n",
    "- What is support vector mean?\n",
    "  - support vectors are the data points that lie farthest to the decision surface or hyperplane. They are the data points which are most difficult to classify\n",
    "- Is SVM applied for classification or regression?\n",
    "  - basically it is a non-probabilistic binary linear classification method\n",
    "  - but different flavors of it SVM exist in a probabilistic classification setting\n",
    "  - applying the kernel trick, we can have non-linear classification\n",
    "- Is SVM a binary method only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6a115",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e503324",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbor (kNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78a494",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "A cluster is a collection of data points aggregated based on its similarities. A centroid is the imaginary location representing the center of the cluster. In the k-means algorithm, k refers to the number of centroids which is defined initially. Each datapoint is allocated to a cluster iteratively based on optimizing the in-cluster variances (squared Euclidean distances).\n",
    "\n",
    "#### Algorithm\n",
    "- Data has no labels : $(x^{(1)}, x^{(2)}, ..., x^{(m)})$\n",
    "- Initialize cluster centroid $\\mu_{1}, \\mu_{2}, ..., \\mu_{k} \\in \\mathbb R^{n}$ - by randomly pick k example out of your training set and set cluster centroids to k-randomly chosen examples\n",
    "- Repeat until convergence\n",
    "  - a. Set $c^{(i)} := \\text{arg }\\min\\limits_{j} \\Vert (x^{(i)} - \\mu_{j})\\Vert_{2}$  (\"color the points\")\n",
    "    - Set $c^{(i)}$ equal to either j = 1 or 2 depending on whether that example $x^{(i)}$ is closer to cluster centroid 1 or 2\n",
    "    - Notation:\n",
    "      - L1 norm: $\\Vert x \\Vert_{1}$\n",
    "      - L2 norm: $\\Vert x \\Vert$ or $\\|x\\|^{2}$\n",
    "  - b. For j =1,2,..,k  (\"move the cluster centroids\")\n",
    "  > $\\mu_{j} := \\frac{\\sum\\limits_{i=1}^{m} \\mathbb 1 \\{c^{(i)} = j\\}x^{(i)}} {\\sum\\limits_{i=1}^{m} \\mathbb 1 \\{c^{(i)} = j\\}}$\n",
    "  \n",
    "- This algorithm is not guaranteed to converge, as it is a non-convex function\n",
    "- Cost/Distortion function\n",
    "> $J(c,\\mu) = \\min\\sum\\limits_{i=1}^{m}\\|x^{(i)} - \\mu_{c^{(i)}}\\|^{2}$\n",
    "- how do you choose k?\n",
    "  - choose manually, depending on what is the purpose of this algorithm\n",
    "  - if it is meant for market segmentation for 4 categories, it makes sense to have 4 cluster rather than more\n",
    "  - some formula available\n",
    "  \n",
    "#### How to choose optimal k\n",
    "- elbow method \n",
    "- silhouette score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48932dc",
   "metadata": {},
   "source": [
    "\n",
    "### Random Forest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Dimensionality Reduction Algorithms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Gradient Boosting algorithms\n",
    "\n",
    "https://www.quora.com/What-is-boosting-in-ML/answer/Mike-West-99?ch=10&oid=250818536&share=4eb23b20&srid=hM1JX&target_type=answer\n",
    "\n",
    "\n",
    "Gradient boosting is a **power technique for building predictive models**. Gradient Boosting is about taking a model that by itself is a **weak predictive model** and **combining that model with other models of the same type** to produce a **more accurate model**. The idea is to **compute a sequence of simple decisions trees**, where **each successive tree** is built for the **prediction residuals of the preceding tree**.\n",
    "\n",
    "In gradient boosting the **weak model** is called a **weak learner**. The term used when **combining various machine learning models** is an **ensemble**. The **weak learner in XGBoost is a decision tree**.\n",
    "\n",
    "Therefore, we need to understand **how a decision tree works** before we can understand boosting.\n",
    "\n",
    "- **Root node** \n",
    "  - The start of the tree is called the root node. \n",
    "- **Decision node**\n",
    "  - After the initial split are decision nodes.\n",
    "- **leaf node** \n",
    "  - will often lead to the answer or to the predicted value  \n",
    "\n",
    "\n",
    "- The Titanic dataset\n",
    "  - some groups of people were more likely to survive than others. \n",
    "    - The target variable, the attribute we want to **predict is survived**. \n",
    "    - Because the target variable is a 1 or a 0 this is a **binary classification problem**.\n",
    "  - **How would a decision tree be applied** to this dataset\n",
    "    - look for the **attribute that is the most important** in the dataset.\n",
    "    - then **analyze the second most import attribute** in the dataset\n",
    "    - creating a tree involves **deciding which features to choose** and what **conditions to use for splitting**, along with knowing when to stop\n",
    "\n",
    "#### GBM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### XGBoost\n",
    "https://logikbot.quora.com/XGBoost-The-King-of-the-Boosters-Why-should-you-be-using-XGBoost-The-simple-answer-is-that-nothing-beats-it-on-struc?ch=10&oid=11410205&share=956e2403&srid=hM1JX&target_type=post\n",
    "\n",
    "\n",
    "\n",
    "#### LightGBM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### CatBoost\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a351c20e",
   "metadata": {},
   "source": [
    "[Microsoft - ML Reference](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/component-reference)\n",
    "<img src=\"https://docs.microsoft.com/en-us/azure/machine-learning/media/algorithm-cheat-sheet/machine-learning-algorithm-cheat-sheet.png#lightbox\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3692df",
   "metadata": {},
   "source": [
    "## Regression - Predict a value\n",
    "### Boosted Decision Tree Regression\n",
    "### Decision Forest Regression\n",
    "### Fast Forest Quantile Regression\n",
    "### Linear Regression\n",
    "### Local Linear Regression\n",
    "\n",
    "### Locally Weighted Linear Regression (LWR)\n",
    "Any parametric model can be made local if the fitting method accommodates observation weights. This is a variant of linear regression where the weights of each training example in the cost function is defined as \n",
    "> $$ w^{(i)}(x) = \\exp \\left( -\\frac{(x^{(i)} - x)^{2}}{2\\tau^{2}} \\right) $$\n",
    "\n",
    "### Neural Network Regression\n",
    "### Poisson Regression\n",
    "### Quantile Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b860e4",
   "metadata": {},
   "source": [
    "## Classification - Predict a class. Choose from binary (two-class) or multiclass algorithms.\t\n",
    "### Multiclass Boosted Decision Tree\n",
    "### Multiclass Decision Forest\n",
    "\n",
    "### Multiclass Logistic Regression (Softmax Regression)\n",
    "Softmax Regression is a generalization of logistic regression where we want to handle multiple classes instead of two classes $(y^{(i)} \\in \\{0,1\\})$\n",
    "\n",
    "- https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "\n",
    "### Multiclass Neural Network\n",
    "### One vs. All Multiclass\n",
    "### One vs. One Multiclass\n",
    "### Two-Class Averaged Perceptron\n",
    "### Two-Class Boosted Decision Tree\n",
    "### Two-Class Decision Forest\n",
    "\n",
    "### Two-Class Logistic Regression\n",
    "\n",
    "<a href=\"#Logistic-Regression\">Logistic Regression</a>\n",
    "\n",
    "The hypothesis looks like \n",
    "\n",
    "$$ h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^{T}x}}$$ \n",
    "and the model parameters $\\theta$ are trained to minimize the cost function\n",
    "$$ J(\\theta) = - \\left [ \\sum\\limits_{i=0}^{n}y^{(i)}\\log\\sigma(\\theta^{T}x^{(i)}) + (1-y^{(i)}) \\log[1-\\sigma(\\theta^{T}x^{(i)})] \\right]  $$ \n",
    "$$ = -\\left [ \\sum\\limits_{i=0}^{n}y^{(i)}\\log h_{\\theta}(x^{(i)}) + (1-y^{(i)}) \\log[1- h_{\\theta}(x^{(i)})) ]\\right] $$ \n",
    "\n",
    "\n",
    "### Two-Class Neural Network\n",
    "### Two Class Support Vector Machine\n",
    "\n",
    "### Binary vs Multiclass vs MultiLabel classification\n",
    "- binary classification\n",
    "  - classifies observation into one of two possible outcomes\n",
    "  - finds a way to separate data from two classes\n",
    "  - examples\n",
    "    - ad will be clicked or not\n",
    "    - spam or not spam\n",
    "- multiclass classification\n",
    "  - also referred as multinomial classification\n",
    "  - examples\n",
    "    - handwritten zip codes\n",
    "- multilabel classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bfc848",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    "- https://www.mathworks.com/help/stats/linear-regression.html\n",
    "\n",
    "### Multiple Linear Regression\n",
    "### Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821c1a0",
   "metadata": {},
   "source": [
    "## Supervised learning (Classification, Regression)\n",
    "### Decision trees\n",
    "### Regression trees\n",
    "### Ensembles\n",
    "### k-NN\n",
    "### Linear regression\n",
    "### Naive Bayes\n",
    "### Artificial neural networks\n",
    "### Logistic regression\n",
    "\n",
    "### Perceptron\n",
    "The perceptron is an algorithm for learning a binary classifier called a threshold function: a function that maps its input $\\mathbf {x}$ (a real-valued vector) to an output value $f(\\mathbf {x} )$ (a single binary value):\n",
    "\n",
    "$$ f(\\mathbf {x} )={\\begin{cases}1&{\\text{if }}\\ \\mathbf {w} \\cdot \\mathbf {x} +b>0,\\\\0&{\\text{otherwise}}\\end{cases}}$$\n",
    "\n",
    "where $\\mathbf {w}$  is a vector of real-valued weights, $ \\mathbf {w} \\cdot \\mathbf {x} $ is the dot product $ \\sum _{i=1}^{m}w_{i}x_{i}$, where m is the number of inputs to the perceptron, and b is the bias. The bias shifts the decision boundary away from the origin and does not depend on any input value.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/1000px-Perceptron_example.svg.png\" width=500 height=500>\n",
    "$\\tiny\\text{Wikipedia}$\n",
    "\n",
    "### Relevance vector machine (RVM)\n",
    "### Support vector machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c5db2",
   "metadata": {},
   "source": [
    "## Unsupervised learning\n",
    "### K-means clustering\n",
    "### KNN (k-nearest neighbors)\n",
    "### Hierarchal clustering\n",
    "### Anomaly detection\n",
    "### Neural Networks\n",
    "### Principle Component Analysis\n",
    "### Independent Component Analysis\n",
    "### Apriori algorithm\n",
    "### Singular value decomposition\n",
    "\n",
    "## Clustering\n",
    "### BIRCH\n",
    "### CURE\n",
    "### Hierarchical\n",
    "### k-means\n",
    "### Expectation–maximization (EM)\n",
    "### DBSCAN\n",
    "### OPTICS\n",
    "### Mean shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97d7a8",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "### Factor analysis\n",
    "### CCA\n",
    "### ICA\n",
    "### LDA\n",
    "### NMF\n",
    "### PCA\n",
    "### PGD\n",
    "### t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811749f",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "- An ensemble of classifiers is a classifier build upon some combination of weak learners\n",
    "- strategy is to learn many weak classifiers and combine them in some way, instead of trying to learn a single strong classifier\n",
    "- rationale is, it is easy to train several simple classifiers and combine them into a more complex classifier than to learn a single complex classifier\n",
    "\n",
    "### Bootstrap aggregating (Bagging)\n",
    "- sample with replacement from the dataset\n",
    "- diversity in ensemble is ensured by the variation within bootstrap set\n",
    "- best suited for small available training datasets\n",
    "- reduces variance for those algorithms that have high variance, eg CART\n",
    "\n",
    "### Bagging vs Boosting \n",
    "- In bagging, \n",
    "  - instances selected to train individual classifiers are bootstrapped replicas of the training data, \n",
    "    - which means that each instance has equal chance of being in each training dataset. \n",
    "- In boosting, \n",
    "  - the training dataset for each subsequent classifier increasingly focuses on instances misclassified by previously generated classifiers.\n",
    "\n",
    "### Boosting\n",
    "- Boosting, in binary class problems, creates sets of three weak classifiers at a time: \n",
    "  - the first classifier (or hypothesis) h1 is trained on a random subset of the available training data, similar to bagging. \n",
    "  - te second classifier, h2, is trained on a different subset of the original dataset, precisely half of which is correctly identified by h1, and the other half is misclassified. Such a training subset is said to be the \"most informative,\" given the decision of h1. \n",
    "  - the third classifier h3 is then trained with instances on which h1 and h2 disagree. \n",
    "- These three classifiers are then combined through a three-way majority vote. \n",
    "- Schapire proved that the training error of this three-classifier ensemble is bounded above by $g(\\epsilon) < 3\\epsilon^2 - 2\\epsilon^3$, where $\\epsilon$ is the error of any of the three classifiers, provided that each classifier has an error rate $\\epsilon \\lt 0.5$, the least we can expect from a classifier on a binary classification problem.\n",
    "\n",
    "#### AdaBoost\n",
    "- AdaBoost(Adaptive Boosting) - Example of Boosting\n",
    "  - extended the original boosting algorithm to multiple classes\n",
    "    - multiple classes (AdaBoost.M1, AdaBost.M2)\n",
    "    - regression problems (AdaBoost.R)\n",
    "- 2 fundamental differences\n",
    "  - instances are drawn into subsequent datasets from an iteratively updated sample distribution of the training data\n",
    "  - classifiers are combined through weighted majority voting\n",
    "    - voting weights are based on classifiers training errors \n",
    "    - the distribution weights of instances correctly classified by the current hypothesis are reduced by a factor\n",
    "    - whereas weights of misclassified instances are left unchanged\n",
    "  - minimizes the exponential loss function that make algorithm sensitive to outlier\n",
    "  \n",
    "<img src=\"./images/adaBoost_algo.png\">\n",
    "\n",
    "#### Gradient Tree Boosting\n",
    "- Gradient Tree Boosting or Gradient Boosted Decision Tree (GBDT)\n",
    "- 3 main components\n",
    "  - additive model\n",
    "  - loss function\n",
    "    - solves differentiable loss function\n",
    "    - can be used both for classification and regression problem\n",
    "  - weak learner\n",
    "- at m-th step, fits a decision tree $h_m(x)$ to residuals\n",
    "\n",
    "#### XGBoost\n",
    "\n",
    "### Random Forest\n",
    "### Stacking\n",
    "### Bayes optimal classifier\n",
    "### Bayesian model averaging (BMA)\n",
    "### Bayesian model combination (BMC)\n",
    "### Bucket of models\n",
    "### Voting (complement of Bagging)\n",
    "### Blending (subtype of Stacking)\n",
    "\n",
    "## Ensemble Learning Techniques\n",
    "### Type of models used\n",
    "- Homogeneous models\n",
    "- Heterogeneous models\n",
    "\n",
    "### Data Sampling\n",
    "- With replacement\n",
    "- Without replacement\n",
    "- k-fold\n",
    "\n",
    "### Decision Function\n",
    "- Voting\n",
    "- Average\n",
    "- Meta Model\n",
    "\n",
    "### Types of learners\n",
    "- Weak learners\n",
    "  - slightly better than random guess\n",
    "- Strong learners\n",
    "  - very accurate predictions\n",
    "\n",
    "### Types of ensemble methods\n",
    "- Decrease Variance - Bagging\n",
    "- Decrease Bias - Boosting\n",
    "- Improve Prediction - Stacking\n",
    "\n",
    "### Types of ensemble learners\n",
    "- Sequential Learners\n",
    "  - mistakes of previous models are learned by their successors\n",
    "    - e.g., AdaBoost\n",
    "- Parallel Learners\n",
    "  - exploits independence between models by averaging out their mistakes\n",
    "    - e.g., Random Forest\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537cf3f0",
   "metadata": {},
   "source": [
    "## Structured Prediction - Graphical models \n",
    "### Bayes net\n",
    "### Conditional random field\n",
    "### Hidden Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f8f86a",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "### k-NN\n",
    "### Local outlier factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754017cb",
   "metadata": {},
   "source": [
    "## Artificial neural network\n",
    "### Autoencoder\n",
    "### Cognitive computing\n",
    "### Deep learning\n",
    "### DeepDream\n",
    "\n",
    "### Multilayer perceptron\n",
    "A **perceptron** was the name given to a model having one single linear layer and, if it has multiple layers, it is  called a **multi-layer perceptron (MLP)**. Note that the input and the output layers are visible from outside, while all the other layers in the middle are hidden – hence the name hidden layers. In this context, a single layer is simply a linear function and the MLP is therefore obtained by stacking multiple single layers one after the other:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Hassan-Afzaal/publication/338103191/figure/fig2/AS:838599264174093@1576949053675/The-multilayer-perceptron-MLP-model-for-various-input-variable-combinations.jpg\" width=500 height=500>\n",
    "\n",
    "### RNN \n",
    "#### LSTM\n",
    "#### GRU\n",
    "#### ESN\n",
    "### Restricted Boltzmann machine\n",
    "### GAN\n",
    "### SOM\n",
    "### Convolutional neural network \n",
    "#### U-Net\n",
    "### Transformer Vision\n",
    "### Spiking neural network\n",
    "### Memtransistor\n",
    "### Electrochemical RAM (ECRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b324d61c",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "### Q-learning\n",
    "### SARSA\n",
    "### Temporal difference (TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75079a82",
   "metadata": {},
   "source": [
    "## Machine Learning Theory\n",
    "### Kernel machines\n",
    "### Bias–variance tradeoff\n",
    "### Computational learning theory\n",
    "### Empirical risk minimization\n",
    "### Occam learning\n",
    "### PAC learning\n",
    "### Statistical learning\n",
    "### VC theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069766eb",
   "metadata": {},
   "source": [
    "## Recommender System - Methods and challenges\n",
    "### Cold start\n",
    "### Collaborative filtering\n",
    "### Dimensionality reduction\n",
    "### Implicit data collection\n",
    "### Item-item collaborative filtering\n",
    "### Matrix factorization\n",
    "### Preference elicitation\n",
    "### Similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63677e4",
   "metadata": {},
   "source": [
    "## 10 most popular deep learning algorithms\n",
    "[Link](https://www.simplilearn.com/tutorials/deep-learning-tutorial/deep-learning-algorithm)\n",
    "\n",
    "### Convolutional Neural Networks (CNNs)\n",
    "- Computer Vision with Deep Learning has been constructed and perfected with time, primarily over one particular algorithm — a **Convolutional Neural Network(ConvNet/CNN)**.\n",
    "- Neural networks came to prominence in **2012** as machine learning expert **Alex Krizhevsky** utilized them to get **first prize in the ImageNet competition**.\n",
    "- Applications\n",
    "  - Facebook’s famous automatic tagging algorithm works? The answer is neural networks.\n",
    "  - product recommendation you get on Amazon and several other similar platforms is because of neural networks.\n",
    "  - Neural networks are the reason behind Google’s superb image searching abilities.\n",
    "  - Instagram’s solid search infrastructure is possible because the social media network uses neural networks.\n",
    "- A convolutional network **ingests such images** as three separate strata of color **stacked one on top of the other**. A normal color image is seen as a rectangular box whose width and height are measured by the number of pixels from those dimensions. The depth layers in the three layers of colours(RGB) interpreted by CNNs are referred to as channels.\n",
    "- A ConvNet is able to successfully **capture the Spatial and Temporal dependencies** in an image through the application of relevant filters. \n",
    "- The **role of the ConvNet** is to **reduce the images** into a form which is easier to process, **without losing features** which are critical for getting a good prediction.\n",
    "- The objective of the Convolution Operation is to **extract the high-level features** such as edges, from the input image. ConvNets need not be limited to only one Convolutional Layer. Conventionally, the **first ConvLayer** is responsible for **capturing the Low-Level features** such as edges, color, gradient orientation, etc.\n",
    "- CNN's have a **ReLU layer** to **perform operations on elements**. The output is a rectified feature map.\n",
    "- the **Pooling layer** is responsible for **reducing the spatial size of the Convolved Feature**. This is to **decrease the computational power required to process the data through dimensionality reduction**. Furthermore, it is useful for **extracting dominant features which are rotational and positional invariant**, thus maintaining the process of effectively training of the model.\n",
    "- **Adding a Fully-Connected layer** is a (usually) **cheap way of learning non-linear combinations of the high-level features** as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space.\n",
    "- There are **various architectures of CNNs available** which have been key in building algorithms which power and shall power AI as a whole in the foreseeable future. Some of them have been listed below:\n",
    "  - LeNet\n",
    "  - AlexNet\n",
    "  - VGGNet\n",
    "  - GoogLeNet\n",
    "  - ResNet\n",
    "  - ZFNet\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*qtinjiZct2w7Dr4XoFixnA.gif\">\n",
    "$$\\text{Convolutional Layer}$$\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*uAeANQIOQPqWZnnuH-VEyw.jpeg\">\n",
    "$$\\text{A CNN sequence to classify handwritten digits}$$\n",
    "\n",
    "------\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/850/1*GLQjM9k0gZ14nYF0XmkRWQ.png\" width=500 height=500>\n",
    "$$\\text{Flattened 3x3 image matrix 1 dimension}$$\n",
    "\n",
    "------\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1052/1*GcI7G-JLAQiEoCON7xFbhg.gif\" width=500 height=500>\n",
    "$$\\text{Convolution Layer — The Kernel - Convoluting a 5x5x1 image(Green) with a 3x3x1 kernel(Yellow) to get a 3x3x1(Red) convolved feature}$$\n",
    "\n",
    "------\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*ciDgQEjViWLnCbmX-EeSrA.gif\">\n",
    "$$\\text{Convolution operation on a MxNx3 image matrix with a 3x3x3 Kernel}$$\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/790/1*1VJDP6qDY9-ExTuQVEOlVg.gif\" width=500 height=500>\n",
    "$$\\text{Convolution Operation with Stride Length = 2}$$\n",
    "\n",
    "------\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/790/1*nYf_cUIHFEWU1JXGwnz-Ig.gif\" width=500 height=500>\n",
    "$$\\text{SAME padding: 5x5x1 image is padded with 0s to create a 6x6x1 image}$$\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1192/1*KQIEqhxzICU7thjaQBfPBQ.png\" width=500 height=500>\n",
    "$$\\text{Types of Pooling}$$\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*kToStLowjokojIQ7pY2ynQ.jpeg\" width=500 height=500>\n",
    "$$\\text{Classification — Fully Connected Layer (FC Layer)}$$\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "- https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
    "- https://github.com/ss-is-master-chief/MNIST-Digit.Recognizer-CNNs/blob/master/MNIST-Hand.Written.Digit.Recognition-CNN.ipynb\n",
    "- https://medium.datadriveninvestor.com/introduction-to-how-cnns-work-77e0e4cde99b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f3316",
   "metadata": {},
   "source": [
    "### Long Short Term Memory Networks (LSTMs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df36a850",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs)\n",
    "- **Recurrent Neural Networks (RNNs)** are widely used for data with some kind of **sequential structure**. For instance, **time series data has an intrinsic ordering based on time**. Sentences are also sequential, “I love dogs” has a different meaning than “Dogs I love.” Simply put, if the **semantics of your data is altered by random permutation, you have a sequential dataset and RNNs may be used for your problem!** To help solidify the types of problems RNNs can solve, here is a list of common applications :\n",
    "- **Applications**\n",
    "  - Speech Recognition\n",
    "  - Sentiment Classification\n",
    "  - Machine Translation (i.e. Chinese to English)\n",
    "  - Video Activity Recognition\n",
    "  - Name Entity Recognition — (i.e. Identifying names in a sentence)\n",
    "- What are RNNs\n",
    "  - RNNs are **different than the classical multi-layer perceptron (MLP) networks** because of two main reasons: \n",
    "    - 1) They take into account what happened previously and \n",
    "    - 2) they share parameters/weights.\n",
    "  - A recurrent neural network is a neural network that is specialized for processing a sequence of data x(t)= x(1), . . . , x(τ) with the time step index t ranging from 1 to τ. For tasks that involve sequential inputs, such as speech and language, it is often better to use RNNs. In a NLP problem, if you want to predict the next word in a sentence it is important to know the words before it. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far.\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "\n",
    "- https://pub.towardsai.net/whirlwind-tour-of-rnns-a11effb7808f\n",
    "- https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85\n",
    "- https://github.com/javaidnabi31/RNN-from-scratch/blob/master/RNN_char_text%20generator.ipynb\n",
    "- https://www.deeplearningbook.org/contents/rnn.html\n",
    "- https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "- https://www.coursera.org/learn/nlp-sequence-models/lecture/0h7gT/why-sequence-models\n",
    "- [A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation](https://arxiv.org/pdf/1610.02583.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e0e51",
   "metadata": {},
   "source": [
    "\n",
    "### Generative Adversarial Networks (GANs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0901c5",
   "metadata": {},
   "source": [
    "### Radial Basis Function Networks (RBFNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1db2ca",
   "metadata": {},
   "source": [
    "### Multilayer Perceptrons (MLPs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab011d2e",
   "metadata": {},
   "source": [
    "### Self Organizing Maps (SOMs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b04825",
   "metadata": {},
   "source": [
    "### Deep Belief Networks (DBNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d773fb",
   "metadata": {},
   "source": [
    "### Restricted Boltzmann Machines( RBMs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4913d",
   "metadata": {},
   "source": [
    "### Autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1aad9",
   "metadata": {},
   "source": [
    "## Optimization algorithms - Deep Learning\n",
    "- [Optimizing GD](https://ruder.io/optimizing-gradient-descent/)\n",
    "- [Optimizer Visualization](https://github.com/Jaewan-Yun/optimizer-visualization)\n",
    "\n",
    "### ASGD\n",
    "### Adadelta\n",
    "### Adagrad\n",
    "### Adam\n",
    "### AdamW\n",
    "### Adamax\n",
    "### LBFGS\n",
    "### NAdam\n",
    "### RAdam\n",
    "### RMSprop\n",
    "### Rprop\n",
    "### SGD\n",
    "### SparseAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2024d8ed",
   "metadata": {},
   "source": [
    "## Evaluation Metrics - Regression\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "### R-Squared (Coefficient of determination)\n",
    "\n",
    "### Adjusted R-squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19c859",
   "metadata": {},
   "source": [
    "## Estimating Errors\n",
    "- Error Metrics\n",
    "  - MAE (Mean Absolute Error)\n",
    "  - MSE (Mean Squared Error)\n",
    "- Accuracy Metrics\n",
    "  - Precision\n",
    "  - Recall\n",
    "- Ranking Metrics\n",
    "  - MAP (Mean Average Precision)\n",
    "  - AUC (Area Under the Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12c8d2",
   "metadata": {},
   "source": [
    "## Evaluation Metrics - Distribution Fit\n",
    "\n",
    "### Bayesian information criterion\n",
    "### Kolmogorov–Smirnov test\n",
    "### Cramér–von Mises criterion\n",
    "### Anderson–Darling test\n",
    "### Shapiro–Wilk test\n",
    "\n",
    "### Chi-square test\n",
    "- http://www.medicine.mcgill.ca/epidemiology/joseph/courses/EPIB-621/fit.pdf\n",
    "- https://stats.stackexchange.com/questions/173748/what-does-goodness-of-fit-mean-in-context-of-linear-regression\n",
    "\n",
    "### Akaike information criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc2f61",
   "metadata": {},
   "source": [
    "## Evaluation Metrics - Classification\n",
    "### Classification Threshold\n",
    "- to map a logistic regression value to a binary category, a classification threshold (also called the decision threshold) is defined\n",
    "- assuming that the classification threshold should always be 0.5 is not true and needs to be tuned\n",
    "  - AUC helps us here\n",
    "- \"Tuning\" a threshold for logistic regression is different from tuning hyperparameters such as learning rate\n",
    "\n",
    "<img src=\"./images/precisionRecallTugOfWar_ClassificationThreshold.png\" width=500 height=500>\n",
    "\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1220/1*ZRU18eG-F5Sjtcph9ru8Og.png\" width=500 height=500> \n",
    "\n",
    ".|.\n",
    "-|- \n",
    "<img src=\"https://miro.medium.com/max/658/1*GJLE_YxjlSA5o_TJ9B_h5A.png\" width=500 height=300> | <img src=\"https://miro.medium.com/max/700/1*KPXQ6VBc1foXXWgkn1q4BA.png\" width=500 height=300>\n",
    "\n",
    "### Precision/Positive Predictive Value(PPV)\n",
    "$$\\text{Precision} = \\frac{t_{p}}{t_{p} + f_{p}}$$\n",
    "\n",
    "### Sensitivity/Recall/Hit Rate/TPR\n",
    "- This tells us what percentage of people **with** heart disease were actually correctly identified\n",
    "- **Sensitivity/Recall** is same as **True Positive Rate(TPR)**\n",
    "$$\\text{True Positive Rate(TPR) = Sensitivity = Recall} = \\frac{t_{p}}{t_{p} + f_{n}}$$\n",
    "\n",
    "### Specificity/Selectivity/TNR\n",
    "- This tells us what percentage of people **without** heart disease were actually correctly identified\n",
    "- Specificity is same as **(1 - False Positive Rate)**\n",
    "$$\\text{Specificity} = \\frac{t_{n}}{t_{n} + f_{p}}$$\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\" width=300 height=300>\n",
    "\n",
    "### Accuracy\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{P + N}$$\n",
    "- Accuracy is a poor or misleading metric\n",
    "  - Typical case includes class imbalance, when positives or negatives are extremely rare\n",
    "\n",
    "\n",
    "### F1\n",
    "- F1 is an overall measure of a model’s accuracy that combines precision and recall\n",
    "- A good F1 score means that you have low false positives and low false negatives, so you’re correctly identifying real threats and you are not disturbed by false alarms. \n",
    "- An F1 score is considered perfect when it’s 1, while the model is a total failure when it’s 0.\n",
    "\n",
    "$$\\text{F1} = \\frac{2 * \\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "### How to identify the correct threshold\n",
    "**Lower the threshold**\n",
    "- increase in number of false positive\n",
    "- decrease in number of false negative\n",
    "\n",
    "**Recalculate the confusion matrix**\n",
    "- increase in False positive rate\n",
    "- decrease in True positive rate\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*FdWxUcsx-sBvAvpn5sTtzg.png\" width=500 height=500>\n",
    "\n",
    "**Raise the threshold**\n",
    "- decrease in number of false positive\n",
    "- increase in number of false negative\n",
    "\n",
    "**Recalculate the confusion matrix**\n",
    "- decrease in False positive rate\n",
    "- increase in True positive rate\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*6EglCqnoHkVdux5hbElQ7w.png\" width=500 height=500>\n",
    "\n",
    "### ROC curve\n",
    "- Receiver Operator Characteristic Curve (ROC) \n",
    "  - helps in deciding the best threshold value\n",
    "  - plot true positive rate(y-axis)(Sensitivity) against false positive rate(x-axis)(1-Specificity)\n",
    "  - summarizes the confusion matrix for each threshold without having to calculate\n",
    "\n",
    ".|.\n",
    "-|-\n",
    "<img src=\"https://miro.medium.com/max/700/1*HTVotuY5L-cZjXGAW2A6_A.png\" width=500 height=500> | <img src=\"https://miro.medium.com/max/700/1*s4tdBI_DQ7xxMmUQUu5wqw.png\" width=500 height=500>\n",
    "<img src=\"https://miro.medium.com/max/693/1*6CxyxXxn9KhX_u-ZqxjbAA.png\" width=500 height=500> | \n",
    "<img src=\"https://miro.medium.com/max/700/1*5-1RHdhFO4TniI-U7-nKeA.png\" width=500 height=500>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*-78-LvCmqxPyAmBIGJaoLw.png\" width=500 height=500>\n",
    "\n",
    "\n",
    "### ROC curve for scenarios\n",
    "Best-case ROC curve|ROC curve with no predictive power|Worst-case ROC curve\n",
    ":-:|:-:|:-:\n",
    "AUC = 1 | AUC = 0.5 | AUC = 0\n",
    "<img src=\"https://www.graphpad.com/guides/prism/latest/curve-fitting/images/hmfile_hash_a3639962.png\" width=300 height=300> | <img src=\"https://www.graphpad.com/guides/prism/latest/curve-fitting/images/hmfile_hash_f3d108a2.png\" width=300 height=300> | <img src=\"https://www.graphpad.com/guides/prism/latest/curve-fitting/images/hmfile_hash_90cc150b.png\" width=300 height=300>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Area under the curve (AUC)\n",
    "- **Intuition:** gives an aggregate measure of performance aggregated across all possible classification thresholds\n",
    "- AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).\n",
    "\n",
    "- gives the rate of successful classification by logistic model\n",
    "- higher the AUC, better the model is at predicting classes\n",
    "- helps compare the ROC curve of different models\n",
    "- is the probability that the classifier will be able to tell which one is which\n",
    "- AUC = 1, implies an excellent model, good measure of separability\n",
    "- AUC = 0, implies a poor model, worst measure of separability\n",
    "- AUC = 0.5, implies an no class separation capacity, a random classifier\n",
    "\n",
    "\n",
    "AUC is desirable for the following two reasons:\n",
    "- AUC is scale-invariant. \n",
    "  - It measures how well predictions are ranked, rather than their absolute values.\n",
    "  - AUC is based on the relative predictions, so any transformation of the predictions that preserves the relative ranking has no effect on AUC\n",
    "- AUC is classification-threshold-invariant. \n",
    "  - multiplying all of the predictions from a given model by 2.0 - No impact on AUC\n",
    "    - It measures the quality of the model's predictions irrespective of what classification threshold is chosen.\n",
    "\n",
    ".|.\n",
    "-|-\n",
    "<img src=\"https://www.ismiletechnologies.com/wp-content/uploads/2021/10/image-9.png\" width=300 height=300> | <img src=\"https://miro.medium.com/max/700/1*U3KzhfUE4FLy4CL_U-ygpg.png\" width=500 height=500>\n",
    "\n",
    "\n",
    "\n",
    ".|.\n",
    "-|-\n",
    "<img src=\"https://miro.medium.com/max/1056/1*Uu-t4pOotRQFoyrfqEvIEg.png\" width=300 height=300> | <img src=\"https://miro.medium.com/max/1014/1*yF8hvKR9eNfqqej2JnVKzg.png\" width=300 height=300>\n",
    "<img src=\"https://miro.medium.com/max/860/1*iLW_BrJZRI0UZSflfMrmZQ.png\" width=300 height=300> | <img src=\"https://miro.medium.com/max/1112/1*aUZ7H-Lw74KSucoLlj1pgw.png\" width=300 height=300>\n",
    "\n",
    "\n",
    "\n",
    "### Diagnostic Testing Diagram\n",
    "<img src=\"./images/confusionMatrixTests3.png\">\n",
    "\n",
    "### Gini Coefficient\n",
    "\n",
    "### Log Loss or Cross Entropy Loss\n",
    "- The loss function for linear regression is squared loss. \n",
    "- The loss function for logistic regression is Log Loss, which is defined as follows\n",
    "\n",
    "<img src=\"./images/logisticRegressionCostFun.png\" width=500 height=500>\n",
    "\n",
    "- The cost function is defined as the sum across all the training examples as:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*s5AmzAfKxh06ymdw1zkNkA.png\" width=500 height=500>\n",
    "\n",
    "$\\tiny{\\text{towardsdatascience.com - Parul Pandey}}$  \n",
    "$\\tiny{\\text{towardsdatascience.com - Sarang Narkhede}}$  \n",
    "$\\tiny{\\text{towardsdatascience.com - Shuyu Luo}}$  \n",
    "\n",
    "### Prediction Bias\n",
    "- quantity that measures how far apart those two averages are. \n",
    "> Prediction Bias = Average of Predictions - Average of labels in dataset  \n",
    "> \"average of predictions\" should $\\approx$ \"average of observations\"   \n",
    "\n",
    "Possible root causes of prediction bias are:\n",
    "- Incomplete feature set\n",
    "- Noisy data set\n",
    "- Buggy pipeline\n",
    "- Biased training sample\n",
    "- Overly strong regularization\n",
    "\n",
    "To examine prediction bias\n",
    "- examine the prediction bias on a \"bucket\" of examples. \n",
    "  - Linearly breaking up the target predictions.\n",
    "  - Forming quantiles\n",
    "- Plot Prediction bias curve \n",
    "  - prediction vs label\n",
    "  - some subsets of the data set are noisier than others\n",
    "  \n",
    "  \n",
    "### Optimal Predicted Probability Cutoff\n",
    "- maximum vertical distance between ROC curve and diagonal line\n",
    "- called as Youden's J index\n",
    "- maximize the difference between True Positive and False Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82faa48f",
   "metadata": {},
   "source": [
    "## Generative Models\n",
    "\n",
    "## Discriminative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e087bae",
   "metadata": {},
   "source": [
    "## Types of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e87fb",
   "metadata": {},
   "source": [
    "## Estimation methods\n",
    "\n",
    "[Estimation Techniques](https://towardsdatascience.com/essential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0)\n",
    "\n",
    "### Maximum Likelihood (ML) Estimation\n",
    "A maximum likelihood estimator $\\hat{\\theta}$ is the solution of maximization problem. $\\hat{\\theta}$ is the parameter that maximizes the likelihood of the sample $\\zeta$\n",
    "\n",
    "$$\\hat{\\theta} = arg \\max\\limits_{\\theta}L(\\theta; \\zeta)$$\n",
    "\n",
    "#### Iterative Procedures\n",
    "To calculate the likelihood equations, which generally cannot be solved explicitly for an estimator $\\hat{\\theta} = \\hat{\\theta}(y)$, except for special cases. \n",
    "$\\frac{\\partial L(\\theta; y)}{\\partial \\theta} = 0$\n",
    "\n",
    "It is solved iteratively, starting iteratively from an initial guess of $\\theta$, seeking a convergent sequence\n",
    "$\\hat{\\theta}_{r+1} = \\hat{\\theta}_{r} + \\eta_{r}d_{r}(\\hat{\\theta})$, where $d_{r}(\\hat{\\theta})$ indicates the descent direection of $r^{th}$ step and $\\eta_{r}$ is the learning rate\n",
    "\n",
    "Few of the iterative methods are:\n",
    "- Gradient descent method\n",
    "- Newton-Raphson method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ea41e",
   "metadata": {},
   "source": [
    "### Maximum a Posteriori (MAP) Estimation\n",
    "- It is an estimate of posterior distribution\n",
    "- It is seen as a regularization of MLE\n",
    "- It tends to look like MLE asymptotically as $n \\rightarrow \\infty$\n",
    "  - __*Prior probability*__ represents what is originally believed before new evidence is introduced \n",
    "  - __*Posterior probability*__ takes this new information into account.\n",
    "\n",
    "$$\\theta_{MAP} = arg \\max\\limits_{\\theta}L(\\theta; D)$$\n",
    "$$\\theta_{MLE} = arg \\max\\limits_{\\theta}L(D; \\theta)$$\n",
    "$$ \\text{Posterior} \\propto \\text{Likelihood  x  Prior} $$\n",
    "\n",
    "### Minimum Mean Square Error (MMSE) Estimation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07191d",
   "metadata": {},
   "source": [
    "### Expectation-Maximization(EM) algorithm\n",
    "The EM algorithm helps us find the MLE for model parameters when the data has missing data points or unobserved (hidden) latent variables. It is an iterative method to approximate maximum likelihood function.\n",
    "\n",
    "#### Gaussian Density\n",
    "\n",
    "#### MLE vs EM\n",
    "\n",
    "#### Applications of EM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aeae45",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Least Square (LS) Estimation\n",
    "The best fit estimation line in least-square minimizes the sum of squared residuals\n",
    "- it has a closed form solution\n",
    "\n",
    "### Linear Least squares\n",
    "> Types of Linear least squares formulation:\n",
    "> - Ordinary Least squares (OLS)\n",
    ">   - The OLS method minimizes the sum of squared residuals, and leads to a closed-form expression for the estimated value of the unknown parameter vector\n",
    ">   - it has the assumption that the error terms have finite variance and are homoscedastic\n",
    "> - Weighted Least squares (WLS)\n",
    ">   - it has the assumption that the error terms are heteroscedasticity\n",
    "> - Generalized Least squares (GLS)\n",
    ">   - it has the assumption that the error terms are either heteroscedasticity or correlations or both are present among the error terms of the model\n",
    "\n",
    "### Non-linear\n",
    "The nonlinear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases.\n",
    "\n",
    "### Ordinary\n",
    "\n",
    "### Weighted\n",
    "\n",
    "### Generalized\n",
    "\n",
    "### Partial\n",
    "\n",
    "### Total\n",
    "\n",
    "### Non-negative\n",
    "\n",
    "### Ridge regression\n",
    "\n",
    "### Regularized\n",
    "\n",
    "### Least absolute deviations\n",
    "\n",
    "### Iteratively reweighted\n",
    "\n",
    "### Bayesian\n",
    "\n",
    "### Bayesian multivariate\n",
    "\n",
    "### Bayes Estimator\n",
    "\n",
    "### Probit\n",
    "\n",
    "### Logit\n",
    "\n",
    "### Kernel density estimation (KDE)\n",
    "KDE is a non-parametric way to estimate the probability density function of a random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aee0be",
   "metadata": {},
   "source": [
    "## Frequentists vs Bayesian Approach\n",
    "- Frequentists define probability as a relative frequency of an event in the long run\n",
    "- Bayesians define probability as a measure of uncertainty and belief for any event.\n",
    "> TODO: READ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0aebc7",
   "metadata": {},
   "source": [
    "## Parametric vs NonParametric Method\n",
    "- Parametric methods makes assumption in regards to the form of the function $f(X) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{p}X_{p}$, where $f(X)$ is the unknown function to be estimated, $\\beta$ are the coefficients to learn, $X$'s are the corresponding inputs and $p$ is the number of independent variables. \n",
    "  - These assumptions may or may not be correct\n",
    "  - these methods are quite fast\n",
    "  - they require significantly less data\n",
    "  - they are more interpretable\n",
    "  - Examples \n",
    "    - Linear Regression\n",
    "    - Naive Bayes\n",
    "    - Perceptron\n",
    "- NonParametric methods donot make any underlying assumption wrt to the form of the function to be estimated.\n",
    "  - they tend to be more accurate\n",
    "  - they require lots of data\n",
    "  - Examples:\n",
    "    - Support Vector Machines\n",
    "    - K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da96f1",
   "metadata": {},
   "source": [
    "## Prediction vs Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2dae6",
   "metadata": {},
   "source": [
    "## Mathematical functions\n",
    "\n",
    "### Logistic/Sigmoid function\n",
    "$$ f(x) = \\frac{L}{1+e^{-k(x-x_{0})}} $$\n",
    "> L - the curve’s maximum value  \n",
    "> $x_{0}$ - the sigmoid’s midpoint  \n",
    "> k - the logistic growth rate or steepness of curve  \n",
    "- they are asympotes, never reaches 0\n",
    "- output always fall between 0 and 1\n",
    "\n",
    "\n",
    "### Standard logistic sigmoid function\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
    "> L = 1 - the curve’s maximum value  \n",
    "> $x_{0}$ = 0 - the sigmoid’s midpoint  \n",
    "> k = 1 - the logistic growth rate or steepness of curve  \n",
    "\n",
    "\n",
    "\n",
    "### Logit function\n",
    "The logit function is the logarithm of the odds ratio (log-odds)\n",
    "$$ \\text{logit(p)} = \\ln\\frac{p}{1-p}$$\n",
    "\n",
    "### Probit function\n",
    "\n",
    "### Softplus function\n",
    "\n",
    "### Weighted sum\n",
    "$$ \\theta^{T}x = \\sum\\limits_{i=1}^{n}\\theta_{i}x_{i} = \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n} $$\n",
    "\n",
    "### Likelihood function\n",
    "- The likelihood talks about the optimal value of mean or standard deviation or parameters for a distribution given a bunch of observed measurements. \n",
    "- The likelihood function defines the joint probability of observed data as a function of parameters of the model. \n",
    "- It gives us a probabilistic prediction to the observed data. \n",
    "- In terms of hypothesis testing, the likelihood function gives us the probability of varying outcomes given a set of parameters defined in the null hypothesis as $f(\\theta \\mid x)$ is to inference (finding the likely parameters given a specific outcome).\n",
    "\n",
    "$$L(\\theta | D) = P(D | \\theta) = \\prod\\limits_{i}P(x_{i}| \\theta) $$\n",
    "\n",
    "<img src=\"./images/mle_vs_prob.png\" width=400 height=300>\n",
    "\n",
    "- The way of expressing how good a model is using the likelihood of the data given the model, which can be expressed as \n",
    "\n",
    "$$L(\\theta) = \\prod\\limits_{i}f(x_{i}|\\theta)$$\n",
    "\n",
    "- It contains the product of probability densities of all of the samples, an aggregate measure of how probable the dataset is given the choice of specific model. The best model is expressed as\n",
    "\n",
    "$$\\theta^{*} = \\text{argmax} L(\\theta) = \\text{argmax} \\prod\\limits_{i} f(x_{i}|\\theta)$$\n",
    "\n",
    "### Log-Likelihood function\n",
    "The log likelihood for logistic regression is given by \n",
    "$$ LL(\\theta) = - \\sum\\limits_{i=0}^{n}\\left[y^{(i)}\\log\\sigma(\\theta^{T}x^{(i)}) + (1-y^{(i)}) \\log[1-\\sigma(\\theta^{T}x^{(i)})]\\right]  $$\n",
    "\n",
    "### Gradient of log likelihood function\n",
    "It tries to choose values of $\\theta$ that maximizes the function\n",
    "$$ \\frac{\\partial LL(\\theta)}{\\partial \\theta_{j}} = \\sum\\limits_{i=0}^{n}\\left[ y^{(i)} - \\sigma(\\theta^{T}x^{(i)}) \\right]x_{j}^{(i)} $$\n",
    "\n",
    "### Negative log likelihood function\n",
    "\n",
    "### Likelihood vs Log-Likelihood function\n",
    "<img src=\"./images/GMM_Likelihood.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba7e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702acfed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "610b8eff",
   "metadata": {},
   "source": [
    "## Additive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0046b",
   "metadata": {},
   "source": [
    "## Machine Learning vs Rule-based Systems\n",
    "- rule-based systems  \n",
    "    - humans manually program rule-based systems \n",
    "    - rule based system explicitly need to be told what to do by humans\n",
    "- self-learning systems \n",
    "    - machines automatically train self-learning systems\n",
    "    - self-learning systems learn from experience "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c534fdae",
   "metadata": {},
   "source": [
    "## Model Selection/Validation\n",
    "- typical split - train(50%), validate(25%), test(25%)\n",
    "\n",
    "**How to choose one among a set of candidate models**\n",
    "- analytical - probabilistic statistical measure\n",
    "  - benefit\n",
    "    - test dataset is not required\n",
    "    - entire dataset is used to fit the model\n",
    "    - final model based on score prediction\n",
    "  - limitation\n",
    "    - cannot apply to compare different models\n",
    "    - metric derived for each model is different\n",
    "    - do not take inaccount uncertainty of the model, favour simple models\n",
    "  - types\n",
    "    - AIC - Akaike Information Criterion\n",
    "      - scoring based on its log-likelihood and complexity\n",
    "      - derived from frequentist probability\n",
    "      - AIC deals with trade-off between goodness of fit of model and  simplicity of the model \n",
    "      - AIC deals with both the risk of overfitting and the risk of underfitting\n",
    "      - AIC estimates the relative amount of information lost by a given model: the **less information a model loses, the higher the quality of that model**\n",
    "    - BIC - Bayesian Information Criterion\n",
    "      - scoring based on its log-likelihood and complexity\n",
    "      - derived from Bayesian probability\n",
    "      - the penalty is different from AIC\n",
    "    - MDL - Minimum Description Length\n",
    "      - comes from information theory\n",
    "    - SRM\n",
    "- sampling method\n",
    "  - types\n",
    "    - cross validation\n",
    "      - LOOCV - Leave-One-Out CV\n",
    "      - Stratified\n",
    "      - Repeated\n",
    "      - Nested\n",
    "    - bootstrap\n",
    "    - jackknife\n",
    "  \n",
    "\n",
    "$$AIC = - 2\\frac{LL}{N} + 2\\frac{d}{N} $$\n",
    "- where \n",
    "  - N - number of examples in training dataset\n",
    "  - LL - log-likelihood of the model on the training dataset\n",
    "  - d - number of parameters in the model\n",
    "\n",
    "$$BIC = -2 LL + d\\ln{N} $$\n",
    "\n",
    "\n",
    "- hold out\n",
    "  - split data into train and test set\n",
    "  - dependent on how the data is split\n",
    "- cross validation or k-fold\n",
    "  - dataset is randomly split into k-groups\n",
    "  - one is used as test set, rest as training set\n",
    "  - gives model opportunity to train on multiple train-test split\n",
    "  - requirees more computational power and time\n",
    "  \n",
    "  \n",
    "- https://robjhyndman.com/hyndsight/crossvalidation/\n",
    "- https://robjhyndman.com/hyndsight/aic/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aaa542",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "### Sigmoid function\n",
    "- Features\n",
    "  - Sigmoid function gives an S-shaped curve \n",
    "  - used to map predicted values to probabilities\n",
    "  - function maps any real value into another value between 0 and 1\n",
    "- Function\n",
    "  - Equation: $$ f(x) = s = \\frac{1}{1+e^{-x}}$$\n",
    "  - Derivative: $$ f’(x) = s*(1-s) $$\n",
    "  - Range: (0,1)\n",
    "- Adv\n",
    "  - function is differentiable. can find the slope of the sigmoid curve at any two points\n",
    "  - Output values bound between 0 and 1, which normalizes output of each neuron\n",
    "- DisAdv\n",
    "  - Vanishing gradient — for very high or very low values of X, there is almost no change to the prediction\n",
    "  - due to vanishing gradient problem, sigmoids have slow convergence\n",
    "  - outputs are not zero centered\n",
    "  - computationally expensive\n",
    "\n",
    "### Softmax\n",
    "- Features\n",
    "  - calculates probability distribution of an event over ’n’ different events\n",
    "- Function\n",
    "  - Equation: $$ f(x) = \\frac{e^{x_{i}}}{\\sum_{j=1}^{n} e^{x_{i}}} $$\n",
    "  - Probabilistic interpretation: $$ S_{j} = P(y=j|x) $$\n",
    "  - Range: (0, 1)\n",
    "- Adv\n",
    "  - Able to handle multiple classes only one class in other activation functions \n",
    "    - normalizes the outputs for each class between 0 and 1\n",
    "  - useful for output neurons to classify inputs as probability of multiple categories.\n",
    "\n",
    "### tanh function\n",
    "- Function\n",
    "  - Equation: $$ f(x) = a = \\tanh(x) =\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$\n",
    "  - Derivative: $$f’(x) = (1- a^{2})$$\n",
    "  - Range: (-1, 1)\n",
    "- Adv\n",
    "  - zero centered \n",
    "    - good for model that have strongly negative, neutral, and strongly positive values\n",
    "  - function and its derivative both are monotonic\n",
    "  - works better than sigmoid function\n",
    "- DisAdv\n",
    "  - vanishing gradient problem and hence slow convergence\n",
    "\n",
    "\n",
    "### ReLU (REctified Linear Unit) function\n",
    "- Features\n",
    "  - a simple non-linear function that chops off at non-zero \n",
    "  - the function is zero for negative values and it grows linearly for positive values. \n",
    "  - is very simple to implement (generally, three instructions are enough), while the sigmoid is a few orders of magnitude more. \n",
    "    - This helped to squeeze the neural networks onto an early GPU\n",
    "- Function\n",
    "  - Equation: $$ f(x) = \\max(x, 0) $$\n",
    "  - Derivative: $$ f'(x) = 1 \\text{ if } x \\gt 0 \\text{ otherwise } 0 $$\n",
    "  - Range: $$[0, +\\infty)$$ \n",
    "- Adv\n",
    "  - Computationally efficient \n",
    "    - allows the network to converge very quickly\n",
    "  - Non-linear \n",
    "    - although it looks like a linear function, ReLU has a derivative function and allows for back-propagation\n",
    "- DisAdv\n",
    "  - Dying ReLU problem \n",
    "    - when inputs approach zero, or are negative, gradient of function becomes zero, network cannot perform back-propagation and cannot learn\n",
    "\n",
    "### ELU \n",
    "\n",
    "### LeakyReLU\n",
    "- Features\n",
    "  - variant of ReLU\n",
    "- Function\n",
    "  - Equation: $$f(x)= a = \\max(0.01x, x)$$\n",
    "  - Derivative: $$ f'(x) = 1 \\text{ if } x \\gt 0 \\text{ otherwise } 0.01 $$\n",
    "  - Range: (0.01, +$\\infty$)\n",
    "- Adv\n",
    "  - Prevents dying ReLU problem \n",
    "    - this variation of ReLU has a small positive slope in the negative area, so it does enable back-propagation, even for negative input values\n",
    "- DisAdv\n",
    "  - Results not consistent \n",
    "    - leaky ReLU does not provide consistent predictions for negative input values.\n",
    "    - During the front propagation if the learning rate is set very high it will overshoot killing the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1897bc",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "### Mapping RawData to Features\n",
    "### Has Good Feature property\n",
    "### One-Hot Encoding\n",
    "### Scaling Feature values\n",
    "- with mean and std deviation\n",
    "- as z-values\n",
    "\n",
    "### Outlier Treatment\n",
    "### Binning by values\n",
    "### Binning by quantiles\n",
    "### Scrubing \n",
    "- Data reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b9567",
   "metadata": {},
   "source": [
    "## Feature Crosses\n",
    "- a synthetic feature formed by multiplying (crossing) two or more feature\n",
    "    - [A x B]: a feature cross formed by multiplying the values of two features.\n",
    "    - [A x B x C x D x E]: a feature cross formed by multiplying the values of five features.\n",
    "    - [A x A]: a feature cross formed by squaring a single feature.\n",
    "- logical conjunctions\n",
    "  - country:usa AND language:spanish\n",
    "- $x_{1}x_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fe1f76",
   "metadata": {},
   "source": [
    "## Regularization - Simplicity\n",
    "- Not trusting your data too much\n",
    "\n",
    "### Early stopping\n",
    "### Penalize model complexity\n",
    "\n",
    "### Empirical Risk Minimization\n",
    "- aim for low training error\n",
    "> minimize: Loss(Data|Model) \n",
    "\n",
    "### Structural Risk Minimization\n",
    "- aim for low training error\n",
    "- while balance against complexity\n",
    "> minimize: Loss(Data|Model) + Complexity(Model)\n",
    "\n",
    "### How to define model complexity\n",
    "- prefer smaller weights\n",
    "\n",
    "### Loss Function with L2 Regularization\n",
    "- the sum of the squares of all the feature weights\n",
    "- this does not depend upon the data, but rely on weights\n",
    "- will encourage many of the non-informative weights to be nearly (but not exactly) 0.0.\n",
    "\n",
    "$$L_{2} = ||w||_{2}^{2} = w_{1}^{2} + w_{2}^{2} + w_{3}^{2} + w_{4}^{2}$$\n",
    "\n",
    "### Regularization Rate\n",
    "> minimize: Loss(Data|Model) + $\\lambda$ Complexity(Model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa4b8f9",
   "metadata": {},
   "source": [
    "## Regularization - Sparsity\n",
    "\n",
    "### Sparse cross-feature\n",
    "- Model size will be huge\n",
    "- Noisy coefficients causing overfitting\n",
    "\n",
    "### Explicitly zero out weights\n",
    "- $L_{2}$ regularization encourages weights to be small, but doesn't force them to exactly 0.0\n",
    "- $L_{0}$ regularization\n",
    "  - penalize for non-zero weights\n",
    "  - but this is not convex and hard to optimize\n",
    "- Relax to $L_{1}$ regularization\n",
    "  - Penalize sum of abs(weights)\n",
    "  - Convex problem\n",
    "  - Encourage sparsity unlike $L_{2}$\n",
    "\n",
    "### L1 vs L2 regularization\n",
    "- $L_{2}$ and $L_{1}$ penalize weights differently:\n",
    "  - $L_{2}$ penalizes $weight^{2}$\n",
    "  - $L_{1}$ penalizes |weight|\n",
    "\n",
    "- Both $L_{2}$ and $L_{1}$ have different derivatives\n",
    "  - derivative of $L_{2}$ is 2 * weight.\n",
    "    - derivative of $L_{2}$ is a force that removes x% of the weight every time\n",
    "  - derivative of $L_{1}$ is k (a constant, whose value is independent of weight)\n",
    "    - derivative of $L_{1}$ is a force that subtracts some constant from the weight every time\n",
    "  \n",
    "### L1 regularization \n",
    "- will encourage most of the non-informative weights to be exactly 0.0.\n",
    "  - $L_{1}$ regularization of sufficient lambda tends to encourage non-informative weights to become exactly 0.0. \n",
    "  - By doing so, these non-informative features leave the model.\n",
    "- $L_{1}$ regularization may cause informative features to get a weight of exactly 0.0.\n",
    "  - **Be careful** \n",
    "    - L1 regularization may cause the following kinds of features to be given weights of exactly 0:\n",
    "      - Weakly informative features.\n",
    "      - Strongly informative features on different scales.\n",
    "      - Informative features strongly correlated with other similarly informative features.\n",
    "\n",
    "### Elastic Nets\n",
    "- is an extension of linear regression that adds regularization penalties to the loss function during training\n",
    "- the estimates from the elastic net method is defined as\n",
    "$$ \\hat{\\beta} = \\text{argmin}_{\\beta}(\\| y - X\\beta \\|^{2} + \\lambda_{2}\\| \\beta\\|^{2} + \\lambda_{1}\\| \\beta\\|_{1}) $$\n",
    "- the benefit of elastic net is that it allows balance of both the penalties as a weighted sum \n",
    "$$\\lambda_{1} + \\lambda_{2} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b7deb",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "- Linear combination of linear function is still linear (Left below)\n",
    "- To make NN non-linear, non-linear transformation needs to be introduced (Right below)\n",
    "- neural networks generally use non-linear activation functions\n",
    "\n",
    ". | .\n",
    "-|-\n",
    "<img src=\"./images/nn_linearNN.png\" width=400> | <img style=\"float: right;\" src=\"./images/nn_nonlinearNN.png\" width=400>\n",
    "\n",
    "\n",
    "\n",
    "### Linear Regression Model vs NN\n",
    "- A neural network without an activation function is just a linear regression model.\n",
    "- Linear combination of linear function is still linear\n",
    "- To make NN non-linear, non-linear transformation needs to be introduced\n",
    "\n",
    "### Why use activation function \n",
    "- without activation function the weights and bias would simply do a linear transformation\n",
    "- A linear equation is simple to solve but is limited in its capacity to solve complex problems and have less power to learn complex functional mappings from data. \n",
    "- A neural network without an activation function is just a linear regression model.\n",
    "- neural networks use non-linear activation functions\n",
    "\n",
    "### Types of Activation Function\n",
    "- Linear or Identity Activation Function\n",
    "- Non-linear Activation Functions\n",
    "\n",
    "### Linear Activation function\n",
    "- Activation function should be differentiable\n",
    "- problem in using linear activation function\n",
    "  - Back-propagation is not possible \n",
    "    - The derivative of linear function is a constant, and has no relation to the input, X. \n",
    "    - It’s not possible to go back and understand which weights in the input neurons can provide a better prediction.\n",
    "  - All layers of the neural network collapse into one \n",
    "    - with linear activation functions, no matter how many layers in the neural network, the last layer will be a linear function of the first layer\n",
    "\n",
    "### Non-Linear Activation function\n",
    "- allow model to create complex mappings between the network’s inputs and outputs \n",
    "- suited fir non-linear or high dimensionality models.\n",
    "- almost any functional computation in a neural network is possible\n",
    "- allow back-propagation because they have a derivative function which is related to the inputs.\n",
    "- allow \"stacking\" of multiple layers of neurons to create a deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9acb8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3af8f86f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3f95792",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco)",
   "language": "python",
   "name": "reco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "390.139px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
