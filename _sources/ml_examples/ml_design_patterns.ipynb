{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee87211",
   "metadata": {},
   "source": [
    "# Machine Learning Design Patterns\n",
    "\n",
    "## The Need for ML Design Patterns\n",
    "\n",
    " .|. \n",
    "--|--\n",
    "<img src=\"./images/mldp_data_representation.png\"> | <img src=\"./images/mldp_Reproducability.png\"> \n",
    "<img src=\"./images/mldp_ProbRepresentation.png\"> | <img src=\"./images/mldp_Resilience.png\"> \n",
    "<img src=\"./images/mldp_ModifyingTrainingLoop.png\"> | <img src=\"./images/mldp_ResponsibleAI.png\"> \n",
    "\n",
    "$\\tiny{\\text{20200917 - Sara Robinson - Responsible-AI-patterns}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90dcd9d",
   "metadata": {},
   "source": [
    "## Data representation design patterns\n",
    "- data representation\n",
    "  - baby weight represented as boolean in the model\n",
    "- feature engineering\n",
    "  - process of creating features\n",
    "- learnable data representation\n",
    "- feature extraction\n",
    "  - feature cross design pattern\n",
    "    - a decision tree where each node can represent only one input variable reduces to a stepwise linear function, \n",
    "    - an oblique decision tree where each node can represent a linear combination of input variables reduces to a piecewise linear function\n",
    "\n",
    "\n",
    "- Simple data representations\n",
    "  - Numerical inputs\n",
    "    - scaling say \\[-1,1\\]\n",
    "      - optimizer works better\n",
    "    - gradient descent \n",
    "      - requires more steps to converge as curvature of loss function increases\n",
    "        - derivative of features with large magnitude will be larger and will have abnormal weight\n",
    "      - centering data within a range makes error function more spherical, will converge faster\n",
    "    - lack of scaling \n",
    "      - affects efficacy of L1/L2 regularization\n",
    "\n",
    "\n",
    "- Linear scaling\n",
    "  - Min-max scaling\n",
    "  - Clipping\n",
    "    - helps in solving outliers\n",
    "  - Z-score normalization\n",
    "  - Winsorizing\n",
    "    - use empirical distribution to clip dataset\n",
    "    - 10 and 90 percentile\n",
    "\n",
    "- Min-max and clipping\n",
    "  - best for uniformly distributed data\n",
    "- Z-score\n",
    "  - best for normally distributed data\n",
    "  \n",
    "\n",
    "- dont throw away outliers\n",
    "\n",
    "\n",
    "- Non-linear transformations\n",
    "  - distribution of number of views of Wikipedia pages is highly skewed\n",
    "  - transform using the logarithm, power function and linear scaling in succession\n",
    "  - histogram equalization\n",
    "    - bins of histogram are chosen based on quantiles of raw distribution\n",
    "  - Box-Cox transformation\n",
    "  \n",
    "\n",
    "- Categorical inputs\n",
    "  - One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35ffb2",
   "metadata": {},
   "source": [
    "### #1 Hashed Feature\n",
    "- Problem Statement: \n",
    "  - incomplete vocabulary\n",
    "  - cardinality of model\n",
    "  - cold start\n",
    "- Solution\n",
    "  - one-hot encoding categorical variable\n",
    "    - Problem: pose problems for doctor_id of person delivering baby\n",
    "    - Potential Sol: \n",
    "      - training data might not contain complete information of all hospitals/physicians\n",
    "        - vocabulary is incomplete\n",
    "      - categorical variables can have high cardinality\n",
    "        - sparse dataset\n",
    "      - cold start problem\n",
    "        - new hired doctor\n",
    "  - Problem: Predict arrival delay of flight - 347 airport in US\n",
    "  - Process:\n",
    "    - feature will be quite sparse\n",
    "  - Potential Sol: `Hashed Feature design pattern`\n",
    "    - convert categorial input to unique string\n",
    "    - deterministic hashing algorithm (for training and testing)\n",
    "    - remainder of hash result as buckets\n",
    "      - returns absolute value of modulo of hashed number, which can be negative\n",
    "      - `farm_fingerprint` in BigQuery\n",
    "      - `feature_column` in TensorFlow\n",
    "    - choice of bucket size\n",
    "      - 10 buckets will have average of 35 airports\n",
    "        - implying characteristics of airports will be shared\n",
    "    - high cardinality\n",
    "      - using small number of buckets makes the model practical\n",
    "      - this is lossy encoding - acceptable compromise\n",
    "    - cold start\n",
    "      - solves the problem \n",
    "  - Trade Offs/Alternatives\n",
    "    - bucket collision\n",
    "      - compromising on the ability of accurately representing data\n",
    "      - dont use if vocabulary is small and cold start is not a problem\n",
    "    - Skew\n",
    "      - Chicago airport is busiest, so all airports in the same bucket will be highly skewed\n",
    "    - Aggregate feature\n",
    "      - for every airport, \n",
    "        - find probability of on-time flights \n",
    "        - add it as a feature of model\n",
    "        - this avoids losing information of airports while hashing\n",
    "    - Hyperparameter tuning\n",
    "      - choose number of buckets as hyperparameter\n",
    "    - Crypotographic hash\n",
    "      - by not using modulo in hashed feature\n",
    "        - this becomes a binary encoding problem\n",
    "      - binary encoding does not solve out-of-vocabulary or cold start problem\n",
    "        - airports starting with letter O have nothing in common\n",
    "        - by not using modulo, encoding will have a spurious correlation between airports that start with same letter\n",
    "        - so binary encoding of farm fingerprint is not recommended\n",
    "      - binary encoding of MD5 hash will not have spurious correlation\n",
    "        - as the output of MD5 hash is uniformly distributed\n",
    "        - but this is not deterministic and not unique\n",
    "      - fingerprint hashing algorithm is needed and not a cryptographic hashing algorithm\n",
    "        - so as to produce deterministic and unique value\n",
    "        - cryptographic hash is not usable in a feature engineering context\n",
    "      - MD5 is non-deterministic because of \"salt\"\n",
    "        - salt is the string that is added to each password\n",
    "        - this ensures that two users using the same password, will have different hashed value\n",
    "    - Order of operation is also important\n",
    "      - abs, mod, farm_fingerprint\n",
    "    - empty hash buckets\n",
    "      - possible\n",
    "      - use L2 regularization\n",
    "        - weights associated with empty bucket will be driven to near-zero\n",
    "        - this ensures, out-of-vocabulary fall into empty bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e508c",
   "metadata": {},
   "source": [
    "### #2 Embedding\n",
    "- Problem Statement: \n",
    "  - convert high cardinality data into low dimensional\n",
    "  - preserve the relevant information\n",
    "- Counterfactuals:\n",
    "  - categorical data represented as one-hot encoding\n",
    "    - does not represent plurality in dataset\n",
    "  - one-hot encoding represents features are independent\n",
    "    - closeness relationship is often required\n",
    "- Solution:\n",
    "  - embeddings capture closeness relationships in the input data\n",
    "  - use embedding layer/weights as replacement for \n",
    "    - clustering technique (customer segmentation)\n",
    "    - dimensionality reduction method (PCA)\n",
    "  - weights can be learned using gradient descent procedure\n",
    "- Text Embeddings\n",
    "  - cardinality of vocabulary \n",
    "    - large sparse matrix\n",
    "  - Tokenization\n",
    "    - lookup table that maps each word to an index\n",
    "- Image Embeddings\n",
    "  - text deals with sparse input\n",
    "  - image/audio deals with dense, high dimensional vectors\n",
    "    - in CNNs, if the last softmax layer is removed\n",
    "      - can be used to extract feature vector for an input\n",
    "- Learned Embeddings\n",
    "  - extracts inherent similarities between categories\n",
    "  - pretrained embeddings are referred as transfer learning\n",
    "- Trade-Offs/Alternatives:\n",
    "  - Choosing the embedding dimension\n",
    "  - Autoencoders\n",
    "    - Encoder\n",
    "      - maps high-dimensional input into low-dimensional embedding layer\n",
    "      - auxillary learning task\n",
    "    - Decoder\n",
    "      - maps representation back into high-dimensional embedding layer\n",
    "  - Context language models\n",
    "    - examples\n",
    "      - Context language models \n",
    "        - Word2Vec\n",
    "          - Continuous Bag of Words (CBOW)\n",
    "          - skip-gram model\n",
    "      - Bidirectional Encoding Representations from Transformers\n",
    "        - BERT\n",
    "          - masked language model\n",
    "            - words are randomly masked from text \n",
    "            - model guesses what the missing words are\n",
    "          - next sentence prediction\n",
    "            - classification task\n",
    "            - whether or not two sentences follow each other\n",
    "    - pre-trained text embedding\n",
    "      - like Word2Vec, NNLM, GLoVE, BERT\n",
    "      - can be added to ML model\n",
    "      - along with structured inputs\n",
    "  - Embeddings in a data warehouse\n",
    "    - TensorFlow Hub: has pretrained models like Swivel\n",
    "  \n",
    "\n",
    "\n",
    "- **Potential Project Work/Notes**:\n",
    "  - build a deep neural network (DNN) model in Keras that implements a simple embedding layer to transform the word integers into dense vectors\n",
    "  - CNN/NLP - image-to-text captioning \n",
    "    - encoder produces low-dimensional embedding representation of image\n",
    "    - encoder is Image2Vec embedding machine\n",
    "  - Read: TabNet\n",
    "  - TensorFlow Hub: has pretrained models like Swivel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64223a99",
   "metadata": {},
   "source": [
    "### #3 Feature Cross\n",
    "- helps model learn relationships between input faster by explicitly making each combination of input values a separate feature\n",
    "- suppose we need to create a binary classifier that separates the label into + and - classes\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/featureCross.png\" width=400 height=400></p>\n",
    "\n",
    "- its a commonly used feature engineering technique\n",
    "- a feature cross is a synthetic feature formed by concatenating two or more categorical features in order to capture interaction between them\n",
    "- by joining two features, nonlinearity can be encoded into the model\n",
    "  - the predictive abilities of features is extended\n",
    "- complex models like neural network and trees can learn feature crosses on their own\n",
    "- explicit features are better than linear models\n",
    "- speeds up model training and reduces model complexity\n",
    "\n",
    "- Trade-Offs and Alternatives\n",
    "  - Handling numerical features\n",
    "    - feature cross cannot be used with continuous input\n",
    "      - for m and n possible values, feature cross will be m\\*n elements\n",
    "    - if data is continous, bucketize the data to make it categorical before applying feature cross\n",
    "      - for example - latitude and longitude\n",
    "  - Handling high cardinality\n",
    "    - feature cross leads to sparsity\n",
    "    - can be useful to pass a feature cross through an Embedding layer\n",
    "      - this will create a lower-dimensional representation\n",
    "      - it will also allow to capture the closeness relationship\n",
    "  - Need for regularization\n",
    "    - as feature cross introduce multiplicative cardinality, the number of category increases\n",
    "    - if individual features have too few items, model will not be able to generalize\n",
    "    - so feature cross can either\n",
    "      - be paired with L1 regularization, which encourages sparsity of features\n",
    "      - or with L2 regularization, which limits overfitting\n",
    "    - thus model will be able to ignore extraneous noise by synthetic features and combat overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd642c7",
   "metadata": {},
   "source": [
    "### #4 Multimodal Input\n",
    "- Basic Idea\n",
    "  - represent different data types or data that can be expressed in complex ways by concatenating all the available data representations\n",
    "  - for example be able to combine image and text inputs in a model\n",
    "- Trade-Offs and Alternatives\n",
    "  - Tabular data multiple ways\n",
    "      - group them in buckets  \n",
    "  - Multimodal representation of text\n",
    "    - Text data multiple ways\n",
    "      - represent text as embeddings\n",
    "      - in BOW approach\n",
    "        - the order of text is not preserved\n",
    "        - but it detects presence/absence of certain words \n",
    "      - which approach to choose of BOW and embedding\n",
    "        - Embeddings\n",
    "          - provide extra information about word meaning\n",
    "            - not available in BOW\n",
    "          - but embeddings require training\n",
    "          - identifies relationships between words\n",
    "          - encodes the frequency of words in text\n",
    "        - BOW\n",
    "          - use BOW with linear regression or decision tree \n",
    "          - providees strong signals for most significant words in the vocabulary\n",
    "          - treats presence of each word as boolean value\n",
    "    - Extracting tabular features from text\n",
    "      - punctuation (question mark) influences the likelihood of answer\n",
    "  - Multimodal representation of images\n",
    "    - Images as pixel values\n",
    "    - Images as tiled structures\n",
    "    - Combining different image representations\n",
    "    - Using images with metadata\n",
    "  - Multimodal feature representations and model interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4c381",
   "metadata": {},
   "source": [
    "## Problem representation design patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e928b03",
   "metadata": {},
   "source": [
    "### #5 Reframing\n",
    "- Basic Idea\n",
    "  - refers to changing the representation of the output of a machine learning problem\n",
    "- Problem\n",
    "  - classify the problem\n",
    "    - type of learning problem\n",
    "    - type of features\n",
    "    - type of labels\n",
    "    - range of acceptable errors\n",
    "    - is it a time series forecasting problem\n",
    "    - is it a probabilistic problem\n",
    "    - is it a regression problem or a classification one\n",
    "    - how to improve the predictions\n",
    "    - do we need to add layers into our network\n",
    "    - do we need to engineer more features\n",
    "    - do we need more data\n",
    "    - do we need another loss function\n",
    "- Solution\n",
    "  - Why it works\n",
    "    - make it a multiclass classification problem\n",
    "      - rainfall is probabilistic\n",
    "      - instead of trying to predict as a single number(using regression), we can predict a range(using classification)\n",
    "      - this allows capturing probability distribution instead of capturing mean as in regression\n",
    "      - rainfall doesnot exhibit bell-shaped curve distribution model\n",
    "      - rainfall follows Tweedie distribution\n",
    "        - this allows prevalance of zeros\n",
    "        - Google research paper - MetNet\n",
    "          - https://arxiv.org/pdf/2003.12140.pdf\n",
    "    - objective is better supported\n",
    "      - recommendation system for videos\n",
    "      - instead of framing it as a classification problem, as how likely will user watch a video\n",
    "      - reframing it to regression problem, will help predict the fraction of video that will be watched\n",
    "    - learn the prediction as a range instead of a single number\n",
    "    - loss of precision due to bucketing\n",
    "    - but gains expressiveness in the form of full probability distribution function\n",
    "    - better understanding of posterior probability distribution of predicted values\n",
    "  - Capturing uncertainty\n",
    "    - task of predicting baby weight\n",
    "      - seems like a regression problem\n",
    "      - distribution of baby weights follows a normal distribution peaked at 7.5 pounds\n",
    "      - but\n",
    "        - width of distribution\n",
    "          - nontrivial likelihood is inherent in the prediction\n",
    "          - best RMSE we can have is SD of the distribution\n",
    "            - how far predictions fall from measured true values using Euclidean distance\n",
    "      - if this problem is framed as regression problem\n",
    "        - prediction would be predicted as 7.5+/-1.0\n",
    "        - width of distribution will differ by inputs\n",
    "        - this will take the shape of quantile regression\n",
    "          - which can be solved in nonparametric way\n",
    "    - by reframing the problem as multiclass classification\n",
    "      - more flexible in capturing uncertainty\n",
    "      - model predicts collection of probabilities for different combinations\n",
    "  - Changing the objective\n",
    "    - sometime reframing classification problem into regression can help\n",
    "      - e.g., build ML model for recommendations for a movie database with customer ratings\n",
    "        - instead of having categorical output, use multitask learning\n",
    "        - model learns about the user characteristics such as income, customer segment, etc\n",
    "        - users who are likely to watch movie\n",
    "      - reframing as regression task\n",
    "        - model predicts user-space representation for a given movie\n",
    "        - choose set of movies that are closest to known characteristics of user\n",
    "        - instead of providing probability that a user will like a movie as in classification\n",
    "        - it will predict cluster of movies that have been watched by users like this\n",
    "        - it can similarly be used to predict trending videos, classic movies\n",
    "    - numerical representation has intuitive interpretation\n",
    "      - real estate pricing surge\n",
    "        - instead of urban area predictions\n",
    "        - use of latitude and longitude pair is easier\n",
    "- Trade-Offs and Alternatives\n",
    "  - Bucketized outputs\n",
    "    - typical approach to reframe regression task to classification is to bucket the output values\n",
    "    - this becomes a multiclass classification\n",
    "    - comparing RMSE of regression model and accuracy of classification model is difficult    \n",
    "  - Other ways of capturing uncertainty\n",
    "    - Quantile regression\n",
    "    - Tensorflow probability to carry out regression\n",
    "    - more complex relationships require more training data\n",
    "    - thumb rule\n",
    "      - for classification tasks, \n",
    "        - have 10 times the number of model features for each label category\n",
    "  - Precision of predictions\n",
    "    - for multiclass classification\n",
    "      - width of bins govern precision of classification model\n",
    "      - sharpness of PDF tells about precision of task as regression\n",
    "        - sharper PDF implies smaller SD\n",
    "        - wider PDF implies larger SD\n",
    "          - very sharp density function(the peak), go with regression model\n",
    "  - Restricting the prediction range\n",
    "    - why? so as to restrict range of prediction output\n",
    "    - suppose output range is [5-17]\n",
    "    - if output layer is using linear activation function, model prediction may fall out of this range\n",
    "      - how?\n",
    "      - make activation function of the last-but-one layer a sigmoid function\n",
    "      - so range is [0,1]\n",
    "      - then scale using the last layer\n",
    "  - Label bias\n",
    "    - Matrix factorization in recommendation system\n",
    "      - can be reframed in NN, both as regression and classification\n",
    "      - can incorporate many additional features\n",
    "    - consider nature of target label when reframing \n",
    "      - suppose we reframed recommendation model to classification problem\n",
    "        - predict likelihood a user will click on video thumbnail\n",
    "        - here change of label is not in line with prediction task\n",
    "      - better choice will be, video watch time, as regression problem\n",
    "        - how long will user watch the video\n",
    "  - Multitask learning\n",
    "    - instead of choosing between regression and classification\n",
    "    - do both\n",
    "    - in Multitask learning, more than loss functions are optimized\n",
    "    - how to achieve this?\n",
    "      - parameter sharing\n",
    "        - parameters of neural network are shared between different output tasks such as regression and classification\n",
    "        - Hard parameter sharing\n",
    "          - when the hidden layer of model are shared between all the output tasks\n",
    "        - Soft parameter sharing\n",
    "          - each label has its own neural network with its own parameters\n",
    "    - through parameter sharing, tasks are learned simultaneously\n",
    "      - gradient updates from two loss functions inform both output and result in a generalized manner\n",
    "  \n",
    "  10am EST Monday \n",
    "  make pay before 8am Monday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66322f",
   "metadata": {},
   "source": [
    "### #6 Multilabel\n",
    "- Basic Idea\n",
    "  - refers to problems where we can assign more than one label to a given training example\n",
    "  - thats different from multiclass classification problem\n",
    "  - an example of multilabel - a movie can belong to multiple genres - can be both comedy and for children\n",
    "  - an example of multiclass - a movie can either be rated as U/A or A or U - it cannot be categorized as more than one class\n",
    "- Problem\n",
    "  - generally prediction is of N possible classes where N is greater than 1\n",
    "    - softmax function as activation function is generally used for the output layer\n",
    "  - if model is classifying images as cats, dogs or bunnies, the softmax output will predict one of these\n",
    "  - but each image can have more than one possible label\n",
    "- Solution\n",
    "  - SIGMOID VERSUS SOFTMAX ACTIVATION\n",
    "  - softmax activation function\n",
    "    - sums to 1\n",
    "    - each value is between 0 and 1\n",
    "    - for example: \n",
    "      - chances of cats, dogs and bunnies [0.75, 0.10, 0.25]\n",
    "    - use this function in multiclass classification problem\n",
    "  - sigmoid activation function\n",
    "    - does not sum to 1\n",
    "    - each value is between 0 and 1\n",
    "    - chances of image having a cat, dog and bunnies [0.85. 0.55, 0.45]\n",
    "    - use this function in multilabel classification problem\n",
    "- Trade-Offs and Alternatives\n",
    "  - Sigmoid output for models with two classes\n",
    "    - binary classification is a special type of multiclass classification problem\n",
    "      - each training example can be assigned only one class\n",
    "      - both sigmoid and softmax can be used as activation function\n",
    "  - Which loss function should we use?\n",
    "    - for binary classification\n",
    "      - binary cross entropy loss function should be used\n",
    "  - Parsing sigmoid results\n",
    "    - for softmax output\n",
    "      - argmax of array gives the predicted class\n",
    "    - for sigmoid output\n",
    "      - evaluate probability of each class in output layer\n",
    "      - use the probability threshold\n",
    "        - determine the confidence threshold \n",
    "        - consider the top K \n",
    "  - Dataset considerations\n",
    "    - generally it is ensured that the dataset is balanced, equal number of training examples for each class\n",
    "    - but for Multilabel design pattern, thats difficult\n",
    "    - for hierarchial labels\n",
    "      - use flat approach \n",
    "        - put every output label in the same output array, irrespective of its hierarchy\n",
    "        - this approach might loses information\n",
    "      - use cascade design pattern\n",
    "        - identify higher labels first\n",
    "        - then based on high level classification, choose lower labels    \n",
    "  - Inputs with overlapping labels\n",
    "    - [10001000]\n",
    "    - first and fifth label correspond to two description for an item\n",
    "  - One versus rest\n",
    "    - train multiple binary classifiers instead of one multilabel model\n",
    "      - choose a confidence threshold \n",
    "      - tag input question with tags for each binary classifier above threshold\n",
    "    - adv\n",
    "      - can be used with SVMs which can only do binary classification\n",
    "      - helps in rare category classification, as it classifies one task at a time, apply rebalancing design pattern then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb434c5b",
   "metadata": {},
   "source": [
    "### #7 Ensembles\n",
    "- Basic Idea\n",
    "  - refers to techniques in machine learning that combine multiple machine learning models and aggregate their results to make prediction\n",
    "- Problem\n",
    "  - irreducible error\n",
    "    - error inherent in the model, resulting from \n",
    "      - the noise in the dataset\n",
    "      - framing of the problem\n",
    "      - bad training examples/measurement errors/confounding factors\n",
    "  - reducible error\n",
    "    - error due to bias\n",
    "      - bias leads to inability to learn relationship about model's features and labels\n",
    "      - high bias will lead to underfit\n",
    "    - error due to variance\n",
    "      - variance leads to inability to generalize on new unseen examples\n",
    "      - high variance will lead to overfit\n",
    "    - goal is to have low bias and low variance model\n",
    "      - increasing model complexity \n",
    "        - decreases bias but increases variance\n",
    "      - decreasing model complexity \n",
    "        - decreases variance but increases bias\n",
    "- Solution\n",
    "  - Bagging\n",
    "    - short for bootstrap aggregating\n",
    "    - addresses high variance in ML models\n",
    "    - example of bagging ensemble method\n",
    "      - random forest\n",
    "        - multiple decision trees are trained on randomly sampled subsets of the entire training data\n",
    "    - model averaging in bagging\n",
    "      - method of reducing model variance\n",
    "  - Boosting\n",
    "    - works by iteratively improving model to reduce prediction error\n",
    "    - model is punished more and more as per the residuals at each iteration step\n",
    "    - each new weak learner corrects for the mistakes of the previous prediction by modeling the residuals delta_i of each step\n",
    "    - after multiple iterations, residuals tend toward zero\n",
    "    - AdaBoost, Gradient Boosting Machines, and XGBoost\n",
    "    - good for high bias sets\n",
    "  - Stacking\n",
    "    - combines outputs of collection of models to make prediction\n",
    "    - \n",
    "- Trade-Offs and Alternatives\n",
    "  - Increased training and design time\n",
    "  - Dropout as bagging\n",
    "  - Decreased model interpretability\n",
    "    - explanation predictions can get difficult\n",
    "    - for example random forest\n",
    "  - Choosing the right tool for the problem\n",
    "  - Other ensemble methods\n",
    "    - incorporate Bayesian approach with neural architecture search and reinforcement learning, like Google’s AdaNet/AutoML \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c2f91",
   "metadata": {},
   "source": [
    "\n",
    "### #8 Cascade\n",
    "- Basic Idea\n",
    "  - addresses situation where a ML problem can be broken down into series of ML problem\n",
    "  - the output of one model is an input to the following model\n",
    "- Problem\n",
    "- Solution\n",
    "  - typically a ML problem can be solved by breaking it into 4 cascade problems\n",
    "    - a classification model to identify the circumstance\n",
    "    - one model trained on unusual circumstances\n",
    "    - separate model trained on typical circumstances\n",
    "    - model to combine the output of the two separate models, because the output is a probabilistic combination of the two outputs\n",
    "  - this pattern adds quite a bit of complexity in the workflow\n",
    "- Trade-Offs and Alternatives\n",
    "  - Deterministic inputs\n",
    "  - Single model\n",
    "    - should not be used where a single model is eenough\n",
    "  - Internal consistency\n",
    "  - Pre-trained models\n",
    "  - Reframing instead of Cascade\n",
    "  - Regression in rare situations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a14de",
   "metadata": {},
   "source": [
    "\n",
    "### #9 Neutral Class\n",
    "- Basic Idea\n",
    "  - train a binary classifier that outputs probability of an event, train a three-class classifier that outputs disjoint probabilities for Yes, No, and Maybe\n",
    "  - Maybe is a disjoint class and they don't overlap\n",
    "- Problem\n",
    "- Solution\n",
    "  - Synthetic data\n",
    "  - In real world\n",
    "- Trade-Offs and Alternatives\n",
    "  - When human experts disagree\n",
    "  - Customer satisfaction\n",
    "  - As a way to improve embeddings\n",
    "  - Reframing with neutral class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c381490",
   "metadata": {},
   "source": [
    "\n",
    "### #10 Rebalancing\n",
    "- Basic Idea\n",
    "  - approaches for handling datasets that are inherently imbalanced\n",
    "- Problem\n",
    "  - accuracy can be misleading on imbalanced datasest\n",
    "- Solution\n",
    "  - Choosing an evaluation metric\n",
    "    - use metrics like precision, recall, or F-measure\n",
    "    - F-measure ranges from 0 to 1, uses both precision and recall\n",
    "    - test class should have the same class balance as original dataset\n",
    "    - ROC curve (AUC)\n",
    "  - Downsampling\n",
    "    - changes the balance of our underlying dataset\n",
    "  - Weighted classes\n",
    "    - changes how model handles certain classes\n",
    "  - Upsampling\n",
    "    - duplicates examples from minority class, and \n",
    "    - apply augmentations to generate additional samples\n",
    "- Trade-Offs and Alternatives\n",
    "  - Reframing and Cascade\n",
    "  - Anomaly detection\n",
    "  - Number of minority class examples available\n",
    "  - Combining different techniques\n",
    "  - Choosing a model architecture\n",
    "  - Importance of explainability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a39421",
   "metadata": {},
   "source": [
    "\n",
    "## Model training patterns\n",
    "- iterative process\n",
    "\n",
    "### Typical Training Loop\n",
    "- Stochastic Gradient Descent(SGD)\n",
    "  - error/loss needs to be monitored\n",
    "    - not a closed form solution\n",
    "  - extensions(de facto optimizer in modern day frameworks)\n",
    "    - Adam\n",
    "    - Adagrad\n",
    "- Keras Training Loop\n",
    "\n",
    "### #11 Useful Overfitting\n",
    "- Basic Idea\n",
    "  - omit use of validation or testing dataset so as to intentionally overfit on the training dataset\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "  - Interpolation and chaos theory\n",
    "  - Monte Carlo methods\n",
    "  - Data-driven discretizations\n",
    "  - Unbounded domains\n",
    "  - Distilling knowledge of neural network\n",
    "  - Overfitting a batch\n",
    "  \n",
    "### #12 Checkpoints\n",
    "- Basic Idea\n",
    "  - store the full state of the model periodically, which can be accessed in partially trained models\n",
    "  - along with also use virtual epochs, wherein the inner loop of the fit() function is used, not on the full training dataset but on a fixed number of training examples\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #13 Transfer Learning\n",
    "- Basic Idea\n",
    "  - use previously trained model, \n",
    "  - freeze the weights, and \n",
    "  - incorporate nontrainable layers into a new model that solves the same problem, but on a smaller dataset\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #14 Distribution Strategy\n",
    "- Basic Idea\n",
    "  - training loop is carried out at scale over multiple workers, \n",
    "    - often with caching, hardware acceleration, and parallelization\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #15 Hyperparameter Tuning\n",
    "- Basic Idea\n",
    "  - training loop is inserted into an optimization method to find the optimal set of model hyperparameters\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb02d8a",
   "metadata": {},
   "source": [
    "\n",
    "## Resilience patterns\n",
    "### #16 Stateless Serving Function\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #17 Batch Serving\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #18 Continuous Model Evaluation\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #19 Two Phase Predictions\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #20 Keyed Predictions\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "## Reproducibility patterns\n",
    "### #21 Transform\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #22 Repeatable Sampling\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #23 Bridged Schema\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #24 Windowed Inference\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #25 Workflow Pipeline\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #26 Feature Store\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #27 Model Versioning\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "## Responsible AI\n",
    "### #28 Heuristic benchmark\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #29 Explainable Predictions\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #30 Fairness Lens\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2301351",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- Machine Learning Design Patterns - by Valliappa Lakshmanan, Sara Robinson, and Michael Munn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
