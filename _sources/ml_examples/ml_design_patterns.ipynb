{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee87211",
   "metadata": {},
   "source": [
    "# Machine Learning Design Patterns\n",
    "\n",
    "## The Need for ML Design Patterns\n",
    "\n",
    " .|. \n",
    "--|--\n",
    "<img src=\"./images/mldp_data_representation.png\"> | <img src=\"./images/mldp_Reproducability.png\"> \n",
    "<img src=\"./images/mldp_ProbRepresentation.png\"> | <img src=\"./images/mldp_Resilience.png\"> \n",
    "<img src=\"./images/mldp_ModifyingTrainingLoop.png\"> | <img src=\"./images/mldp_ResponsibleAI.png\"> \n",
    "\n",
    "$\\tiny{\\text{20200917 - Sara Robinson - Responsible-AI-patterns}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90dcd9d",
   "metadata": {},
   "source": [
    "## Data representation design patterns\n",
    "- data representation\n",
    "  - baby weight represented as boolean in the model\n",
    "- feature engineering\n",
    "  - process of creating features\n",
    "- learnable data representation\n",
    "- feature extraction\n",
    "  - feature cross design pattern\n",
    "    - a decision tree where each node can represent only one input variable reduces to a stepwise linear function, \n",
    "    - an oblique decision tree where each node can represent a linear combination of input variables reduces to a piecewise linear function\n",
    "\n",
    "\n",
    "- Simple data representations\n",
    "  - Numerical inputs\n",
    "    - scaling say \\[-1,1\\]\n",
    "      - optimizer works better\n",
    "    - gradient descent \n",
    "      - requires more steps to converge as curvature of loss function increases\n",
    "        - derivative of features with large magnitude will be larger and will have abnormal weight\n",
    "      - centering data within a range makes error function more spherical, will converge faster\n",
    "    - lack of scaling \n",
    "      - affects efficacy of L1/L2 regularization\n",
    "\n",
    "\n",
    "- Linear scaling\n",
    "  - Min-max scaling\n",
    "  - Clipping\n",
    "    - helps in solving outliers\n",
    "  - Z-score normalization\n",
    "  - Winsorizing\n",
    "    - use empirical distribution to clip dataset\n",
    "    - 10 and 90 percentile\n",
    "\n",
    "- Min-max and clipping\n",
    "  - best for uniformly distributed data\n",
    "- Z-score\n",
    "  - best for normally distributed data\n",
    "  \n",
    "\n",
    "- dont throw away outliers\n",
    "\n",
    "\n",
    "- Non-linear transformations\n",
    "  - distribution of number of views of Wikipedia pages is highly skewed\n",
    "  - transform using the logarithm, power function and linear scaling in succession\n",
    "  - histogram equalization\n",
    "    - bins of histogram are chosen based on quantiles of raw distribution\n",
    "  - Box-Cox transformation\n",
    "  \n",
    "\n",
    "- Categorical inputs\n",
    "  - One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35ffb2",
   "metadata": {},
   "source": [
    "### #1 Hashed Feature\n",
    "- Problem Statement: \n",
    "  - incomplete vocabulary\n",
    "  - cardinality of model\n",
    "  - cold start\n",
    "- Solution\n",
    "  - one-hot encoding categorical variable\n",
    "    - Problem: pose problems for doctor_id of person delivering baby\n",
    "    - Potential Sol: \n",
    "      - training data might not contain complete information of all hospitals/physicians\n",
    "        - vocabulary is incomplete\n",
    "      - categorical variables can have high cardinality\n",
    "        - sparse dataset\n",
    "      - cold start problem\n",
    "        - new hired doctor\n",
    "  - Problem: Predict arrival delay of flight - 347 airport in US\n",
    "  - Process:\n",
    "    - feature will be quite sparse\n",
    "  - Potential Sol: `Hashed Feature design pattern`\n",
    "    - convert categorial input to unique string\n",
    "    - deterministic hashing algorithm (for training and testing)\n",
    "    - remainder of hash result as buckets\n",
    "      - returns absolute value of modulo of hashed number, which can be negative\n",
    "      - `farm_fingerprint` in BigQuery\n",
    "      - `feature_column` in TensorFlow\n",
    "    - choice of bucket size\n",
    "      - 10 buckets will have average of 35 airports\n",
    "        - implying characteristics of airports will be shared\n",
    "    - high cardinality\n",
    "      - using small number of buckets makes the model practical\n",
    "      - this is lossy encoding - acceptable compromise\n",
    "    - cold start\n",
    "      - solves the problem \n",
    "  - Trade Offs/Alternatives\n",
    "    - bucket collision\n",
    "      - compromising on the ability of accurately representing data\n",
    "      - dont use if vocabulary is small and cold start is not a problem\n",
    "    - Skew\n",
    "      - Chicago airport is busiest, so all airports in the same bucket will be highly skewed\n",
    "    - Aggregate feature\n",
    "      - for every airport, \n",
    "        - find probability of on-time flights \n",
    "        - add it as a feature of model\n",
    "        - this avoids losing information of airports while hashing\n",
    "    - Hyperparameter tuning\n",
    "      - choose number of buckets as hyperparameter\n",
    "    - Crypotographic hash\n",
    "      - by not using modulo in hashed feature\n",
    "        - this becomes a binary encoding problem\n",
    "      - binary encoding does not solve out-of-vocabulary or cold start problem\n",
    "        - airports starting with letter O have nothing in common\n",
    "        - by not using modulo, encoding will have a spurious correlation between airports that start with same letter\n",
    "        - so binary encoding of farm fingerprint is not recommended\n",
    "      - binary encoding of MD5 hash will not have spurious correlation\n",
    "        - as the output of MD5 hash is uniformly distributed\n",
    "        - but this is not deterministic and not unique\n",
    "      - fingerprint hashing algorithm is needed and not a cryptographic hashing algorithm\n",
    "        - so as to produce deterministic and unique value\n",
    "        - cryptographic hash is not usable in a feature engineering context\n",
    "      - MD5 is non-deterministic because of \"salt\"\n",
    "        - salt is the string that is added to each password\n",
    "        - this ensures that two users using the same password, will have different hashed value\n",
    "    - Order of operation is also important\n",
    "      - abs, mod, farm_fingerprint\n",
    "    - empty hash buckets\n",
    "      - possible\n",
    "      - use L2 regularization\n",
    "        - weights associated with empty bucket will be driven to near-zero\n",
    "        - this ensures, out-of-vocabulary fall into empty bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e508c",
   "metadata": {},
   "source": [
    "### #2 Embedding\n",
    "- Problem Statement: \n",
    "  - convert high cardinality data into low dimensional\n",
    "  - preserve the relevant information\n",
    "- Counterfactuals:\n",
    "  - categorical data represented as one-hot encoding\n",
    "    - does not represent plurality in dataset\n",
    "  - one-hot encoding represents features are independent\n",
    "    - closeness relationship is often required\n",
    "- Solution:\n",
    "  - embeddings capture closeness relationships in the input data\n",
    "  - use embedding layer/weights as replacement for \n",
    "    - clustering technique (customer segmentation)\n",
    "    - dimensionality reduction method (PCA)\n",
    "  - weights can be learned using gradient descent procedure\n",
    "- Text Embeddings\n",
    "  - cardinality of vocabulary \n",
    "    - large sparse matrix\n",
    "  - Tokenization\n",
    "    - lookup table that maps each word to an index\n",
    "- Image Embeddings\n",
    "  - text deals with sparse input\n",
    "  - image/audio deals with dense, high dimensional vectors\n",
    "    - in CNNs, if the last softmax layer is removed\n",
    "      - can be used to extract feature vector for an input\n",
    "- Learned Embeddings\n",
    "  - extracts inherent similarities between categories\n",
    "  - pretrained embeddings are referred as transfer learning\n",
    "- Trade-Offs/Alternatives:\n",
    "  - Choosing the embedding dimension\n",
    "  - Autoencoders\n",
    "    - Encoder\n",
    "      - maps high-dimensional input into low-dimensional embedding layer\n",
    "      - auxillary learning task\n",
    "    - Decoder\n",
    "      - maps representation back into high-dimensional embedding layer\n",
    "  - Context language models\n",
    "    - examples\n",
    "      - Context language models \n",
    "        - Word2Vec\n",
    "          - Continuous Bag of Words (CBOW)\n",
    "          - skip-gram model\n",
    "      - Bidirectional Encoding Representations from Transformers\n",
    "        - BERT\n",
    "          - masked language model\n",
    "            - words are randomly masked from text \n",
    "            - model guesses what the missing words are\n",
    "          - next sentence prediction\n",
    "            - classification task\n",
    "            - whether or not two sentences follow each other\n",
    "    - pre-trained text embedding\n",
    "      - like Word2Vec, NNLM, GLoVE, BERT\n",
    "      - can be added to ML model\n",
    "      - along with structured inputs\n",
    "  - Embeddings in a data warehouse\n",
    "    - TensorFlow Hub: has pretrained models like Swivel\n",
    "  \n",
    "\n",
    "\n",
    "- **Potential Project Work/Notes**:\n",
    "  - build a deep neural network (DNN) model in Keras that implements a simple embedding layer to transform the word integers into dense vectors\n",
    "  - CNN/NLP - image-to-text captioning \n",
    "    - encoder produces low-dimensional embedding representation of image\n",
    "    - encoder is Image2Vec embedding machine\n",
    "  - Read: TabNet\n",
    "  - TensorFlow Hub: has pretrained models like Swivel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64223a99",
   "metadata": {},
   "source": [
    "### #3 Feature Cross\n",
    "- helps model learn relationships between input faster by explicitly making each combination of input values a separate feature\n",
    "- suppose we need to create a binary classifier that separates the label into + and - classes\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/featureCross.png\" width=400 height=400></p>\n",
    "\n",
    "- its a commonly used feature engineering technique\n",
    "- a featuree cross is a synthetic feature formed by concatenating two or more categorical features in order to capture interaction between them\n",
    "- by joining two features, nonlinearity can be encoded into the model\n",
    "  - the predictive abilities of features is extended\n",
    "- complex models like neural network and trees can learn feature crosses on their own\n",
    "- explicit features are better than linear models\n",
    "- speeds up model training and reduces model complexity\n",
    "\n",
    "- Trade-Offs and Alternatives\n",
    "  - Handling numerical features\n",
    "    - feature cross cannot be used with continuous input\n",
    "      - for m and n possible values, feature cross will be m\\*n elements\n",
    "    - if data is continous, bucketize the data to make it categorical before applying feature cross\n",
    "      - for example - latitude and longitude\n",
    "  - Handling high cardinality\n",
    "    - feature cross leads to sparsity\n",
    "    - can be useful to pass a feature cross through an Embedding layer\n",
    "      - this will create a lower-dimensional representation\n",
    "      - it will also allow to capture the closeness relationship\n",
    "  - Need for regularization\n",
    "    - as feature cross introduce multiplicative cardinality, the number of category increases\n",
    "    - if individual features have too few items, model will not be able to generalize\n",
    "    - so feature cross can either\n",
    "      - be paired with L1 regularization, which encourages sparsity of features\n",
    "      - or with L2 regularization, which limits overfitting\n",
    "    - thus model will be able to ignore extraneous noise by synthetic features and combat overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd642c7",
   "metadata": {},
   "source": [
    "### #4 Multimodal Input\n",
    "- Basic Idea\n",
    "  - represent different data types or data that can be expressed in complex ways by concatenating all the available data representations\n",
    "  - for example be able to combine image and text inputs in a model\n",
    "- Trade-Offs and Alternatives\n",
    "  - Tabular data multiple ways\n",
    "      - group them in buckets  \n",
    "  - Multimodal representation of text\n",
    "    - Text data multiple ways\n",
    "      - represent text as embeddings\n",
    "      - in BOW approach\n",
    "        - the order of text is not preserved\n",
    "        - but it detects presence/absence of certain words \n",
    "      - which approach to choose of BOW and embedding\n",
    "        - Embeddings\n",
    "          - provide extra information about word meaning\n",
    "            - not available in BOW\n",
    "          - but embeddings require training\n",
    "          - identifies relationships between words\n",
    "          - encodes the frequency of words in text\n",
    "        - BOW\n",
    "          - use BOW with linear regression or decision tree \n",
    "          - providees strong signals for most significant words in the vocabulary\n",
    "          - treats presence of each word as boolean value\n",
    "    - Extracting tabular features from text\n",
    "      - punctuation (question mark) influences the likelihood of answer\n",
    "  - Multimodal representation of images\n",
    "    - Images as pixel values\n",
    "    - Images as tiled structures\n",
    "    - Combining different image representations\n",
    "    - Using images with metadata\n",
    "  - Multimodal feature representations and model interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4c381",
   "metadata": {},
   "source": [
    "## Problem representation design patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e928b03",
   "metadata": {},
   "source": [
    "### #5 Reframing\n",
    "- Basic Idea\n",
    "  - refers to changing the representation of the output of a machine learning problem\n",
    "- Problem\n",
    "- Solution\n",
    "  - Capturing uncertainty\n",
    "  - Changing the objective\n",
    "- Trade-Offs and Alternatives\n",
    "  - Bucketized outputs\n",
    "  - Other ways of capturing uncertainty\n",
    "  - Precision of predictions\n",
    "  - Restricting the prediction range\n",
    "  - Label bias\n",
    "  - Multitask learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66322f",
   "metadata": {},
   "source": [
    "### #6 Multilabel\n",
    "- Basic Idea\n",
    "  - refers to problems where we can assign more than one label to a given training example\n",
    "- Problem\n",
    "- Solution\n",
    "  - SIGMOID VERSUS SOFTMAX ACTIVATION\n",
    "- Trade-Offs and Alternatives\n",
    "  - Sigmoid output for models with two classes\n",
    "  - Which loss function should we use?\n",
    "  - Parsing sigmoid results\n",
    "  - Dataset considerations\n",
    "  - Inputs with overlapping labels\n",
    "  - One versus rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb434c5b",
   "metadata": {},
   "source": [
    "### #7 Ensembles\n",
    "- Basic Idea\n",
    "  - refers to techniques in machine learning that combine multiple machine learning models and aggregate their results to make prediction\n",
    "- Problem\n",
    "  - irreducible error\n",
    "    - error inherent in the model, resulting from the noise in the dataset\n",
    "  - reducible error\n",
    "    - error due to bias\n",
    "      - bias leads to inability to learn relationship about model's features and labels\n",
    "      - high bias will lead to underfit\n",
    "    - error due to variance\n",
    "      - variance leads to inability to generalize on new unseen examples\n",
    "      - high variance will lead to overfit\n",
    "    - goal is to have low bias and low variance model\n",
    "      - increasing model complexity \n",
    "        - decreases bias but increases variance\n",
    "      - decreasing model complexity \n",
    "        - decreases variance but increases bias\n",
    "- Solution\n",
    "  - Bagging\n",
    "    - bootstrap aggregating\n",
    "  - Boosting\n",
    "    - works by iteratively improving model to reduce prediction error\n",
    "    - model is punished more and more as per the residuals at each iteration step\n",
    "    - each new weak learner corrects for the mistakes of the previous prediction by modeling the residuals delta_i of each step\n",
    "    - AdaBoost, Gradient Boosting Machines, and XGBoost\n",
    "    - good for high bias sets\n",
    "  - Stacking\n",
    "- Trade-Offs and Alternatives\n",
    "  - Increased training and design time\n",
    "  - Dropout as bagging\n",
    "  - Decreased model interpretability\n",
    "    - explanation predictions can get difficult\n",
    "    - for example random forest\n",
    "  - Choosing the right tool for the problem\n",
    "  - Other ensemble methods\n",
    "    - incorporate Bayesian approach with neural architecture search and reinforcement learning, like Google’s AdaNet/AutoML \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c2f91",
   "metadata": {},
   "source": [
    "\n",
    "### #8 Cascade\n",
    "- Basic Idea\n",
    "  - addresses situation where a ML problem can be broken down into series of ML problem\n",
    "  - the output of one model is an input to the following model\n",
    "- Problem\n",
    "- Solution\n",
    "  - typically a ML problem can be solved by breaking it into 4 cascade problems\n",
    "    - a classification model to identify the circumstance\n",
    "    - one model trained on unusual circumstances\n",
    "    - separate model trained on typical circumstances\n",
    "    - model to combine the output of the two separate models, because the output is a probabilistic combination of the two outputs\n",
    "  - this pattern adds quite a bit of complexity in the workflow\n",
    "- Trade-Offs and Alternatives\n",
    "  - Deterministic inputs\n",
    "  - Single model\n",
    "    - should not be used where a single model is eenough\n",
    "  - Internal consistency\n",
    "  - Pre-trained models\n",
    "  - Reframing instead of Cascade\n",
    "  - Regression in rare situations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a14de",
   "metadata": {},
   "source": [
    "\n",
    "### #9 Neutral Class\n",
    "- Basic Idea\n",
    "  - train a binary classifier that outputs probability of an event, train a three-class classifier that outputs disjoint probabilities for Yes, No, and Maybe\n",
    "  - Maybe is a disjoint class and they don't overlap\n",
    "- Problem\n",
    "- Solution\n",
    "  - Synthetic data\n",
    "  - In real world\n",
    "- Trade-Offs and Alternatives\n",
    "  - When human experts disagree\n",
    "  - Customer satisfaction\n",
    "  - As a way to improve embeddings\n",
    "  - Reframing with neutral class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c381490",
   "metadata": {},
   "source": [
    "\n",
    "### #10 Rebalancing\n",
    "- Basic Idea\n",
    "  - approaches for handling datasets that are inherently imbalanced\n",
    "- Problem\n",
    "  - accuracy can be misleading on imbalanced datasest\n",
    "- Solution\n",
    "  - Choosing an evaluation metric\n",
    "    - use metrics like precision, recall, or F-measure\n",
    "    - F-measure ranges from 0 to 1, uses both precision and recall\n",
    "    - test class should have the same class balance as original dataset\n",
    "    - ROC curve (AUC)\n",
    "  - Downsampling\n",
    "    - changes the balance of our underlying dataset\n",
    "  - Weighted classes\n",
    "    - changes how model handles certain classes\n",
    "  - Upsampling\n",
    "    - duplicates examples from minority class, and \n",
    "    - apply augmentations to generate additional samples\n",
    "- Trade-Offs and Alternatives\n",
    "  - Reframing and Cascade\n",
    "  - Anomaly detection\n",
    "  - Number of minority class examples available\n",
    "  - Combining different techniques\n",
    "  - Choosing a model architecture\n",
    "  - Importance of explainability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a39421",
   "metadata": {},
   "source": [
    "\n",
    "## Model training patterns\n",
    "- iterative process\n",
    "\n",
    "### Typical Training Loop\n",
    "- Stochastic Gradient Descent(SGD)\n",
    "  - error/loss needs to be monitored\n",
    "    - not a closed form solution\n",
    "  - extensions(de facto optimizer in modern day frameworks)\n",
    "    - Adam\n",
    "    - Adagrad\n",
    "- Keras Training Loop\n",
    "\n",
    "### #11 Useful Overfitting\n",
    "- Basic Idea\n",
    "  - omit use of validation or testing dataset so as to intentionally overfit on the training dataset\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "  - Interpolation and chaos theory\n",
    "  - Monte Carlo methods\n",
    "  - Data-driven discretizations\n",
    "  - Unbounded domains\n",
    "  - Distilling knowledge of neural network\n",
    "  - Overfitting a batch\n",
    "  \n",
    "### #12 Checkpoints\n",
    "- Basic Idea\n",
    "  - store the full state of the model periodically, which can be accessed in partially trained models\n",
    "  - along with also use virtual epochs, wherein the inner loop of the fit() function is used, not on the full training dataset but on a fixed number of training examples\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #13 Transfer Learning\n",
    "- Basic Idea\n",
    "  - use previously trained model, \n",
    "  - freeze the weights, and \n",
    "  - incorporate nontrainable layers into a new model that solves the same problem, but on a smaller dataset\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #14 Distribution Strategy\n",
    "- Basic Idea\n",
    "  - training loop is carried out at scale over multiple workers, \n",
    "    - often with caching, hardware acceleration, and parallelization\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #15 Hyperparameter Tuning\n",
    "- Basic Idea\n",
    "  - training loop is inserted into an optimization method to find the optimal set of model hyperparameters\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb02d8a",
   "metadata": {},
   "source": [
    "\n",
    "## Resilience patterns\n",
    "### #16 Stateless Serving Function\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #17 Batch Serving\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #18 Continuous Model Evaluation\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #19 Two Phase Predictions\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #20 Keyed Predictions\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "## Reproducibility patterns\n",
    "### #21 Transform\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #22 Repeatable Sampling\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #23 Bridged Schema\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #24 Windowed Inference\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #25 Workflow Pipeline\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #26 Feature Store\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #27 Model Versioning\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "## Responsible AI\n",
    "### #28 Heuristic benchmark\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #29 Explainable Predictions\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "### #30 Fairness Lens\n",
    "- Basic Idea\n",
    "- Problem\n",
    "- Solution\n",
    "- Trade-Offs and Alternatives\n",
    "\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2301351",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- Machine Learning Design Patterns - by Valliappa Lakshmanan, Sara Robinson, and Michael Munn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "372.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
