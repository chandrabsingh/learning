{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c188b9",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Andrew Ng - CS229 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60472310",
   "metadata": {},
   "source": [
    "# Lec 02-Linear Regression - Gradient Descent\n",
    "\n",
    "- Assume there is house property dataset, with size and price, and goal is to have a function which can predict the price given size\n",
    "    - in supervised learning, we have a training set, which we fed to learning algorithm, whose job is to output a function h or hypothesis, which can make predictions about housing prices\n",
    "    - the job of hypothesis for a given house size, it gives price estimation \n",
    "- Question \n",
    "    - how to represent learning algorithm h?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902aef6",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "- $\\theta$ - parameters or the weights of learning algorithm parameterizing the space of linear functions mapping from $x$ to $y$\n",
    "  - choose $\\theta$ such that $h(x) \\approx y$ for training examples\n",
    "- $x_{j}^{(i)}$ - inputs/features - $j^{th}$ training example of ith feature in the training set ( a bit confusing with i/j and subscript/superscript)\n",
    "  - input features \n",
    "    - $x_{1}$ - size of house\n",
    "    - $x_{2}$ - # of bedroom\n",
    "  - Weight vector $\\begin{equation*}\n",
    "\\theta   = \n",
    "\\begin{bmatrix}\n",
    "\\theta_{0}  \\\\\n",
    "\\theta_{1}  \\\\\n",
    "\\theta_{2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$\n",
    "  - Feature vector $\\begin{equation*}\n",
    "x   = \n",
    "\\begin{bmatrix}\n",
    "x_{0}  \\\\\n",
    "x_{1}  \\\\\n",
    "x_{2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$, where $x_{0}$ = 1\n",
    "\n",
    "- $y$ - output\n",
    "- $(x, y)$ - one  training example\n",
    "- $(x^{(i)}, y^{(i)})$ - $i^{th}$  training example\n",
    "- $h_{\\theta}(x)$ - hypothesis\n",
    "  - hypothesis depends on both the parameters $\\theta$ and the input features $x$\n",
    "> $$h_{\\theta}(x) = \\sum\\limits_{i=0}^{n}\\theta_{i}x_{i} = \\theta^{T}x$$\n",
    "- $J(\\theta)$ - cost function\n",
    "  - choose values of $\\theta$ so that the equation below is minimized\n",
    "  - adding 1/2 makes math a little bit simpler\n",
    "  - why squared error?\n",
    "    - will talk about it during generalized linear models (GLM)\n",
    "> $$ J(\\theta) = \\frac{1}{2}\\sum\\limits_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})^{2} $$\n",
    "\n",
    "- m - number of training examples (# of rows)\n",
    "- n - number of features\n",
    "- number of dimensions is n+1 because of constant \n",
    "\n",
    "<img src=\"./images/02_housingPrices_sqft.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff777323",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "- find an algorithm that minimizes the cost function\n",
    "\n",
    "- _generally speaking if you run gradient descents on linear regression, we don't end up with local optimum_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56af61",
   "metadata": {},
   "source": [
    "### Least Mean Squares (LMS) algorithm\n",
    "\n",
    "* horizontal axis - $ \\theta _{0} \\space and \\space \\theta _{1} $   \n",
    "* vertical axis - look all around you - keep changing $ \\theta $ to reduce the $ J(\\theta) $\n",
    "  * $ J(\\theta) $ - cost function\n",
    "* start with some $ \\theta $ (say $ \\theta = \\overrightarrow{\\rm 0} $ )\n",
    "* keep changing/moving till you reach at the global minima and not local minima  \n",
    "  * <i> Use $ := $ sign for assignments </i>\n",
    "* $ \\theta _{j} := \\theta _{j} - \\alpha \\frac{\\partial}{\\partial \\theta _{j}}J(\\theta) $ - __Eq 1__\n",
    "  * $ \\alpha $ is the learning rate\n",
    "  * for each value of $ j = 0,1,2,..n $, for n features\n",
    "    * <i> $ a := a + 1 $ - means increment the value of a and assign it to a </i>\n",
    "    * <i> $ a = b $ - means that is an assertment that it is a fact that a is equal to b </i>\n",
    "* derivative of function defines the direction of the gradient \n",
    "* assuming for 1 training example..  \n",
    "> $ \\frac{\\partial}{\\partial \\theta _{j}} J(\\theta) $   \n",
    "> $ = \\frac{\\partial}{\\partial \\theta _{j}} \\frac{1}{2} (h _{\\theta}(x) - y)^{2} $   \n",
    "> $ = (h _{\\theta}(x) - y) \\frac{\\partial}{\\partial \\theta _{j}} (\\theta _{0} x _{0} + \\theta _{1} x _{1} + .. \\theta _{n} x _{n} - y) $   \n",
    "* partial derivative of every term will be 0 other than $ \\theta _{j} $ term, which resolves into following  \n",
    "$ = (h_{\\theta}(x) - y).x_{j} $  \n",
    "* substituting in Eq1  \n",
    "> $ \\theta _{j} := \\theta _{j} - \\alpha (h_{\\theta}(x) - y).x_{j} $  \n",
    "\n",
    "* for __\"m training example\"__, the above results in:  \n",
    "$ \\theta _{j} := \\theta _{j} - \\alpha \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)}).x_{j}^{(i)} $  - __Eq 2__ \n",
    "* all that is done, that sum over all m training examples, where $(i)$ is the $ i^{th} $ training example.  \n",
    "* Gradient descent algorithm is to be repeated till it convergences\n",
    "  * for each value of $ j = 0,1,2,..n $, for __\"n features\"__ ( in this example it's 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aacc9c",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "* $J(\\theta)$ has no local optima, it has only global optimum\n",
    "* other way to look into the cost function is to look at the contours of this curve - ellipses\n",
    "    * if GD is run on this\n",
    "    * if \\alpha is too large - it will overshoot\n",
    "    * if you look into the contours, the direction of steepest descent is always orthogonal to contour direction\n",
    "    * try a few values, to \n",
    "    * If cost function is increasing, it indicates that the learning rate is too large\n",
    "    * try few values at exponential rate, 0.02, 0.04, 0.08, 0.16, .. - which tells you the direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f80cac",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "\n",
    "* training egs here are 49\n",
    "* initially hypothesis - $ \\theta _{0} \\space and \\space \\theta _{1} $ are assigned the value 0 - the cost function result will be too high\n",
    "* after each iteration hypothesis, the cost function is minimized by the gradiend descent algorithm\n",
    "* eventually it converges\n",
    "<br>\n",
    "\n",
    "- Iteration - Base\n",
    "<img src=\"./images/02_housingPrices_sqft.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "- Iteration 0\n",
    "<img src=\"./images/02_housingPrices_sqft0.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "- Iteration 1\n",
    "<img src=\"./images/02_housingPrices_sqft1.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "- Iteration 2\n",
    "<img src=\"./images/02_housingPrices_sqft2.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "- Iteration n\n",
    "<img src=\"./images/02_housingPrices_sqftn.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c86304d",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Why is negative $\\alpha$ multiplied to the gradient descent, instead of positive $\\alpha$ ?\n",
    "* because you will go uphill the gradient descent instead of going downhill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bedf34",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "When do you stop\n",
    "* Plot $J(\\theta)$ over time\n",
    "* linear regression does not have local minima, so you will not have the problem of convergence\n",
    "* but training nonlinear like neural network will have such acute problem of convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa50705",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "- we look into data in batches - for example in this case, there is a batch of 49 training example\n",
    "- disadv\n",
    "  - if we have a large dataset, inorder to make one single step of gd, we will have to calculate the sum of __Eq 2__ above\n",
    "  - if m is 1M, to make one step we will have to iterate over 1M times\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "- Alternative to Batch GD\n",
    "- instead of scaning through 1M training examples, we loop over i (features) to update for all j from 1 to n, using 1 training example\n",
    "- this never truely converge\n",
    "- but makes very faster progress\n",
    "- Mini-Batch Gradient Descent\n",
    "\n",
    "<img src=\"./images/02_sgd.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718d33b",
   "metadata": {},
   "source": [
    "## Normal Equation \n",
    "- If our goal is to solve linear regression, we dont need to run the process iteratively\n",
    "- we can solve for the optimal value of parameter $\\theta$ straight-away\n",
    "- this works only for linear regression and not anything else\n",
    "<br>\n",
    "\n",
    "- Partial derivative of cost function\n",
    "  - $ \\nabla_{\\theta} J(\\theta) $ - derivative of $J(\\theta)$ wrt to $\\theta$, where $\\theta \\in \\mathbb R ^{n+1}$. In our case with $ \\theta _{0} \\space, \\theta _{1} and \\space \\theta _{2} $, we have 3 dimensions of $\\mathbb R$, i.e., $\\theta \\in \\mathbb R^{n+1}$  \n",
    "\n",
    "> $\\begin{equation*}\n",
    "\\nabla_{\\theta} J(\\theta)  = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial \\theta_{0}}  \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_{1}}  \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_{2}}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f91e70",
   "metadata": {},
   "source": [
    "### How to find the global minima\n",
    "\n",
    "> $$ \\nabla_{\\theta} J(\\theta) \\stackrel{set}{=} \\overrightarrow{\\rm 0} $$  \n",
    "- solving this gives you a global minima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11663d75",
   "metadata": {},
   "source": [
    "### Matrix trace properties:\n",
    "* tr(A) = tr A = sum of diagonal entries = $\\sum_{i}A_{ii}$\n",
    "* $tr A = tr A^{T}$\n",
    "* If $ f(A) = tr AB $, then $ \\nabla _{A} f(A) = B^{T}$\n",
    "* tr AB = tr BA \n",
    "* tr ABC = tr CAB - by cyclic permutation property\n",
    "* $ \\nabla _{A} tr AA^{T}C = CA + C^{T}A $\n",
    "  * The above is analogous to $\\frac{d}{da}a^{2}c = 2ac $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b3a5a",
   "metadata": {},
   "source": [
    "### How to solve for $\\theta$\n",
    "\n",
    "- Let the cost function \n",
    "> $$ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^{2} $$  \n",
    "\n",
    "- and design matrix \n",
    "> $$\\begin{equation*}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "--(X^{(1)})^{T}--  \\\\\n",
    "--(X^{(2)})^{T}--  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "--(X^{(m)})^{T}--  \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$$  \n",
    "\n",
    "- and \n",
    "$$\\overrightarrow{\\rm y} = \n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}  \\\\\n",
    "y^{(2)}  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "y^{(m)}  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$$  \n",
    "\n",
    "- then  \n",
    "> $$\\begin{equation*}\n",
    "X \\theta = \n",
    "\\begin{bmatrix}\n",
    "(X^{(1)})^{T}\\theta  \\\\\n",
    "(X^{(2)})^{T}\\theta  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "(X^{(m)})^{T}\\theta \n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "h_{\\theta}(X^{(1)})  \\\\\n",
    "h_{\\theta}(X^{(2)})  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "h_{\\theta}(X^{(m)})  \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "- Sum of all the errors the algorithm is making between prediction and actual for m training examples  \n",
    "= Sum of the residuals =  \n",
    "> $$\\begin{equation*}\n",
    "X \\theta - y = \n",
    "\\begin{bmatrix}\n",
    "h_{\\theta}(X^{(1)}) - y^{(1)}  \\\\\n",
    "h_{\\theta}(X^{(2)}) - y^{(2)}  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "h_{\\theta}(X^{(m)}) - y^{(m)} \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "- So, we can write:  \n",
    "> $$ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^{2} $$  \n",
    "> $$ = \\frac{1}{2}(X\\theta - y)^{T}(X\\theta - y) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e2e28",
   "metadata": {},
   "source": [
    "- Substituting\n",
    "$ \\nabla_{\\theta} J(\\theta)  \\\\\n",
    "= \\nabla_{\\theta} \\frac{1}{2}(X\\theta - y)^{T}(X\\theta - y)  \\\\\n",
    "= \\frac{1}{2} \\nabla_{\\theta} (\\theta^{T}X^{T} - y^{T})(X\\theta - y)  \\\\\n",
    "= \\frac{1}{2} \\nabla_{\\theta} (\\theta^{T}X^{T}X\\theta - \\theta^{T}X^{T}y - y^{T}X\\theta + y^{T}y)  \\\\\n",
    "$ using matrix derivative $ \\\\\n",
    "= \\frac{1}{2} (X^{T}X\\theta + X^{T}X\\theta - X^{T}y - X^{T}y) \\\\\n",
    "= (X^{T}X\\theta - X^{T}y) \\stackrel{set}{=} \\overrightarrow{\\rm 0} $  \n",
    "which results in  \n",
    "$ X^{T}X\\theta = X^{T}y $ - which is called __\"Normal equation\"__  \n",
    "> $ \\theta = (X^{T}X)^{-1}X^{T}y $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eed2f5",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Implement GD/SGD/MBGD with Keras/Tensorflow/PyTorch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22472d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: \n",
      " 152.91886182616113\n",
      "Coefficients: \n",
      " [938.23786125]\n",
      "Mean squared error: 2548.07\n",
      "Coefficient of determination: 0.47\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code source: Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The intercept\n",
    "print(\"Intercept: \\n\", regr.intercept_)\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test, color=\"black\")\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color=\"blue\", linewidth=3)\n",
    "\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92eec49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance:  0.5349\n",
      "mean_squared_log_error:  0.2171\n",
      "r2:  0.4726\n",
      "MAE:  41.2271\n",
      "MSE:  2548.0724\n",
      "RMSE:  50.4784\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    \n",
    "regression_results(diabetes_y_test, diabetes_y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
