{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330dce23",
   "metadata": {},
   "source": [
    "# Introduction to Kafka/ksqldb/stream processing\n",
    "\n",
    "- https://www.youtube.com/watch?v=-kFU6mCnOFw\n",
    "\n",
    "## Motivation for event streaming\n",
    "- two ways of connecting applications\n",
    "  - ETL \n",
    "    - pros\n",
    "      - large batch processing\n",
    "    - cons\n",
    "      - gave a retro view of data post extraction, processing and loading\n",
    "  - Messaging \n",
    "    - pros\n",
    "      - real time processing\n",
    "    - cons\n",
    "      - messages are processed in isolation\n",
    "      - no historical context\n",
    "  - What kafka tries to solve\n",
    "    - brings both world together\n",
    "      - high throughput like in ETL world\n",
    "      - solves messages in real time in low latency fashion\n",
    "    - other motivation\n",
    "      - constant feed about activities of friends\n",
    "      - real time news feeds, accessible online anytime, anywhere\n",
    "      - entering in company level\n",
    "\n",
    "### Event vs Message\n",
    "- Event\n",
    "  - is a lightweight notification of a condition or a state change\n",
    "  - is immutable, so an event cannot be changed\n",
    "  - consumer decides what to do, publisher has no expectation\n",
    "- Message\n",
    "  - there is a definitive expectation as what will happen with the message\n",
    "  - who will consume the message, what will be done, is acknowledged\n",
    "  - publisher has an expectation about how the consumer will handle the message\n",
    "\n",
    "### Confluent Journey\n",
    "- _Event_ holds way more information that what the _state_ does\n",
    "  - \"I changed my job from Snaplogic to Confluent in April 2019\" - event\n",
    "  - \"I work at Confluent\" - state\n",
    "- but there was data storage for events\n",
    "- Hadoop had a file system, database has tables, but nothing was there was events\n",
    "- this was how idea of Kafka came into existence\n",
    "\n",
    "\n",
    "- this allows customer rich experience\n",
    "- When will the driver arrive?\n",
    "  - combines real time data with historical data\n",
    "  - only event streaming platforms can do this\n",
    "- Event driven app vs contextual event driven app\n",
    "  - contextual event driven app combines historical data to make precise prediction/ETA \n",
    "  \n",
    "<p align=\"center\"><img src=\"./images/event_driven_app.png\"></p>\n",
    "\n",
    "$\\tiny{\\text{Introduction to ksqldb and stream processing - Vish Srinivasan}}$\n",
    "\n",
    "### Capital One\n",
    "- contextual event-driven (event streaming platform) apps in the enterprise\n",
    "  - real-time fraud notifications\n",
    "  - real time \"Second Look\"\n",
    "    - if you swipe your card twice, or if you buy two burgers one after another\n",
    "    - gives a notifications\n",
    "  - automated transaction analysis\n",
    "\n",
    "### Take Away 1\n",
    "- Event streeaming platforms lets you build contextual event driven applications combining real time and historical data\n",
    "\n",
    "### Event Streaming Platform\n",
    "- 3 key functionalities\n",
    "  - publish/subscribe to events\n",
    "  - store events\n",
    "    - persists events\n",
    "    - key differentiator vs any other event platform\n",
    "  - process and analyze events\n",
    "  \n",
    "## Kafka 101\n",
    "### Event streams\n",
    "\n",
    "- at its core Kafka is a commit log\n",
    "- from _publisher_ point of view\n",
    "  - it always append data to the end of the log\n",
    "  - writing data is immutable\n",
    "    - we cannot change existing state\n",
    "- from _consumer_ point of view \n",
    "  - consumer can read from anywhere in the log\n",
    "  - consumers keep track of offset, of upto where they have read\n",
    "  - system A can be a batch ETL process which says give me all the events that happened\n",
    "  - system B can be upto date with events as and when it is happening\n",
    "\n",
    "### Store Events - Distributed and Replicated\n",
    "\n",
    "- 3 things to consider\n",
    "  - __Producer__ writes data into Kafka\n",
    "  - __Consumer__ consumes these messages\n",
    "  - data is stored in partitions\n",
    "    - __Partition__ is fundamental to Kafka\n",
    "      - partition is the reason why/how elasticity, scalability, fault-tolerance is achieved\n",
    "      - machines only understand partitions\n",
    "    - __Topic__ is the logical grouping of partition \n",
    "      - topic holds similar event types\n",
    "      - topic is meant for humans to understand\n",
    "- in the example below, topic has 2 partitions those 2 partitions have then 3 replicas each\n",
    "- this can then be distributed across __Broker__\n",
    "- this allows fault tolerance\n",
    "  - if Broker 2 goes down, it has replica in 2 other systems\n",
    "  - this allows data recovery and prevents data loss\n",
    "- Kafka leverages _page cache network socket_ very heavily\n",
    "  - this is the reason why inter-broker communication is so fast and happens in seamless manner\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/kafka_topics_partitions.png\"></p>\n",
    "\n",
    "$\\tiny{\\text{Introduction to ksqldb and stream processing - Vish Srinivasan - Storage: Distributed and Replicated}}$\n",
    "\n",
    "### Producing to Kafka\n",
    "- data is written to end of the log\n",
    "- but the log itself has 4 different partitions\n",
    "- if not specified, Kafka would distribute data in a round-robin manner\n",
    "- data is always written into the leader of the partition\n",
    "\n",
    "- similar events are written into the same partition\n",
    "- Kafka guarantees order only within a partition\n",
    "- it doesnot guarantees across partition\n",
    "- how do you make sure that similar events goes to the same partition\n",
    "  - using key\n",
    "    - every message written to Kafka has 3 things\n",
    "      - key, message and timestamp\n",
    "      - can leverage key for something that is domain specific\n",
    "      - key can be a userid or device id\n",
    "      - reading from partition preserves the order and does not read from across partitions\n",
    "      \n",
    "\n",
    "<p align=\"center\"><img src=\"./images/kafka_producing.png\"></p>\n",
    "\n",
    "$\\tiny{\\text{Introduction to ksqldb and stream processing - Vish Srinivasan - Producing to Kafka}}$\n",
    "\n",
    "### Consuming from Kafka\n",
    "#### Consuming using a single client\n",
    "\n",
    "- a client can consume from all the partitions at once\n",
    "- this is scalable\n",
    "- but there are changes that we read data that is written afterwards\n",
    "- to make sure data is written in particular order, it must be ensured that data goes into the same partition\n",
    "\n",
    "#### Consuming with consumer groups\n",
    "\n",
    "- partition is good way to scale applications\n",
    "- while using Kafka, the limit is on the network bandwidth\n",
    "- Kafka allows to create multiple consumers\n",
    "- there can be scenarios where same data should not be processed by two different consumers\n",
    "- using consumer groups\n",
    "  - resolves this problem \n",
    "  - it reads the data and automatically load balance the data across various consumers\n",
    "  - so each partition is read by different consumer\n",
    "  - if the consumer goes down, then this partition will automatically be reassigned to another consumer within the same consumer group\n",
    "  - __Consumer group__ is logical grouping within consumers\n",
    "  \n",
    "\n",
    "<p align=\"center\"><img src=\"./images/kafka_consuming1.png\"></p>\n",
    "\n",
    "$\\tiny{\\text{Introduction to ksqldb and stream processing - Vish Srinivasan - Consuming to Kafka}}$\n",
    "\n",
    "#### Consuming with consumer groups\n",
    "- multiple consumer groups allows you to read same data by multiple applications\n",
    "- Kafka can be applied across different models\n",
    "  - a consumer group mimicks very much similar to _message queue_\n",
    "  - but if there are multiple consumer group, it works like _publish/subscribe model_\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/kafka_consuming2.png\"></p>\n",
    "\n",
    "$\\tiny{\\text{Introduction to ksqldb and stream processing - Vish Srinivasan - Consuming to Kafka}}$\n",
    "\n",
    "### Delivery Guarantees\n",
    "- Producer Guarantees\n",
    "  - Acks = 0\n",
    "    - don't care if data has been written or not\n",
    "  - Acks = 1\n",
    "    - ensures that atleast gets that message\n",
    "    - leader can then partition as and when needed\n",
    "    - get acknowledgement back as soon as the leader has the data\n",
    "  - Acks = all\n",
    "    - will wait till data has been written to all the different partitions\n",
    "    - durability is important, but latency will get a hit\n",
    "- Consumer Guarantees\n",
    "  - at least once\n",
    "    - makes sure that you always process the data\n",
    "    - makes sure that the data is not dropped\n",
    "  - at most once\n",
    "    - consuming application is not idempotent\n",
    "    - message that is read is read at most once, could be zero, but wont have any duplicates\n",
    "  - exactly once\n",
    "    - best of both world\n",
    "\n",
    "### Take Away 2\n",
    "- Kafka lets you publish/subscribe to events as well as store event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe45b6c2",
   "metadata": {},
   "source": [
    "\n",
    "## Stream Processing\n",
    "### Kafka streams\n",
    "    \n",
    "### Process & Analyze events\n",
    "- Kafka topics work very much like unix pipes\n",
    "- kafka connect is easy way of getting data in and out of these pipes\n",
    "- but along with moving data in and out, some processing is also needed\n",
    "  - that is what stream processing does\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/kafka_stream_processing.png\"></p>\n",
    "\n",
    "$\\tiny{\\text{Introduction to ksqldb and stream processing - Vish Srinivasan - Stream Processing}}$\n",
    "\n",
    "- two ways of doing stream processing in Kafka\n",
    "  - kafka streams\n",
    "    - Java library that you embed directly into your application\n",
    "    - no separate processing cluster\n",
    "    - this is embedded directly into your system\n",
    "    - allows both stateful and stateless stream processing\n",
    "      - stateful\n",
    "        - aggregate/join where we need to remember and maintain state\n",
    "      - stateless\n",
    "        - filters like events where you need to apply probability filter through \n",
    "  - Confluent KSQL\n",
    "    - streaming SQL engine for kafka\n",
    "    - SQL interface that sits on top of Kafka interface\n",
    "    - similar to SQL syntax\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/kafka_streams_vs_confluent_ksql.png\"></p>\n",
    "\n",
    "$\\tiny{\\text{Introduction to ksqldb and stream processing - Vish Srinivasan - kafka streams vs Confluent KSQL}}$\n",
    "\n",
    "### Topics vs Streams and Tables\n",
    "- data that is written into topics and partitions is in the format of bytes\n",
    "- it is raw byte array\n",
    "- to be able to do this processing layer is built on top of it\n",
    "  - both have \"stream and table\" built on top of it\n",
    "  - a stream \n",
    "    - is a topic with schema that allows you to serialize and deserialize the data (serdes)\n",
    "    - alice and bob visits to 3 cities\n",
    "    - _streams record history_\n",
    "      - in chess will record sequence of moves\n",
    "  - a table\n",
    "    - along with stream it is also stored as table (this is quite different from table in SQL)\n",
    "    - kafka table works like a materialized aggregatted view \n",
    "    - _tables represent state_\n",
    "      - in chess will store state of board\n",
    "      \n",
    "- topics are stored in partitions to boost scalability\n",
    "- on the same lines of storage, processing is also partitioned\n",
    "- unit of parallelism\n",
    "  - in a stream __stream-task__\n",
    "    - one to one mapping between stream and partition\n",
    "    - as more number of partitions are added, scaling can be achieved\n",
    "  - in a table __underlying topic (generally) compacted__\n",
    "    - if there are keys and values and for same key there are multiple values, only the latest key-value is stored\n",
    "      - older key-values are dropped\n",
    "    - useful in cases of total bank balance, only the total is needed\n",
    "    - ever table has a _state-store_ (mutable)\n",
    "      - even if you loose the state-store, it can be recreated from the underlying\n",
    "      \n",
    "### Take-away 3\n",
    "- 2 tools to process data \n",
    "  - Kafka streams and KSQL\n",
    "- 2 concepts in both\n",
    "  - Streams and Tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c9e0c",
   "metadata": {},
   "source": [
    "## ksqldb - An introduction\n",
    "\n",
    "### KSQL\n",
    "- used for real-time monitoring\n",
    "  - log data monitoring\n",
    "  - tracking and alerting\n",
    "  - syslog data\n",
    "  - sensor/IoT data\n",
    "  - application metrics\n",
    "    - example below is of creating a stream\n",
    "  \n",
    "```sql\n",
    "CREATE STREAM syslog_invalid_users AS\n",
    "SELECT host, message\n",
    "FROM syslog\n",
    "WHERE message LIKE '%Invalid user%'\n",
    "```\n",
    "\n",
    "- used for anomaly detection\n",
    "  - identify patterns or anomalies in real time data, surfaced in milliseconds\n",
    "    - example below is of creating a table\n",
    "    - if user has swiped more than 3 times in last 5 seconds\n",
    "      - if so, creates a possible_fraud table\n",
    "\n",
    "```sql\n",
    "CREATE TABLE possible_fraud AS\n",
    "SELEECT card_number, COUNT(*) \n",
    "FROM authorization_attempts\n",
    "WINDOW TUMBLING (SIZE 5 SECONDS)\n",
    "GROUP BY card_number\n",
    "HAVING COUNT(*)>3;\n",
    "```\n",
    "\n",
    "### What is KSQL not good for?\n",
    "- there are no indexes in KSQL\n",
    "- for ad-hoc query\n",
    "  - suppose there is a user id inside of a message\n",
    "    - interested in searching for all the information by that user id\n",
    "    - as the messages are not indexed on user id\n",
    "  - configured to retain data for only a limited span of time\n",
    "- BI reports\n",
    "  - view only static data\n",
    "  - not dynamic changing data in real time\n",
    "    - JDBC in future will support live-streaming updates\n",
    "\n",
    "### Where is KSQL evolving?\n",
    "#### Pull queries\n",
    "\n",
    "- in KSQL, if you do a query, it is a continuous running query\n",
    "- so if a new change comes over, it gets included in the query result\n",
    "\n",
    "- push query\n",
    "  - entire history\n",
    "\n",
    "```sql\n",
    "SELECT user, credit_score\n",
    "FROM credit_history\n",
    "WHERE ROWKEY = 'jay'\n",
    "EMIT CHANGES;\n",
    "```\n",
    "\n",
    "- pull query\n",
    "  - only latest state\n",
    "  - pull query will always show the latest result as and when new message comes\n",
    "  - point-in-time lookup of that information\n",
    "  \n",
    "```sql\n",
    "SELECT user, credit_score\n",
    "FROM credit_history\n",
    "WHERE ROWKEY = 'jay';\n",
    "```\n",
    "\n",
    "#### Embedded Connectors\n",
    "- combine information from extternal data systems as well\n",
    "- can say give informatiton from Splunk or Sumologic or any others data sources\n",
    "- list of open source and enterprise connectors are available\n",
    "\n",
    "### What use case is ksqldb good for?\n",
    "- it does not replace traditional databases\n",
    "  - raw storage is a commit log\n",
    "    - if you loose your database, you replay the log and recover it back\n",
    "  - materialized events into opinionated structure (table) in the form of SQL\n",
    "    - take a subset of commit log and making it accessible to others\n",
    "    - active data vs passive data\n",
    "      - data is sitting passively unless you query it\n",
    "  - in a query, state of query is produced by the processor executing the commit log\n",
    "    - same is done in Kafka in the form of materialized views or materialized cache\n",
    "- useful for 3 categories of applications\n",
    "  - building and serving materialized views that power apps\n",
    "  - creating real-time streaming apps that react to event streams and trigger side effects\n",
    "    - joins against streams\n",
    "    - trigger based on events that happen\n",
    "  - creating real-time streaming pipelines that continuously transform event streams\n",
    "    - real time stream ETL\n",
    "\n",
    "### Take Away 4\n",
    "- ksqldb makes it easy to build and serve materialized views that power apps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
