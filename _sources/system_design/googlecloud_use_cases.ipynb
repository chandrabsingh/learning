{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07305e1",
   "metadata": {},
   "source": [
    "# Google - Customer story\n",
    "\n",
    "## How do Pokemon Go scale to millions of request\n",
    "\n",
    "<img src=\"images/pokemonGo.png\">\n",
    "\n",
    "### How does Pokemon scale?\n",
    "- uses Google cloud - Google Kubernetes Engine and Spanner\n",
    "- front end service hosted on GKE, as it is easy to scale\n",
    "- uses 5000 spanner nodes to handle traffic\n",
    "- uses 1000 GKE nodes just for Pokemon Go service\n",
    "- uses microservices for gaming augmentations\n",
    "- millions of players play the same game at any given moment, unlike other multiplayer games like \"World of Warcraft\" which split players into multiple realms\n",
    "\n",
    "### Was the architecture always the same?\n",
    "- starting off, it started on Google Data Store \n",
    "- later it decided to scale\n",
    "- Spanner was a choice, as it has consistent indexing that allows to do more complex database schemas with primary and secondary keys\n",
    "- Data store was also non-relational with atomic and durable transactions \n",
    "- requirement was to have a relational database with full consistency, so Spanner was chosen, which provided global ACID transactions\n",
    "\n",
    "### How does the request flow work when one logs into Pokemon Go?\n",
    "- when a user catches a Pokemon, a request is received through Google Cloud load balancer \n",
    "- all static media is stored on Google cloud storage bucket, which is downloaded onto the phone when the app is opened on phone\n",
    "- the load balancers are cached and served through Google cloud SDN\n",
    "- when a player catches the Pokemon, the GCLB sends a request to GKE cluster\n",
    "- there is a front end service that sits behind a Nginx reverse proxy \n",
    "- the request goes from user phone to reverse proxy to one of these player front end services\n",
    "- there is a spatial query backend, which a cache by location, which stores information that determines \n",
    "  - where a Pokemon is shown on map, \n",
    "  - where are the gyms \n",
    "  - where are the Pokestops around, \n",
    "  - what is the timezone or any other feature locationwise  \n",
    "- the front end manages the player and their interaction with the game\n",
    "- the spatial query backend handles the map\n",
    "- the front end retrieves the information from the squibby to send back to the user\n",
    "- the spatial query is a custom database that presents the map and the Pokemons\n",
    "\n",
    "\n",
    "### What happens when a Pokemon is hunted and caught?\n",
    "- a request is sent from the front end to the Spanner database where player entity is stored in the store \n",
    "- if someone is battling in the gym or adding lures enhancing Pokestop, that information is stored on spatial query backend\n",
    "- eventually the data gets consistent, at which point the player will receive the update and then other players in the vicinity will get update as well\n",
    "- the front end then will retrieve the information from the spatial query backend and send it back to the user\n",
    "- this information is also stored using protobuf into Bigtable\n",
    "- Bigtable is also used for logging and tracking purposes \n",
    "- Pub/Sub is also used to send topic which is then used for analytics pipeline\n",
    "\n",
    "### How is it ensured that two people in the same geographic location, see the same Pokemon data and keep that relatively in sync, especially during the events?\n",
    "- everything in the servers are deterministic\n",
    "- if they are in same locations, both the players will get returned the same details\n",
    "- this is achieved using caching which manages precise timing especially when the settings are changed and they need to be in sync across all servers\n",
    "\n",
    "### How does the data pipelines work and what analytics happen?\n",
    "- about 10G of data is generated daily and stored in BigQuery\n",
    "- this data is utilized by data science team\n",
    "- this is used for marketing purposes, for verification\n",
    "- Dataflow is used to run batch jobs to process the player logs sitting in Bigtable\n",
    "- this is used for cheat detection \n",
    "  - a player in Japan and a minute later they were in Australia\n",
    "  - check for such logs and respond to improper player signals\n",
    "- Google Dataflow is also used to set up Pokestops and gyms and habitat information around the world\n",
    "- information from various sources are used, like OpenStreetMap, US Geological survey and WayFarer, a crowdsourced website, for players to submit points of interests in their neighborhood and combine all that information to build a living map of the world\n",
    "\n",
    "### As the traffic grows during events, how does the data pipelines scale?\n",
    "- with the increase of transactions in pipelines, is managed by BigQuery\n",
    "  - BigQuery's serverless architecture decouples storage and compute and allows them to scale independently on demand\n",
    "- scaling works premptively on Google cloud to support the traffic\n",
    "\n",
    "### How is the health of the system monitored during these massive events?\n",
    "- multiple monitoring systems like Prometheus, Grafana, and Google Cloud Monitoring is used\n",
    "\n",
    "### What are the next plans?\n",
    "- exploring Agones and Google Game Servers\n",
    "\n",
    "### References\n",
    "- https://www.scaleyourapp.com/distributed-systems-scalability-part1-heroku-client-rate-throttling/\n",
    "- https://cloud.google.com/blog/topics/developers-practitioners/how-pok%C3%A9mon-go-scales-millions-requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e6a87d",
   "metadata": {},
   "source": [
    "## How does Uber scale to millions of concurrent requests\n",
    "\n",
    "### Uber Request Flow\n",
    "\n",
    "<img src=\"./images/uberRequestFlow.png\">\n",
    "\n",
    "- Fulfillment platform\n",
    "  - goes through series of checks and balances and then triggers fulfillment to create new order for consumer\n",
    "  - order is the intent of consumer\n",
    "  - intent is then translated into jobs that needs to be processed\n",
    "  - this information is stored in spanner\n",
    "  - these jobs are read by matching engine\n",
    "  - offered to any open provider or supplier nearby\n",
    "  - earlier all of this data was saved in **on-prem database** and recently moved to **Spanner**\n",
    "\n",
    "- Challenges with on-prem database\n",
    "  - No-SQL storage engine **Cassandra** was used to save all real time fulfillment entities\n",
    "  - to maintain serializability on top of Cassandra, **Ringpop** was used to provide individual **entity-level serialization**\n",
    "  - Challenges with NoSQL\n",
    "    - Cassandra follows the principle of **eventual consistency**\n",
    "    - there is **no guarantee of low-latency quorum writes with Cassandra**\n",
    "    - witnessed complex storage interactions that required multi-row and multi-table writes.\n",
    "    - to tackle this, an application layer framework was build to orchestrate this operation using saga pattern\n",
    "    - these inconsistencies had to be manually corrected\n",
    "    - in real world - two different drivers are dispatched to a customer\n",
    "  - Horizontal scaling bottlenecks were observed in overall architecture\n",
    "    - this was primarily because how application layer was sharded with Ringpop\n",
    "    - properties of traditional SQL-based storage ACID guarantees were observed\n",
    "    - consistency was the primary requirement\n",
    "    - NoSQL requirements with strong ACID guarantees were needed\n",
    "- Google Spanner was the solution\n",
    "  - what were the explorations for transitioning from NoSQL to NewSQL\n",
    "    - Consistency with high resilency and availability were the requirement criterias\n",
    "    - CockroachDB, FoundationDB and Cloud Spanner were considered\n",
    "    - functional requirements were fulfilled\n",
    "      - scaled horizontally with our benchmarks, and provided a managed solution for cluster management and maintenance\n",
    "    - what is the new architecture\n",
    "      - every user request results in a transaction against one or more rows and across one or more tables in cloud Spanner\n",
    "      - consistent view of data is available both to internal and external customers\n",
    "\n",
    "### Uber Fulfillment Architecture\n",
    "\n",
    "<img src=\"./images/uberNewFulfillmentArchitecture.png\">\n",
    "\n",
    "- latency is the biggest factor, what is the networking architecture looks like \n",
    "  - resilient and highly reliable infrastructure to support Uber's workload\n",
    "  - networking infrastructure was divided into 2 major components\n",
    "    - physical layer consisting of interconnections between Uber and Cloud vendors\n",
    "    - logical layer consisting of virtual connections on top of physical layer to achieve redundancy\n",
    "      - this is achieved by making multiple routers and having local access points at each physical network route\n",
    "      - leverage Google's private Google access to route using cloud interconnect VLAN attachments\n",
    "      - reduced the need of routing traffic through public internet and provided additional reliability\n",
    "      - benchmarking tests were designed     \n",
    "\n",
    "### Uber Low Latency Cloud Architecture\n",
    "\n",
    "<img src=\"./images/uberLowLatencyNetwork.png\">\n",
    "\n",
    "- how was the migration and transition made seemless\n",
    "  - database topology was significant different from old platform, any kind of live migration of data was ruled out\n",
    "  - most of the data is ephemeral and changing continuously, backups would have resulted in loss of data\n",
    "  - systems were built that will intercept request from user session and attach to the new user session \n",
    "  - till the past order is not complete, this migration is not done\n",
    "  - only open user session with no active orders were migrated to new architecture\n",
    "  - tooling were tested rigorously in testing, staging and shadow environments\n",
    "  - test cities were developed to check migrations with one city at a time\n",
    "  - continuously migrate over 6 month period\n",
    "\n",
    "  \n",
    "- Performance Optimization\n",
    "  - networking stack\n",
    "    - TCP Reset\n",
    "      - the spanner session needs to be re-established for the forwarded requests\n",
    "      - by optimizing gRPC channel pool whenever a channel is affected by TCP resets, requests are automatically forwarded temporarily to backup empty gRPC channel\n",
    "    - Intermittent packet loss\n",
    "      - with frequent health checks, broken TCP connections can be detected within 5 sec and graceful connections can be triggered\n",
    "  \n",
    "- To take advantage of cloud Spanner elasticity, autoscaler was built\n",
    "  - constantly tunes the number of nodes based on CPU utilization target\n",
    "  - as traffic is variable based on constant CPU utilization target, maximum elasticity is achieved from Cloud spanner cluster \n",
    "\n",
    "### Uber Auto scaler\n",
    "\n",
    "<img src=\"./images/uberAutoscaler.png\">\n",
    "\n",
    "- On-prem cache was made to handle workload which is very read-heavy\n",
    "  - improves latency and cost\n",
    "  - only route stale reads to Spanner \n",
    "  - allow cache to serve snapshot isolation based on Spanner's queue time\n",
    "\n",
    "### Uber On-prem Cache\n",
    "\n",
    "<img src=\"./images/uberOnpremcache.png\">\n",
    "\n",
    "\n",
    "### READ\n",
    "- \"How Ringpop from Uber Engineering helps distribute your application\"\n",
    "- Saga pattern - in microservices architecture it tracks all events of distributed transactions as a sequence and decides the rollbacks events in case of a failure\n",
    "- Planet scale\n",
    "- Strong read and Stale read\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a584cec",
   "metadata": {},
   "source": [
    "## How to create a microservice architecture with Google Cloud - Nylas\n",
    "\n",
    "### Legacy Architecture\n",
    "\n",
    "<img src=\"./images/nylasOldArchitecture.png\">\n",
    "\n",
    "- Workflow Automation \n",
    "  - by collecting data from multiple applications \n",
    "  - compiling insights\n",
    "  - create end-to-end automation workflow\n",
    "\n",
    "- handle communication data driving business process automation\n",
    "\n",
    "- Problems to tackle \n",
    "  - security\n",
    "  - scalability\n",
    "  - performance\n",
    "  - cost\n",
    "  \n",
    "- Why Google Cloud\n",
    "  - best distributed technology\n",
    "  - GKE - Google Kubernetes Engine is the best cloud provider to run Kubernetes workload securely at scale \n",
    "  - one of the most performant data stores in Spanner\n",
    "  \n",
    "- Architecture  \n",
    "  - the advantage of using **GKE** is its ability to **handle the container orchestration**, **Cloud Pub/Sub for message bus** and **Cloud Spanner for relational data store**   \n",
    "  - Spanner processes 1B requests per second at peak\n",
    "  - latency of less than 7 milliseconds\n",
    "  - GCP services are very robust\n",
    "\n",
    "- Transition \n",
    "  - from Python based application to Go - code rewrite\n",
    "    - services were rewritten in Go \n",
    "    - 10x throughput improvement was observed\n",
    "  - move from virtual machine instances on AWS EC2 to GKE \n",
    "    - move the orchestration from **AWS autoscaler group to GKE**\n",
    "    - GKE was used to orchestrate containers \n",
    "    - due to security reasons, nodes have to be cycled very quickly\n",
    "    - were able to have upto 15000 nodes in Kubernetes cluster \n",
    "    - other cloud providers dont provide this level of support\n",
    "    - **gVisor** is used to run containers to create strong isolation between application and operating system\n",
    "    - helps lock down host memory and storage access and enforce least permission principle at operating system level\n",
    "    - **gVisor is an application kernel**, written in Go, that implements substantial portion of Linux system call interface. It provides an additional layer of isolation between running applications and host operating system    \n",
    "    \n",
    "- 20TB of data processing in MySQL shards\n",
    "  - Cloud spanner was used to keep application states \n",
    "  - store keys for each accounts that needs to be connected\n",
    "  - fast read/write is extremely critical for large number of calls\n",
    "  - high performance to have predictable end-to-end latency\n",
    "\n",
    "### Current Architecture\n",
    "\n",
    "<img src=\"./images/nylasNewArchitecture.png\">\n",
    "\n",
    "- PII(Personally Identifiable Information)\n",
    "  - Nylas does not have to hold PII\n",
    "  - for security services\n",
    "  \n",
    "- Migrated from database centric design to event based design\n",
    "  - message bus is the heart of all application\n",
    "  - to make this transition, Google Pub/Sub was choosen\n",
    "  - Google Pub/Sub allows services to communicate asynchronously with latencies on the order of 100 milliseconds\n",
    "  - in legacy AWS architecture which was more of data driven architecture\n",
    "    - the data is written into **Dynamo**\n",
    "    - then gets converted by **lambda function**\n",
    "    - then goes to **Kinesis event stream**\n",
    "  - in new architecture, this is simplified\n",
    "    - event it first\n",
    "    - rewrote it directly on Google Pub/Sub\n",
    "    - puts lot of strain on system, with trust that this will get scale and perform reliably and consistently\n",
    "    - on **Google Pub/Sub, the performance latency is minimal**\n",
    "    - event bus can act as data store\n",
    "    \n",
    "- Benefits of migrating\n",
    "  - **30 times performance on each node** was observed\n",
    "  - **Elasticity of architecture** is amazing\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9f758",
   "metadata": {},
   "source": [
    "## How to architect an AI/ML powered healthcare platform on Google Cloud - Vida Health\n",
    "### Data Analytics Architecture\n",
    "\n",
    "<img src=\"./images/vidaDataAnalyticsArchitecture.png\">\n",
    "\n",
    "- provides comprehensive virtual care solution tailored to personalized health goals and preferences\n",
    "- what are the challenges in building such application\n",
    "  - data coming from different sources of different types \n",
    "  - needs to be integrated seemlessly\n",
    "  - both for mobile and web users\n",
    "  \n",
    "- Why Google cloud\n",
    "  - 50% cost reduction\n",
    "  - developer productivity, one single place to look at\n",
    "  - focus on AI/ML\n",
    "  \n",
    "- Challenges with ML and AI\n",
    "  - make provider more efficient at their jobs\n",
    "  - use NLP and speech to text - take down notes for doctors/assistants\n",
    "  - how to get more patients\n",
    "    - depending on severity and acuity of patients \n",
    "    - give signals to providers\n",
    "    - prioritize such cases \n",
    "    \n",
    "- Vidapedia - major recommendation platform - using Workspace and BigQuery. How does it work?\n",
    "  - Vidapedia \n",
    "    - set of protocols, as Google docs, written to help providers/clinicians \n",
    "    - 200 different protocols are created, not feasible to remember/memorize\n",
    "    - how to get such information at high speed\n",
    "  - Vidapedia recommendation system does on machine learning\n",
    "    - look into patient-provider interaction going on\n",
    "    - surface right protocol for such interaction\n",
    "    - ML model detects and triggers particular protocol during or after the session\n",
    "    - provider is able to give right care to patient\n",
    "  - this was possible using Google Doc protocols interaction with Google Workspace \n",
    "    - use this data, index them\n",
    "    - store it in BigQuery\n",
    "    - build ML tools on top of that\n",
    "    \n",
    "- Data pipeline architecture\n",
    "  - data comes from different sources\n",
    "  - providers provide consultation notes\n",
    "  - derive insights \n",
    "  - data gets ingested both in relational and unstructured format\n",
    "  - all these data is saved in BigQuery\n",
    "  - which is then used in analytics application deriving insights\n",
    "  - visualization tools such as data studio and looker\n",
    "  - ml models are built which is then sourced into visualization tools\n",
    "  - exploration in terms of VertexAI for quickly building these models and deploying them\n",
    "\n",
    "- Transformations\n",
    "  - in past custom ETL pipelines were designed, which is a challenge\n",
    "  - currently **dbt** is used for transformations\n",
    "  \n",
    "- how is the orchestration done\n",
    "  - **Cloud Composer** is used for orchestration the pipeline, scheduling all of this ingest, processing data, and running all the stuff\n",
    "\n",
    "- how is security requirement resolved\n",
    "  - patient data is very sensitive\n",
    "  - stringent privacy laws\n",
    "  - use **Google Data Loss Protection(DLP)**\n",
    "  - **mask PII data**\n",
    "  - reduce surface area\n",
    "  \n",
    "- what is the Web Architecture designed\n",
    "  - SADA practices - well known in the industry\n",
    "  - standard 3-tier architecture\n",
    "  - load balancers at the front\n",
    "  - deployed in Google Kubernetes engine\n",
    "  - middle API layer that returns all the request response\n",
    "  - at the end there is GCS Postgres database, scaling is easy \n",
    "  - easy to scale\n",
    "\n",
    "### Web Architecture\n",
    "\n",
    "<img src=\"./images/vidaWebArchitecture.png\">\n",
    "\n",
    "- What next\n",
    "  - Google Cloud Healthcare API\n",
    "    - provides managed solution for storing and accessing healthcare data in Google cloud\n",
    "  - Google FHIR - Fast Healthcare Interoperability Resources\n",
    "    - Explore and integrate google health services and api\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9603f",
   "metadata": {},
   "source": [
    "## How are the video monitoring system architectured - Arcules?\n",
    "\n",
    "### How does Arcules work?\n",
    "\n",
    "- Arcules takes information from multitude of IoT devices, captures it, processes it and then pushes it into the Google cloud\n",
    "\n",
    "<img src=\"images/arculesArch.png\">\n",
    "\n",
    "\n",
    "### What is video security as a service?\n",
    "\n",
    "- it is like a SaaS based product, that is storing videos. Its competitors are video management services(VMS)\n",
    "- the collected vides help finding incidences or people within video, all captured in cameras\n",
    "\n",
    "### What is the high level architecture\n",
    "\n",
    "- the customers have a small appliance at the edge that allows buffering information before sending it to the cloud\n",
    "- it can be deployed inside of bare metal or VMware\n",
    "- Kubernetes allows deploying multiple containers \n",
    "- last mile piece - allows near zero latency\n",
    "- have 90 microservices in GKE\n",
    "- different types of load balancers are used as HTTP/HTTPS and endpoint connections, along with instances of L4 with WebRTC\n",
    "- also use IoT core, which is used for metadata that flows up through some the services at the edge, which gets fed into pub/sub, which then gets fed into BigQuery\n",
    "\n",
    "### What type of databases are used?\n",
    "\n",
    "- different types of dbs are used\n",
    "- use cloud SQL, both Postgres and a bit of MySQL\n",
    "- graph database like Arango is used for NIST based permission graph, which makes permission way better for horizontally scaling\n",
    "- Singlestore is used for high rate of ingestion, with 400,000 inserts per second\n",
    "- Postgres is mostly used for device configuration storage\n",
    "- BigQuery is used for slow storage, which is powerful to run jobs across large datasets\n",
    "\n",
    "### Storing video chunks in cloud storage\n",
    "\n",
    "- videos are stored today inside of custom container format H.264\n",
    "- every videos have keyframes\n",
    "- these keyframes are chunked up, depending on the camera, will be one, two or four second\n",
    "- these informations are aggregated up, so that no extra spending is required on PUT requests\n",
    "- all the messages fly through pub/sub\n",
    "\n",
    "### How are the CI/CD microservices deployed\n",
    "\n",
    "- deployment is typically through Seneca, with build process through the cloud build platform\n",
    "- heavy users of container services for container management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153bc6bf",
   "metadata": {},
   "source": [
    "## CBS Interactive AI/ML Group\n",
    "- https://youtu.be/hX71H78UYAc\n",
    "\n",
    "### Functional Infrastructure\n",
    "\n",
    "<img src=\"images/googleFunctionalInfra.png\">\n",
    "\n",
    "### Content Awareness Architecture\n",
    "\n",
    "<img src=\"images/googleContentAwarenessProduct.png\">\n",
    "\n",
    "### Video Services\n",
    "\n",
    "- the video narratives are broken down into\n",
    "  - foreground information that drives the story forward\n",
    "  - background information that provides the setting\n",
    "\n",
    "<img src=\"images/googleFunctionalInfra.png\">\n",
    "\n",
    "### Recommendations and Search\n",
    "\n",
    "- goal - general purpose recommendations to brands with simple API interface\n",
    "- accomplished dynamically using Redis\n",
    "  - recommendation combination at request time is fast\n",
    "  - backend failures only result in delays in updating recommendations\n",
    "  \n",
    "<img src=\"images/googleGenReco.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132142cd",
   "metadata": {},
   "source": [
    "- these recommendation models must generate new recommendations without fear of breaking\n",
    "- the front end of these recommendation models must be distinctly different from the backend of already generated recommendations\n",
    "  - applying business logic on one and not the other\n",
    "- goal is to serve the best recommendation at any given time\n",
    "- Google cloud endpoints play a crucial role in this \n",
    "  - it handles authentication, monitoring as well as giving us a central place for API documentation\n",
    "- Admiral decides what recommendations are selected and what recommendations are returned\n",
    "  - this is the only choke point for reliability\n",
    "  - Admiral servees using JSON blobs\n",
    "  - business logics can be easily changed in Admiral\n",
    "- The ingest process starts with Trawler, which sources data into the content awareness system \n",
    "- this is then augmented with Cloud AI APIs and store it\n",
    "- GenReco is three cron jobs, which is an auto-scaling worker service\n",
    "- Data collector sources data from content awareness system which is augmented with the output of the cloud AI APIs\n",
    "  - it stores it in GenReco in a format that can be used\n",
    "- If every piece of compared against every other piece of content, it will be an intractable problem with lots of resources and many useless comparisons. GenReco submit helps in this\n",
    "  - first step is a filter to identify based on some initial processing, content that is most likely to be recommended\n",
    "  - they are then submitted as jobs to auto-scaling workers\n",
    "  - these workers then compute the actual similarity metrics\n",
    "  - they can then scale up when the content creation is high and scale down when no content is coming through\n",
    "- the last step is GenReco where the recommendations are actually created \n",
    "  - all the comparisons are pulled that are computed by workers \n",
    "  - generate the best recommendations of that particular time\n",
    "- the recommendations can have score, order, weight, interleaving\n",
    "- all this is required is to have models in Redis, which can be combined dynamically\n",
    "\n",
    "<img src=\"images/googleFutureDirection.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03472b2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "166.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
