
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>ML Conceptual brief</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Python FAQs" href="../codes/python_faqs.html" />
    <link rel="prev" title="My .zshrc script" href="../unix/my_zshrc.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/mylogo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    My learning notebook
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  System Design
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../system_design/intro.html">
   My ML Book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../system_design/design_patterns.html">
     Asynchronous request reply
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="intro.html">
   My ML Book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Store_Sales_Forecasting_With_Tensorflow.html">
     Store Sales Forecasting with TensorFlow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="decision_tree_classification.html">
     Decision Tree - classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_design_patterns.html">
     The Need for ML Design Patterns
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unix/Shell Scripting
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unix/intro.html">
   My Unix/Shell Book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unix/my_zshrc.html">
     My .zshrc script
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   ML Conceptual brief
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../codes/python_faqs.html">
   Python FAQs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../unix/unix_shell_script.html">
   Unix/Shell FAQs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markdown.html">
   Markdown Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks.html">
   Content with notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markdown-notebooks.html">
   Notebooks with MyST Markdown
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lecture Notes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../cs229_ml/intro.html">
   CS229 ML - by Andrew Ng
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec02-LinearReg-GradientDescent.html">
     Lec 02-Linear Regression - Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec03-LocallyWeighted-LogisticRegression.html">
     Lec 03-Locally Weighted Regression - Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec04-Perceptron-GLM.html">
     Lec 04-Perceptron - GLM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec05-GDA-NaiveBayes.html">
     Lec 05-GDA - Naive Bayes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec06-NaiveBayes-SVM.html">
     Lec 06-Naive Bayes - SVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec07-Kernels-SVM.html">
     Lec 07-Kernels - SVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec08-DataSplits-Models-CrossValidation.html">
     Lec 08-Data Splits - Models - Cross Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec09-Approx-EstimationError-ERM.html">
     Lec 09-Estimation Error - ERM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec10-DecisionTrees-EnsembleMethods.html">
     Lec 10-Decision Trees - Ensemble Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec11-Intro-NN.html">
     Lec 11-Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec12-Backpropagation-ImprovingNN.html">
     Lec 12-Improving NN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec13-DebuggingMLModels-ErrorAnalysis.html">
     Lec 13-Debugging ML Models-Error Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec14-Expectation-MaximizationAlgo.html">
     Lec 14-Expectation-Maximization Algo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec15-EMAlgo-FactorAnalysis.html">
     Lec 15-EM Algo-Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec16-IndependentComponentAnalysis-RL.html">
     Lec 16-Independent Component Analysis-RL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec17-MDPs-ValuePolicyIteration.html">
     Lec 17-MDPs-Value Policy Iteration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs229_ml/lec18-continuousMDPs-ModelSimulation.html">
     Lec 18-Continuous MDPs-Model Simulation
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/chandrabsingh/learning/master?urlpath=tree/learning/ml_examples/ml_glossary.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/chandrabsingh/learning"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/chandrabsingh/learning/issues/new?title=Issue%20on%20page%20%2Fml_examples/ml_glossary.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/ml_examples/ml_glossary.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning">
   Machine Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#supervised-learning">
     Supervised Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised-learning">
     Unsupervised Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reinforcement-learning">
     Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classification-algorithms">
   Classification algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probabilistic">
     Probabilistic
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#naive-bayes">
       Naive Bayes
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rule-based">
     Rule based
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decision-tree">
       Decision Tree
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gini-impurity">
       Gini Impurity
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#entropy">
       Entropy
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#information-gain">
       Information Gain
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#most-common">
   Most common
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression">
     Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logistic-regression">
     Logistic Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Decision Tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#svm">
     SVM
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#links">
       Links
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#explanation">
       Explanation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#questionnaires">
       Questionnaires
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Naive Bayes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-nearest-neighbor-knn">
     k-Nearest Neighbor (kNN)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-means">
     K-Means
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#algorithm">
       Algorithm
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-choose-optimal-k">
       How to choose optimal k
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-forest">
     Random Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dimensionality-reduction-algorithms">
     Dimensionality Reduction Algorithms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-boosting-algorithms">
     Gradient Boosting algorithms
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gbm">
       GBM
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#xgboost">
       XGBoost
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lightgbm">
       LightGBM
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#catboost">
       CatBoost
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regression-predict-a-value">
   Regression - Predict a value
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#boosted-decision-tree-regression">
     Boosted Decision Tree Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-forest-regression">
     Decision Forest Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fast-forest-quantile-regression">
     Fast Forest Quantile Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#local-linear-regression">
     Local Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#locally-weighted-linear-regression-lwr">
     Locally Weighted Linear Regression (LWR)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-network-regression">
     Neural Network Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#poisson-regression">
     Poisson Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#quantile-regression">
     Quantile Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classification-predict-a-class-choose-from-binary-two-class-or-multiclass-algorithms">
   Classification - Predict a class. Choose from binary (two-class) or multiclass algorithms.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiclass-boosted-decision-tree">
     Multiclass Boosted Decision Tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiclass-decision-forest">
     Multiclass Decision Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiclass-logistic-regression-softmax-regression">
     Multiclass Logistic Regression (Softmax Regression)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiclass-neural-network">
     Multiclass Neural Network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-vs-all-multiclass">
     One vs. All Multiclass
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-vs-one-multiclass">
     One vs. One Multiclass
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-class-averaged-perceptron">
     Two-Class Averaged Perceptron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-class-boosted-decision-tree">
     Two-Class Boosted Decision Tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-class-decision-forest">
     Two-Class Decision Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-class-logistic-regression">
     Two-Class Logistic Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-class-neural-network">
     Two-Class Neural Network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-class-support-vector-machine">
     Two Class Support Vector Machine
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binary-vs-multiclass-vs-multilabel-classification">
     Binary vs Multiclass vs MultiLabel classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-linear-regression">
     Multiple Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multivariate-linear-regression">
     Multivariate Linear Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-learning-classification-regression">
   Supervised learning (Classification, Regression)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-trees">
     Decision trees
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-trees">
     Regression trees
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ensembles">
     Ensembles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-nn">
     k-NN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     Linear regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     Naive Bayes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#artificial-neural-networks">
     Artificial neural networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     Logistic regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#perceptron">
     Perceptron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relevance-vector-machine-rvm">
     Relevance vector machine (RVM)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#support-vector-machine-svm">
     Support vector machine (SVM)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   Unsupervised learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-means-clustering">
     K-means clustering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#knn-k-nearest-neighbors">
     KNN (k-nearest neighbors)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hierarchal-clustering">
     Hierarchal clustering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anomaly-detection">
     Anomaly detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-networks">
     Neural Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principle-component-analysis">
     Principle Component Analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independent-component-analysis">
     Independent Component Analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apriori-algorithm">
     Apriori algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#singular-value-decomposition">
     Singular value decomposition
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clustering">
   Clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#birch">
     BIRCH
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cure">
     CURE
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hierarchical">
     Hierarchical
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     k-means
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectationmaximization-em">
     Expectation–maximization (EM)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dbscan">
     DBSCAN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optics">
     OPTICS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mean-shift">
     Mean shift
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dimensionality-reduction">
   Dimensionality Reduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#factor-analysis">
     Factor analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cca">
     CCA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ica">
     ICA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lda">
     LDA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nmf">
     NMF
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pca">
     PCA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pgd">
     PGD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#t-sne">
     t-SNE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ensemble-methods">
   Ensemble Methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bootstrap-aggregating-bagging">
     Bootstrap aggregating (Bagging)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bagging-vs-boosting">
     Bagging vs Boosting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#boosting">
     Boosting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#adaboost">
       AdaBoost
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gradient-tree-boosting">
       Gradient Tree Boosting
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id10">
       XGBoost
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     Random Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stacking">
     Stacking
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayes-optimal-classifier">
     Bayes optimal classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-model-averaging-bma">
     Bayesian model averaging (BMA)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-model-combination-bmc">
     Bayesian model combination (BMC)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bucket-of-models">
     Bucket of models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#voting-complement-of-bagging">
     Voting (complement of Bagging)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#blending-subtype-of-stacking">
     Blending (subtype of Stacking)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ensemble-learning-techniques">
   Ensemble Learning Techniques
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#type-of-models-used">
     Type of models used
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-sampling">
     Data Sampling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-function">
     Decision Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-learners">
     Types of learners
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-ensemble-methods">
     Types of ensemble methods
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-ensemble-learners">
     Types of ensemble learners
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#structured-prediction-graphical-models">
   Structured Prediction - Graphical models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayes-net">
     Bayes net
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-random-field">
     Conditional random field
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hidden-markov">
     Hidden Markov
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id12">
   Anomaly detection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     k-NN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#local-outlier-factor">
     Local outlier factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#artificial-neural-network">
   Artificial neural network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#autoencoder">
     Autoencoder
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cognitive-computing">
     Cognitive computing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-learning">
     Deep learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deepdream">
     DeepDream
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multilayer-perceptron">
     Multilayer perceptron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rnn">
     RNN
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lstm">
       LSTM
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gru">
       GRU
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#esn">
       ESN
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#restricted-boltzmann-machine">
     Restricted Boltzmann machine
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gan">
     GAN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#som">
     SOM
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolutional-neural-network">
     Convolutional neural network
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#u-net">
       U-Net
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformer-vision">
     Transformer Vision
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spiking-neural-network">
     Spiking neural network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#memtransistor">
     Memtransistor
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#electrochemical-ram-ecram">
     Electrochemical RAM (ECRAM)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id14">
   Reinforcement Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#q-learning">
     Q-learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sarsa">
     SARSA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#temporal-difference-td">
     Temporal difference (TD)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-theory">
   Machine Learning Theory
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kernel-machines">
     Kernel machines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#biasvariance-tradeoff">
     Bias–variance tradeoff
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computational-learning-theory">
     Computational learning theory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#empirical-risk-minimization">
     Empirical risk minimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#occam-learning">
     Occam learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pac-learning">
     PAC learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#statistical-learning">
     Statistical learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vc-theory">
     VC theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recommender-system-methods-and-challenges">
   Recommender System - Methods and challenges
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cold-start">
     Cold start
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#collaborative-filtering">
     Collaborative filtering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id15">
     Dimensionality reduction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implicit-data-collection">
     Implicit data collection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#item-item-collaborative-filtering">
     Item-item collaborative filtering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-factorization">
     Matrix factorization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preference-elicitation">
     Preference elicitation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similarity-search">
     Similarity search
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#most-popular-deep-learning-algorithms">
   10 most popular deep learning algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolutional-neural-networks-cnns">
     Convolutional Neural Networks (CNNs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#long-short-term-memory-networks-lstms">
     Long Short Term Memory Networks (LSTMs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recurrent-neural-networks-rnns">
     Recurrent Neural Networks (RNNs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generative-adversarial-networks-gans">
     Generative Adversarial Networks (GANs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#radial-basis-function-networks-rbfns">
     Radial Basis Function Networks (RBFNs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multilayer-perceptrons-mlps">
     Multilayer Perceptrons (MLPs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#self-organizing-maps-soms">
     Self Organizing Maps (SOMs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-belief-networks-dbns">
     Deep Belief Networks (DBNs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#restricted-boltzmann-machines-rbms">
     Restricted Boltzmann Machines( RBMs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#autoencoders">
     Autoencoders
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-algorithms-deep-learning">
   Optimization algorithms - Deep Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#asgd">
     ASGD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adadelta">
     Adadelta
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adagrad">
     Adagrad
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adam">
     Adam
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adamw">
     AdamW
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adamax">
     Adamax
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lbfgs">
     LBFGS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nadam">
     NAdam
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#radam">
     RAdam
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rmsprop">
     RMSprop
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rprop">
     Rprop
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sgd">
     SGD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sparseadam">
     SparseAdam
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-metrics-regression">
   Evaluation Metrics - Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mean-absolute-error-mae">
     Mean Absolute Error (MAE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mean-squared-error-mse">
     Mean Squared Error (MSE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#root-mean-squared-error-rmse">
     Root Mean Squared Error (RMSE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-squared-coefficient-of-determination">
     R-Squared (Coefficient of determination)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adjusted-r-squared">
     Adjusted R-squared
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimating-errors">
   Estimating Errors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-metrics-distribution-fit">
   Evaluation Metrics - Distribution Fit
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-information-criterion">
     Bayesian information criterion
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kolmogorovsmirnov-test">
     Kolmogorov–Smirnov test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cramervon-mises-criterion">
     Cramér–von Mises criterion
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#andersondarling-test">
     Anderson–Darling test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#shapirowilk-test">
     Shapiro–Wilk test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chi-square-test">
     Chi-square test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#akaike-information-criterion">
     Akaike information criterion
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-metrics-classification">
   Evaluation Metrics - Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-threshold">
     Classification Threshold
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#confusion-matrix">
     Confusion Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#precision-positive-predictive-value-ppv">
     Precision/Positive Predictive Value(PPV)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sensitivity-recall-hit-rate-tpr">
     Sensitivity/Recall/Hit Rate/TPR
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#specificity-selectivity-tnr">
     Specificity/Selectivity/TNR
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#accuracy">
     Accuracy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#f1">
     F1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-identify-the-correct-threshold">
     How to identify the correct threshold
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#roc-curve">
     ROC curve
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#roc-curve-for-scenarios">
     ROC curve for scenarios
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#area-under-the-curve-auc">
     Area under the curve (AUC)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#diagnostic-testing-diagram">
     Diagnostic Testing Diagram
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gini-coefficient">
     Gini Coefficient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#log-loss-or-cross-entropy-loss">
     Log Loss or Cross Entropy Loss
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prediction-bias">
     Prediction Bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimal-predicted-probability-cutoff">
     Optimal Predicted Probability Cutoff
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generative-models">
   Generative Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#discriminative-models">
   Discriminative Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#types-of-classifiers">
   Types of classifiers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimation-methods">
   Estimation methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood-ml-estimation">
     Maximum Likelihood (ML) Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#iterative-procedures">
       Iterative Procedures
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-a-posteriori-map-estimation">
     Maximum a Posteriori (MAP) Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#minimum-mean-square-error-mmse-estimation">
     Minimum Mean Square Error (MMSE) Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectation-maximization-em-algorithm">
     Expectation-Maximization(EM) algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gaussian-density">
       Gaussian Density
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mle-vs-em">
       MLE vs EM
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#applications-of-em">
       Applications of EM
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#least-square-ls-estimation">
     Least Square (LS) Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-least-squares">
     Linear Least squares
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-linear">
     Non-linear
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ordinary">
     Ordinary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weighted">
     Weighted
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalized">
     Generalized
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partial">
     Partial
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#total">
     Total
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-negative">
     Non-negative
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ridge-regression">
     Ridge regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularized">
     Regularized
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#least-absolute-deviations">
     Least absolute deviations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#iteratively-reweighted">
     Iteratively reweighted
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian">
     Bayesian
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-multivariate">
     Bayesian multivariate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayes-estimator">
     Bayes Estimator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probit">
     Probit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logit">
     Logit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kernel-density-estimation-kde">
     Kernel density estimation (KDE)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#frequentists-vs-bayesian-approach">
   Frequentists vs Bayesian Approach
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parametric-vs-nonparametric-method">
   Parametric vs NonParametric Method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prediction-vs-inference">
   Prediction vs Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mathematical-functions">
   Mathematical functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logistic-sigmoid-function">
     Logistic/Sigmoid function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#standard-logistic-sigmoid-function">
     Standard logistic sigmoid function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logit-function">
     Logit function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probit-function">
     Probit function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#softplus-function">
     Softplus function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weighted-sum">
     Weighted sum
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood-function">
     Likelihood function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#log-likelihood-function">
     Log-Likelihood function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-of-log-likelihood-function">
     Gradient of log likelihood function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#negative-log-likelihood-function">
     Negative log likelihood function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood-vs-log-likelihood-function">
     Likelihood vs Log-Likelihood function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additive-models">
   Additive Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-vs-rule-based-systems">
   Machine Learning vs Rule-based Systems
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-selection-validation">
   Model Selection/Validation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions">
   Activation Functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sigmoid-function">
     Sigmoid function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#softmax">
     Softmax
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tanh-function">
     tanh function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relu-rectified-linear-unit-function">
     ReLU (REctified Linear Unit) function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#elu">
     ELU
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#leakyrelu">
     LeakyReLU
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-engineering">
   Feature Engineering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mapping-rawdata-to-features">
     Mapping RawData to Features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#has-good-feature-property">
     Has Good Feature property
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-hot-encoding">
     One-Hot Encoding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scaling-feature-values">
     Scaling Feature values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#outlier-treatment">
     Outlier Treatment
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binning-by-values">
     Binning by values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binning-by-quantiles">
     Binning by quantiles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scrubing">
     Scrubing
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-crosses">
   Feature Crosses
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization-simplicity">
   Regularization - Simplicity
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#early-stopping">
     Early stopping
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#penalize-model-complexity">
     Penalize model complexity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     Empirical Risk Minimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structural-risk-minimization">
     Structural Risk Minimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-define-model-complexity">
     How to define model complexity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-function-with-l2-regularization">
     Loss Function with L2 Regularization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization-rate">
     Regularization Rate
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization-sparsity">
   Regularization - Sparsity
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sparse-cross-feature">
     Sparse cross-feature
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#explicitly-zero-out-weights">
     Explicitly zero out weights
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l1-vs-l2-regularization">
     L1 vs L2 regularization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l1-regularization">
     L1 regularization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#elastic-nets">
     Elastic Nets
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network">
   Neural Network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression-model-vs-nn">
     Linear Regression Model vs NN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-use-activation-function">
     Why use activation function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-activation-function">
     Types of Activation Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-activation-function">
     Linear Activation function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-linear-activation-function">
     Non-Linear Activation function
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>ML Conceptual brief</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning">
   Machine Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#supervised-learning">
     Supervised Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised-learning">
     Unsupervised Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reinforcement-learning">
     Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classification-algorithms">
   Classification algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probabilistic">
     Probabilistic
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#naive-bayes">
       Naive Bayes
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rule-based">
     Rule based
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decision-tree">
       Decision Tree
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gini-impurity">
       Gini Impurity
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#entropy">
       Entropy
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#information-gain">
       Information Gain
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#most-common">
   Most common
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression">
     Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logistic-regression">
     Logistic Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Decision Tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#svm">
     SVM
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#links">
       Links
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#explanation">
       Explanation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#questionnaires">
       Questionnaires
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Naive Bayes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-nearest-neighbor-knn">
     k-Nearest Neighbor (kNN)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-means">
     K-Means
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#algorithm">
       Algorithm
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-choose-optimal-k">
       How to choose optimal k
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-forest">
     Random Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dimensionality-reduction-algorithms">
     Dimensionality Reduction Algorithms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-boosting-algorithms">
     Gradient Boosting algorithms
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gbm">
       GBM
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#xgboost">
       XGBoost
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lightgbm">
       LightGBM
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#catboost">
       CatBoost
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regression-predict-a-value">
   Regression - Predict a value
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#boosted-decision-tree-regression">
     Boosted Decision Tree Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-forest-regression">
     Decision Forest Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fast-forest-quantile-regression">
     Fast Forest Quantile Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#local-linear-regression">
     Local Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#locally-weighted-linear-regression-lwr">
     Locally Weighted Linear Regression (LWR)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-network-regression">
     Neural Network Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#poisson-regression">
     Poisson Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#quantile-regression">
     Quantile Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classification-predict-a-class-choose-from-binary-two-class-or-multiclass-algorithms">
   Classification - Predict a class. Choose from binary (two-class) or multiclass algorithms.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiclass-boosted-decision-tree">
     Multiclass Boosted Decision Tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiclass-decision-forest">
     Multiclass Decision Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiclass-logistic-regression-softmax-regression">
     Multiclass Logistic Regression (Softmax Regression)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiclass-neural-network">
     Multiclass Neural Network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-vs-all-multiclass">
     One vs. All Multiclass
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-vs-one-multiclass">
     One vs. One Multiclass
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-class-averaged-perceptron">
     Two-Class Averaged Perceptron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-class-boosted-decision-tree">
     Two-Class Boosted Decision Tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-class-decision-forest">
     Two-Class Decision Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-class-logistic-regression">
     Two-Class Logistic Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-class-neural-network">
     Two-Class Neural Network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-class-support-vector-machine">
     Two Class Support Vector Machine
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binary-vs-multiclass-vs-multilabel-classification">
     Binary vs Multiclass vs MultiLabel classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-linear-regression">
     Multiple Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multivariate-linear-regression">
     Multivariate Linear Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-learning-classification-regression">
   Supervised learning (Classification, Regression)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-trees">
     Decision trees
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-trees">
     Regression trees
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ensembles">
     Ensembles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-nn">
     k-NN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     Linear regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     Naive Bayes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#artificial-neural-networks">
     Artificial neural networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     Logistic regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#perceptron">
     Perceptron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relevance-vector-machine-rvm">
     Relevance vector machine (RVM)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#support-vector-machine-svm">
     Support vector machine (SVM)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   Unsupervised learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-means-clustering">
     K-means clustering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#knn-k-nearest-neighbors">
     KNN (k-nearest neighbors)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hierarchal-clustering">
     Hierarchal clustering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anomaly-detection">
     Anomaly detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-networks">
     Neural Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principle-component-analysis">
     Principle Component Analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independent-component-analysis">
     Independent Component Analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apriori-algorithm">
     Apriori algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#singular-value-decomposition">
     Singular value decomposition
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clustering">
   Clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#birch">
     BIRCH
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cure">
     CURE
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hierarchical">
     Hierarchical
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     k-means
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectationmaximization-em">
     Expectation–maximization (EM)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dbscan">
     DBSCAN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optics">
     OPTICS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mean-shift">
     Mean shift
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dimensionality-reduction">
   Dimensionality Reduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#factor-analysis">
     Factor analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cca">
     CCA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ica">
     ICA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lda">
     LDA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nmf">
     NMF
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pca">
     PCA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pgd">
     PGD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#t-sne">
     t-SNE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ensemble-methods">
   Ensemble Methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bootstrap-aggregating-bagging">
     Bootstrap aggregating (Bagging)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bagging-vs-boosting">
     Bagging vs Boosting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#boosting">
     Boosting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#adaboost">
       AdaBoost
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gradient-tree-boosting">
       Gradient Tree Boosting
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id10">
       XGBoost
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     Random Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stacking">
     Stacking
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayes-optimal-classifier">
     Bayes optimal classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-model-averaging-bma">
     Bayesian model averaging (BMA)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-model-combination-bmc">
     Bayesian model combination (BMC)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bucket-of-models">
     Bucket of models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#voting-complement-of-bagging">
     Voting (complement of Bagging)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#blending-subtype-of-stacking">
     Blending (subtype of Stacking)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ensemble-learning-techniques">
   Ensemble Learning Techniques
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#type-of-models-used">
     Type of models used
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-sampling">
     Data Sampling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-function">
     Decision Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-learners">
     Types of learners
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-ensemble-methods">
     Types of ensemble methods
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-ensemble-learners">
     Types of ensemble learners
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#structured-prediction-graphical-models">
   Structured Prediction - Graphical models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayes-net">
     Bayes net
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-random-field">
     Conditional random field
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hidden-markov">
     Hidden Markov
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id12">
   Anomaly detection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     k-NN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#local-outlier-factor">
     Local outlier factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#artificial-neural-network">
   Artificial neural network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#autoencoder">
     Autoencoder
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cognitive-computing">
     Cognitive computing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-learning">
     Deep learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deepdream">
     DeepDream
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multilayer-perceptron">
     Multilayer perceptron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rnn">
     RNN
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lstm">
       LSTM
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gru">
       GRU
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#esn">
       ESN
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#restricted-boltzmann-machine">
     Restricted Boltzmann machine
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gan">
     GAN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#som">
     SOM
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolutional-neural-network">
     Convolutional neural network
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#u-net">
       U-Net
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformer-vision">
     Transformer Vision
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spiking-neural-network">
     Spiking neural network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#memtransistor">
     Memtransistor
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#electrochemical-ram-ecram">
     Electrochemical RAM (ECRAM)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id14">
   Reinforcement Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#q-learning">
     Q-learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sarsa">
     SARSA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#temporal-difference-td">
     Temporal difference (TD)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-theory">
   Machine Learning Theory
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kernel-machines">
     Kernel machines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#biasvariance-tradeoff">
     Bias–variance tradeoff
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computational-learning-theory">
     Computational learning theory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#empirical-risk-minimization">
     Empirical risk minimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#occam-learning">
     Occam learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pac-learning">
     PAC learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#statistical-learning">
     Statistical learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vc-theory">
     VC theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recommender-system-methods-and-challenges">
   Recommender System - Methods and challenges
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cold-start">
     Cold start
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#collaborative-filtering">
     Collaborative filtering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id15">
     Dimensionality reduction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implicit-data-collection">
     Implicit data collection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#item-item-collaborative-filtering">
     Item-item collaborative filtering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-factorization">
     Matrix factorization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preference-elicitation">
     Preference elicitation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similarity-search">
     Similarity search
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#most-popular-deep-learning-algorithms">
   10 most popular deep learning algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolutional-neural-networks-cnns">
     Convolutional Neural Networks (CNNs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#long-short-term-memory-networks-lstms">
     Long Short Term Memory Networks (LSTMs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recurrent-neural-networks-rnns">
     Recurrent Neural Networks (RNNs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generative-adversarial-networks-gans">
     Generative Adversarial Networks (GANs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#radial-basis-function-networks-rbfns">
     Radial Basis Function Networks (RBFNs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multilayer-perceptrons-mlps">
     Multilayer Perceptrons (MLPs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#self-organizing-maps-soms">
     Self Organizing Maps (SOMs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-belief-networks-dbns">
     Deep Belief Networks (DBNs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#restricted-boltzmann-machines-rbms">
     Restricted Boltzmann Machines( RBMs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#autoencoders">
     Autoencoders
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-algorithms-deep-learning">
   Optimization algorithms - Deep Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#asgd">
     ASGD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adadelta">
     Adadelta
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adagrad">
     Adagrad
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adam">
     Adam
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adamw">
     AdamW
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adamax">
     Adamax
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lbfgs">
     LBFGS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nadam">
     NAdam
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#radam">
     RAdam
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rmsprop">
     RMSprop
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rprop">
     Rprop
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sgd">
     SGD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sparseadam">
     SparseAdam
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-metrics-regression">
   Evaluation Metrics - Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mean-absolute-error-mae">
     Mean Absolute Error (MAE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mean-squared-error-mse">
     Mean Squared Error (MSE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#root-mean-squared-error-rmse">
     Root Mean Squared Error (RMSE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-squared-coefficient-of-determination">
     R-Squared (Coefficient of determination)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adjusted-r-squared">
     Adjusted R-squared
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimating-errors">
   Estimating Errors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-metrics-distribution-fit">
   Evaluation Metrics - Distribution Fit
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-information-criterion">
     Bayesian information criterion
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kolmogorovsmirnov-test">
     Kolmogorov–Smirnov test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cramervon-mises-criterion">
     Cramér–von Mises criterion
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#andersondarling-test">
     Anderson–Darling test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#shapirowilk-test">
     Shapiro–Wilk test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chi-square-test">
     Chi-square test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#akaike-information-criterion">
     Akaike information criterion
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-metrics-classification">
   Evaluation Metrics - Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-threshold">
     Classification Threshold
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#confusion-matrix">
     Confusion Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#precision-positive-predictive-value-ppv">
     Precision/Positive Predictive Value(PPV)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sensitivity-recall-hit-rate-tpr">
     Sensitivity/Recall/Hit Rate/TPR
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#specificity-selectivity-tnr">
     Specificity/Selectivity/TNR
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#accuracy">
     Accuracy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#f1">
     F1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-identify-the-correct-threshold">
     How to identify the correct threshold
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#roc-curve">
     ROC curve
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#roc-curve-for-scenarios">
     ROC curve for scenarios
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#area-under-the-curve-auc">
     Area under the curve (AUC)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#diagnostic-testing-diagram">
     Diagnostic Testing Diagram
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gini-coefficient">
     Gini Coefficient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#log-loss-or-cross-entropy-loss">
     Log Loss or Cross Entropy Loss
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prediction-bias">
     Prediction Bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimal-predicted-probability-cutoff">
     Optimal Predicted Probability Cutoff
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generative-models">
   Generative Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#discriminative-models">
   Discriminative Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#types-of-classifiers">
   Types of classifiers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimation-methods">
   Estimation methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood-ml-estimation">
     Maximum Likelihood (ML) Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#iterative-procedures">
       Iterative Procedures
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-a-posteriori-map-estimation">
     Maximum a Posteriori (MAP) Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#minimum-mean-square-error-mmse-estimation">
     Minimum Mean Square Error (MMSE) Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectation-maximization-em-algorithm">
     Expectation-Maximization(EM) algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gaussian-density">
       Gaussian Density
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mle-vs-em">
       MLE vs EM
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#applications-of-em">
       Applications of EM
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#least-square-ls-estimation">
     Least Square (LS) Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-least-squares">
     Linear Least squares
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-linear">
     Non-linear
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ordinary">
     Ordinary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weighted">
     Weighted
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalized">
     Generalized
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partial">
     Partial
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#total">
     Total
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-negative">
     Non-negative
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ridge-regression">
     Ridge regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularized">
     Regularized
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#least-absolute-deviations">
     Least absolute deviations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#iteratively-reweighted">
     Iteratively reweighted
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian">
     Bayesian
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-multivariate">
     Bayesian multivariate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayes-estimator">
     Bayes Estimator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probit">
     Probit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logit">
     Logit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kernel-density-estimation-kde">
     Kernel density estimation (KDE)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#frequentists-vs-bayesian-approach">
   Frequentists vs Bayesian Approach
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parametric-vs-nonparametric-method">
   Parametric vs NonParametric Method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prediction-vs-inference">
   Prediction vs Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mathematical-functions">
   Mathematical functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logistic-sigmoid-function">
     Logistic/Sigmoid function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#standard-logistic-sigmoid-function">
     Standard logistic sigmoid function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logit-function">
     Logit function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probit-function">
     Probit function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#softplus-function">
     Softplus function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weighted-sum">
     Weighted sum
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood-function">
     Likelihood function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#log-likelihood-function">
     Log-Likelihood function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-of-log-likelihood-function">
     Gradient of log likelihood function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#negative-log-likelihood-function">
     Negative log likelihood function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood-vs-log-likelihood-function">
     Likelihood vs Log-Likelihood function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additive-models">
   Additive Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-vs-rule-based-systems">
   Machine Learning vs Rule-based Systems
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-selection-validation">
   Model Selection/Validation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions">
   Activation Functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sigmoid-function">
     Sigmoid function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#softmax">
     Softmax
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tanh-function">
     tanh function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relu-rectified-linear-unit-function">
     ReLU (REctified Linear Unit) function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#elu">
     ELU
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#leakyrelu">
     LeakyReLU
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-engineering">
   Feature Engineering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mapping-rawdata-to-features">
     Mapping RawData to Features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#has-good-feature-property">
     Has Good Feature property
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-hot-encoding">
     One-Hot Encoding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scaling-feature-values">
     Scaling Feature values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#outlier-treatment">
     Outlier Treatment
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binning-by-values">
     Binning by values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binning-by-quantiles">
     Binning by quantiles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scrubing">
     Scrubing
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-crosses">
   Feature Crosses
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization-simplicity">
   Regularization - Simplicity
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#early-stopping">
     Early stopping
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#penalize-model-complexity">
     Penalize model complexity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     Empirical Risk Minimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structural-risk-minimization">
     Structural Risk Minimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-define-model-complexity">
     How to define model complexity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-function-with-l2-regularization">
     Loss Function with L2 Regularization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization-rate">
     Regularization Rate
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization-sparsity">
   Regularization - Sparsity
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sparse-cross-feature">
     Sparse cross-feature
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#explicitly-zero-out-weights">
     Explicitly zero out weights
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l1-vs-l2-regularization">
     L1 vs L2 regularization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l1-regularization">
     L1 regularization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#elastic-nets">
     Elastic Nets
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network">
   Neural Network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression-model-vs-nn">
     Linear Regression Model vs NN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-use-activation-function">
     Why use activation function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-activation-function">
     Types of Activation Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-activation-function">
     Linear Activation function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-linear-activation-function">
     Non-Linear Activation function
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="ml-conceptual-brief">
<h1>ML Conceptual brief<a class="headerlink" href="#ml-conceptual-brief" title="Permalink to this headline">#</a></h1>
<p><img alt="ML Mind Map" src="../_images/ml_mind_map.png" /></p>
<p><span class="math notranslate nohighlight">\(\tiny{\text{Jason Brownlee}}\)</span></p>
<section id="machine-learning">
<h2>Machine Learning<a class="headerlink" href="#machine-learning" title="Permalink to this headline">#</a></h2>
<section id="supervised-learning">
<h3>Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">#</a></h3>
<p>Supervised Learning is a machine learning approach that uses labeled data to train algorithms into classifying or predicting outcomes accurately.</p>
<blockquote>
<div><ul class="simple">
<li><p>Types of Supervised Learning algorithms</p></li>
</ul>
</div></blockquote>
</section>
<section id="unsupervised-learning">
<h3>Unsupervised Learning<a class="headerlink" href="#unsupervised-learning" title="Permalink to this headline">#</a></h3>
<p>Unsupervised Learning is a machine learning approach that uses unlabeled data to analyze and cluster datasets. It makes inference without knowing the label or outcomes.</p>
<blockquote>
<div><ul class="simple">
<li><p>Types of Unsupervised Learning algorithms</p>
<ul>
<li><p>Clustering</p></li>
<li><p>Association</p></li>
</ul>
</li>
</ul>
</div></blockquote>
</section>
<section id="reinforcement-learning">
<h3>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permalink to this headline">#</a></h3>
<p>Reinforcement learning is a machine learning training method based on rewarding desired behaviors and punishing the undesirable ones, thereby learning about the environment by trial and error</p>
<blockquote>
<div><ul class="simple">
<li><p>Types of Reinforcement Learning algorithms</p></li>
</ul>
</div></blockquote>
</section>
</section>
<section id="classification-algorithms">
<h2>Classification algorithms<a class="headerlink" href="#classification-algorithms" title="Permalink to this headline">#</a></h2>
<p>Classification algorithms use input training data to predict the likelihood that subsequent data will fall into one of predetermined categories</p>
<blockquote>
<div><ul class="simple">
<li><p>Types of Classification algorithms</p></li>
</ul>
</div></blockquote>
<section id="probabilistic">
<h3>Probabilistic<a class="headerlink" href="#probabilistic" title="Permalink to this headline">#</a></h3>
<p>Probabilistic classification models classify, given an observation of an input, a probability distribution over a set of classes</p>
<blockquote>
<div><ul class="simple">
<li><p>Types of Probabilistic Classification algorithms</p>
<ul>
<li><p>Naive Bayes</p></li>
<li><p>Logistic regression</p></li>
<li><p>Multilayer perceptrons</p></li>
</ul>
</li>
</ul>
</div></blockquote>
<p><img alt="" src="https://www.ismiletechnologies.com/wp-content/uploads/2021/10/image-15.png" /></p>
<p><span class="math notranslate nohighlight">\(\tiny{\text{www.ismiletechnologies.com}}\)</span></p>
<section id="naive-bayes">
<h4>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this headline">#</a></h4>
<p>Bayesian classification helps us find the probability of a label given some observed features, using Bayes theorem, which describes the relationship of conditional probabilities of statistical quantities</p>
<blockquote>
<div><ul class="simple">
<li><p>READ: <a class="reference external" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html">https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html</a></p></li>
<li><p>Types of Bayesian Classification</p>
<ul>
<li><p>Multinomial Naïve Bayes Classifier</p></li>
<li><p>Bernoulli Naïve Bayes Classifier</p></li>
<li><p>Gaussian Naïve Bayes Classifier</p></li>
</ul>
</li>
</ul>
</div></blockquote>
<p><img alt="" src="https://www.analyticsvidhya.com/wp-content/uploads/2015/09/Bayes_rule-300x172-300x172.png" /></p>
<p><span class="math notranslate nohighlight">\(\tiny{\text{www.analyticsvidhya.com}}\)</span></p>
</section>
</section>
<section id="rule-based">
<h3>Rule based<a class="headerlink" href="#rule-based" title="Permalink to this headline">#</a></h3>
<p>Rule based classification helps in classifying datasets by using a collection of “if.. else..” rules. The classifier may contain mutually exclusive rules, exhaustive rules, not mutually exclusive rules, or not exhaustive rules</p>
<blockquote>
<div><ul class="simple">
<li><p>READ: <a class="reference external" href="http://jcsites.juniata.edu/faculty/rhodes/ml/rulebasedClass.htm">http://jcsites.juniata.edu/faculty/rhodes/ml/rulebasedClass.htm</a></p></li>
</ul>
</div></blockquote>
<section id="decision-tree">
<h4>Decision Tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">#</a></h4>
<p>A decision tree is a data mining/machine learning method of predicting/classifying the value of a target variable based on several input variables. In this classification tree, each internal node is labeled with an input feature and each leaf of the tree is labeled with a class or a probability distribution over the classes of either class or into a particular probability distribution</p>
<blockquote>
<div><ul class="simple">
<li><p>Decision Tree Types</p>
<ul>
<li><p>CART - Classification and Regression Trees</p></li>
<li><p>Ensemble methods, construct more than one decision tree</p>
<ul>
<li><p>Boosted trees</p></li>
<li><p>Bootstrap aggregated (or bagged/bagging) decision trees</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div></blockquote>
<ul class="simple">
<li><p>When a decision tree</p>
<ul>
<li><p>classifies things into categories</p>
<ul>
<li><p>it’s called Classification Tree</p></li>
</ul>
</li>
<li><p>predicts numerical values</p>
<ul>
<li><p>it’s called Regression Tree</p></li>
</ul>
</li>
</ul>
</li>
<li><p>In classification tree, features of different datatypes can be mixed together</p>
<ul>
<li><p>exercise &lt; 20 minutes</p>
<ul>
<li><p>classifies as True/False</p></li>
</ul>
</li>
<li><p>eat doughnuts</p>
<ul>
<li><p>classifies as True/False</p></li>
</ul>
</li>
</ul>
</li>
<li><p>numerical thresholds can be different for the same data</p>
<ul>
<li><p>For example</p>
<ul>
<li><p>40 years or older</p>
<ul>
<li><p>True</p>
<ul>
<li><p>exercise &lt; 20 minutes</p>
<ul>
<li><p>classifies further as True/False</p></li>
</ul>
</li>
</ul>
</li>
<li><p>False</p>
<ul>
<li><p>exercise &gt; 30 minutes</p>
<ul>
<li><p>classifies further as True/False</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>how to choose optimal k</p>
<ul>
<li><p>pick the columns which are least impure</p></li>
</ul>
</li>
<li><p>how to measure quality of split</p>
<ul>
<li><p>based on the lowest impurity factor</p></li>
<li><p>the feature with lowest impurity takes the root position and same iterative process is repeated</p></li>
</ul>
</li>
</ul>
<blockquote>
<div><ul class="simple">
<li><p>|  criterion : {“gini”, “entropy”}, default=”gini”</p></li>
<li><p>|      The function to measure the quality of a split. Supported criteria are</p></li>
<li><p>|      “gini” for the Gini impurity and “entropy” for the information gain.</p></li>
</ul>
</div></blockquote>
<img src="https://miro.medium.com/max/1200/1*ZkQXt7mqI7MXuXhHrfvgtQ.png" width=400 height=400> 
<p><span class="math notranslate nohighlight">\(\tiny{\text{miro.medium.com}}\)</span></p>
<img src="https://upload.wikimedia.org/wikipedia/commons/2/25/Cart_tree_kyphosis.png"> 
<p><span class="math notranslate nohighlight">\(\tiny{\text{Wikipedia}}\)</span></p>
</section>
<section id="gini-impurity">
<h4>Gini Impurity<a class="headerlink" href="#gini-impurity" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>Gini Impurity is the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.</p>
<ul>
<li><p>If there are <span class="math notranslate nohighlight">\(C\)</span> total classes and <span class="math notranslate nohighlight">\(p(i)\)</span> is the probability of picking a datapoint with class i, then the Gini Impurity is calculated as</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[I_G(p) = \sum\limits_{i=1}^{J}\left(p_i\sum_{k \ne i} p_k \right) = \sum\limits_{i=1}^{J}p_i\left(1 - p_i \right) = \sum\limits_{i=1}^{J}\left(p_i - p^2_i \right) = \sum\limits_{i=1}^{J}p_i - \sum\limits_{i=1}^{J}p^2_i = 1 - \sum\limits_{i=1}^{J}p^2_i\]</div>
</section>
<section id="entropy">
<h4>Entropy<a class="headerlink" href="#entropy" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>Entropy is a measure of state of disorder, randomness, or uncertainty. It is used to represent the uncertainty associated with data</p></li>
<li><p>If all the data points belong to a single class, then there is no real uncertainty, and will have a low entropy</p></li>
<li><p>If all the data points are evenly distributed across the classes, there is lot of uncertainty, and will have a high entropy</p></li>
</ul>
<div class="math notranslate nohighlight">
\[H(Q_m) = -\sum\limits_{k}p_{mk}\log(p_{mk})\]</div>
</section>
<section id="information-gain">
<h4>Information Gain<a class="headerlink" href="#information-gain" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>measures reduction in entropy or surprise from an additional piece of information</p></li>
<li><p>greater the reduction in uncertainty, more is the information gained
$<span class="math notranslate nohighlight">\(IG(Y,X) = E(Y) - E(Y|X)\)</span>$</p></li>
</ul>
</section>
</section>
</section>
<section id="most-common">
<h2>Most common<a class="headerlink" href="#most-common" title="Permalink to this headline">#</a></h2>
<section id="linear-regression">
<h3>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">#</a></h3>
<p>Linear regression helps us model the relationship between two variables by fitting a linear equation to observed data. The most common method for fitting a regression line is the method of least-squares. This method calculates the best-fitting line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line (if a point lies on the fitted line exactly, then its vertical deviation is 0). Because the deviations are first squared, then summed, there are no cancellations between positive and negative values.</p>
<p>We assume here that <span class="math notranslate nohighlight">\(y|x; \theta \sim \mathbb N(\mu, \sigma^{2})\)</span></p>
<p>The closed form solution for the <span class="math notranslate nohighlight">\(\theta\)</span> that minimizes the cost function is</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\theta = (X^{T}X)^{-1}X^{T}y\]</div>
</div></blockquote>
</section>
<section id="logistic-regression">
<h3>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">#</a></h3>
<p>Logistic regression models generate probabilities.</p>
<p>The logistic regression is used to model the relationship between a set of independent and dependent variables. The dependent variables are categorical in nature, which is predicted based on the probabilities given some characteristics of class variables. The logistic regression uses sigmoid function to assign class labels.</p>
<p>The logit function is defined as the logarithm of the log odds</p>
<div class="math notranslate nohighlight">
\[ z = \text{logit(p)} = \ln\frac{p}{1-p} \]</div>
<p><span class="math notranslate nohighlight">\(z\)</span> is also referred to as the log-odds because the inverse of the sigmoid states that <span class="math notranslate nohighlight">\(z\)</span> can be defined as the log of the probability of the <span class="math notranslate nohighlight">\(1\)</span> label (e.g., “dog bark”) divided by the probability of the <span class="math notranslate nohighlight">\(0\)</span> label(e.g., “dog does not bark”)</p>
<p>A standard logistic sigmoid function is defined as the</p>
<div class="math notranslate nohighlight">
\[ \sigma(z) = \frac{1}{1+e^{-z}} \]</div>
<p>The linear part of the model predicts the log-odds of the dataset example in the form of probability using logistic sigmoid function.</p>
<p>It tries to learn a function that approximates P(Y|X), by assuming that P(Y|X) can be approximated as a sigmoid
function when applied to the linear combination of input features.</p>
<div class="math notranslate nohighlight">
\[ P(Y = 1|X = x) = \sigma(z) = \sigma(\theta^{T}x) \]</div>
<p>where <span class="math notranslate nohighlight">\(z = \theta_{0} + \sum\limits_{i=1}^{m}\theta_{i}x_{i} \)</span></p>
<p>Similarly,</p>
<div class="math notranslate nohighlight">
\[ P(Y = 0|X = x) = 1 - \sigma(\theta^{T}x) \]</div>
<p>The gradient descent is calculated as the partial derivative of logistic cost function wrt weight, which is used to maximize the logistic cost function.</p>
<p>The loss function for logistic regression is Log Loss defined as</p>
<img src="./images/logisticRegressionCostFun.png" width=500 height=500>
<img src="./images/logisticRegressionCostFun2.png" width=500 height=500>
<p><span class="math notranslate nohighlight">\(\tiny{\text{towardsdatascience.com - Shuyu Luo}}\)</span></p>
</section>
<section id="id1">
<h3>Decision Tree<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<p><a href="#Decision-Tree">Link (above)</a></p>
</section>
<section id="svm">
<h3>SVM<a class="headerlink" href="#svm" title="Permalink to this headline">#</a></h3>
<section id="links">
<h4>Links<a class="headerlink" href="#links" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://stats.stackexchange.com/questions/23391/how-does-a-support-vector-machine-svm-work">StackOverflow - How does SVM work</a>
-<a class="reference external" href="https://static1.squarespace.com/static/58851af9ebbd1a30e98fb283/t/58902fbae4fcb5398aeb7505/1485844411772/SVM+Explained.pdf">SVM Explained - Tristan Fletcher</a></p></li>
<li><p><a class="reference external" href="https://www.baeldung.com/cs/svm-hard-margin-vs-soft-margin">Hard vs Soft margin</a></p></li>
<li><p>[Kernel trick](<a class="reference external" href="https://datamites.com/blog/support-vector-machine-algorithm-svm-understanding-kernel-trick/#:~:text=A%20Kernel%20Trick%20is%20a,Lagrangian%20formula%20using%20Lagrangian%20multipliers.%20()">https://datamites.com/blog/support-vector-machine-algorithm-svm-understanding-kernel-trick/#:~:text=A Kernel Trick is a,Lagrangian formula using Lagrangian multipliers. ()</a></p></li>
</ul>
</section>
<section id="explanation">
<h4>Explanation<a class="headerlink" href="#explanation" title="Permalink to this headline">#</a></h4>
<p>SVM should maximize the distance between the two decision boundaries. Mathematically, this means we want to <strong><em>maximize the distance between the hyperplane</em></strong> defined by <span class="math notranslate nohighlight">\(w^{T}x+b=−1\)</span> and the hyperplane defined by <span class="math notranslate nohighlight">\(w^{T}x+b = 1\)</span>. This distance is equal to <span class="math notranslate nohighlight">\(\frac{2}{\|w\|}\)</span>. This means we want to solve <span class="math notranslate nohighlight">\(\max\limits_{w}\frac{2}{\|w\|}\)</span>. Equivalently we want <span class="math notranslate nohighlight">\(\min\limits_{w}\frac{\|w\|}{2}\)</span></p>
<p>SVM should also classify all <span class="math notranslate nohighlight">\(x^{(i)}\)</span>, which means <span class="math notranslate nohighlight">\(y^{(i)}(𝐰^{T}x^{(i)}+b \ge 1)\)</span>, which gives us the <strong><em>quadratic optimization problem</em></strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\min\limits_{w,b}\frac{\|w\|^{2}}{2}\]</div>
<p>s.t., <span class="math notranslate nohighlight">\(y^{(i)}(𝐰^{T}x^{(i)}+b \ge 1)\)</span> for all <span class="math notranslate nohighlight">\(i \in \{1,..N\}\)</span></p>
</div></blockquote>
<p>the above is the <strong><em>hard margin SVM</em></strong>, which is possible iff data is <strong><em>linearly separable</em></strong></p>
<p>This optimization is called the <strong><em>primal problem</em></strong> and is guaranteed to have a global minimum. We can solve this by introducing <strong><em>Lagrange multipliers</em></strong> and converting it to the <strong><em>dual problem</em></strong></p>
<p>If we allow misclassifications to happen, we call it <strong><em>soft margin SVM</em></strong> optimization problem. The loss function that we minimize is the hinge loss</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\max\{0, 1-y^{(i)}(𝐰^{T}x^{(i)}+b \ge 1)\}\)</span></p>
</div></blockquote>
<p>The loss of a misclassified point is called a <em><strong>slack variable</strong></em> and is added to the primal problem of hard margin SVM. So the primal problem for soft margin becomes</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\min\limits_{w,b}\frac{\|w\|^{2}}{2} + C\sum\limits_{i=0}^{n}\zeta^{(i)}\]</div>
<p>s.t., <span class="math notranslate nohighlight">\(y^{(i)}(𝐰^{T}x^{(i)}+b \ge 1 - \zeta^{(i)})\)</span> for all <span class="math notranslate nohighlight">\(i \in \{1,..N\}\)</span><br />
and <span class="math notranslate nohighlight">\(\zeta^{(i)} \gt 0\)</span> for all <span class="math notranslate nohighlight">\(i \in \{1,..N\}\)</span></p>
</div></blockquote>
</section>
<section id="questionnaires">
<h4>Questionnaires<a class="headerlink" href="#questionnaires" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>What are the goals of SVM?</p>
<ul>
<li><p>maximizes the margin around separating hyperplane</p></li>
<li><p>classify the class labels as {-1,1}</p></li>
</ul>
</li>
<li><p>What does linearly separable means?</p>
<ul>
<li><p>Linearly separable data means if graphed in two dimensions, it can be separated by straight line.</p></li>
</ul>
</li>
<li><p>What is the difference between hard margin SVM and soft margin SVM?</p>
<ul>
<li><p>if the data is linearly separable, we use hard margin SVM</p></li>
<li><p>if it is impossible to find linear classifier, we allow misclassification of data points, and use appropriate soft margin SVM</p></li>
<li><p>other use case for using soft margin SVM will be</p>
<ul>
<li><p>if data is linearly separable but the margin is small and the model either tends to overfit or is too sensitive to outliers, in that case, we opt for soft margin SVM and get larger margin, thereby generalizing the model better</p></li>
</ul>
</li>
</ul>
</li>
<li><p>What is functional margin?</p></li>
<li><p>What is geometric margin?</p></li>
<li><p>What is a kernel?</p>
<ul>
<li><p>A kernel is a method of placing a two dimensional plane into a higher dimensional space, so that it is curved in the higher dimensional space. (In simple terms, a kernel is a function from the low dimensional space into a higher dimensional space.)</p></li>
</ul>
</li>
<li><p>What is kernel trick?</p>
<ul>
<li><p>SVM works better in two dimensional space which is linearly separable, as for non-linear data SVM finds it difficult to classify. So solve this kernel trick is used. A kernel trick is a method of projecting non-linear data onto higher dimensional space so as to easily classify linearly with a hyperplane. This is achieved using Lagrnagian formula/Lagrnagian multipliers.</p></li>
</ul>
</li>
<li><p>What are the different types of kernel?</p>
<ul>
<li><p>Fisher Kernel: It is a kernel function that analyses and measures the similarity of two objects. This is done on the basis of sets of measurements for each object and a statistical model.</p></li>
<li><p>Graph Kernel: It is a kernel function that computes an inner product on graphs.</p></li>
<li><p>Polynomial Kernel: It is a kernel commonly used with support vector machines (SVMs). It is also used with other kernelised models that symbolizes the similarity of vectors in a feature space over polynomials of the original variables, allowing learning of non-linear models.</p></li>
<li><p>Radial Basis Function Kernel (RBF): It is a real-valued kernel function whose value depends only on the distance from the origin, or distance from some other point called a centre.</p></li>
</ul>
</li>
<li><p>What is support vector mean?</p>
<ul>
<li><p>support vectors are the data points that lie farthest to the decision surface or hyperplane. They are the data points which are most difficult to classify</p></li>
</ul>
</li>
<li><p>Is SVM applied for classification or regression?</p>
<ul>
<li><p>basically it is a non-probabilistic binary linear classification method</p></li>
<li><p>but different flavors of it SVM exist in a probabilistic classification setting</p></li>
<li><p>applying the kernel trick, we can have non-linear classification</p></li>
</ul>
</li>
<li><p>Is SVM a binary method only</p></li>
</ul>
</section>
</section>
<section id="id2">
<h3>Naive Bayes<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h3>
</section>
<section id="k-nearest-neighbor-knn">
<h3>k-Nearest Neighbor (kNN)<a class="headerlink" href="#k-nearest-neighbor-knn" title="Permalink to this headline">#</a></h3>
</section>
<section id="k-means">
<h3>K-Means<a class="headerlink" href="#k-means" title="Permalink to this headline">#</a></h3>
<p>A cluster is a collection of data points aggregated based on its similarities. A centroid is the imaginary location representing the center of the cluster. In the k-means algorithm, k refers to the number of centroids which is defined initially. Each datapoint is allocated to a cluster iteratively based on optimizing the in-cluster variances (squared Euclidean distances).</p>
<section id="algorithm">
<h4>Algorithm<a class="headerlink" href="#algorithm" title="Permalink to this headline">#</a></h4>
<ul>
<li><p>Data has no labels : <span class="math notranslate nohighlight">\((x^{(1)}, x^{(2)}, ..., x^{(m)})\)</span></p></li>
<li><p>Initialize cluster centroid <span class="math notranslate nohighlight">\(\mu_{1}, \mu_{2}, ..., \mu_{k} \in \mathbb R^{n}\)</span> - by randomly pick k example out of your training set and set cluster centroids to k-randomly chosen examples</p></li>
<li><p>Repeat until convergence</p>
<ul class="simple">
<li><p>a. Set <span class="math notranslate nohighlight">\(c^{(i)} := \text{arg }\min\limits_{j} \Vert (x^{(i)} - \mu_{j})\Vert_{2}\)</span>  (“color the points”)</p>
<ul>
<li><p>Set <span class="math notranslate nohighlight">\(c^{(i)}\)</span> equal to either j = 1 or 2 depending on whether that example <span class="math notranslate nohighlight">\(x^{(i)}\)</span> is closer to cluster centroid 1 or 2</p></li>
<li><p>Notation:</p>
<ul>
<li><p>L1 norm: <span class="math notranslate nohighlight">\(\Vert x \Vert_{1}\)</span></p></li>
<li><p>L2 norm: <span class="math notranslate nohighlight">\(\Vert x \Vert\)</span> or <span class="math notranslate nohighlight">\(\|x\|^{2}\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>b. For j =1,2,..,k  (“move the cluster centroids”)</p></li>
</ul>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\mu_{j} := \frac{\sum\limits_{i=1}^{m} \mathbb 1 \{c^{(i)} = j\}x^{(i)}} {\sum\limits_{i=1}^{m} \mathbb 1 \{c^{(i)} = j\}}\)</span></p>
</div></blockquote>
</li>
<li><p>This algorithm is not guaranteed to converge, as it is a non-convex function</p></li>
<li><p>Cost/Distortion function</p></li>
</ul>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(J(c,\mu) = \min\sum\limits_{i=1}^{m}\|x^{(i)} - \mu_{c^{(i)}}\|^{2}\)</span></p>
</div></blockquote>
<ul class="simple">
<li><p>how do you choose k?</p>
<ul>
<li><p>choose manually, depending on what is the purpose of this algorithm</p></li>
<li><p>if it is meant for market segmentation for 4 categories, it makes sense to have 4 cluster rather than more</p></li>
<li><p>some formula available</p></li>
</ul>
</li>
</ul>
</section>
<section id="how-to-choose-optimal-k">
<h4>How to choose optimal k<a class="headerlink" href="#how-to-choose-optimal-k" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>elbow method</p></li>
<li><p>silhouette score</p></li>
</ul>
</section>
</section>
<section id="random-forest">
<h3>Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">#</a></h3>
</section>
<section id="dimensionality-reduction-algorithms">
<h3>Dimensionality Reduction Algorithms<a class="headerlink" href="#dimensionality-reduction-algorithms" title="Permalink to this headline">#</a></h3>
</section>
<section id="gradient-boosting-algorithms">
<h3>Gradient Boosting algorithms<a class="headerlink" href="#gradient-boosting-algorithms" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://www.quora.com/What-is-boosting-in-ML/answer/Mike-West-99?ch=10&amp;oid=250818536&amp;share=4eb23b20&amp;srid=hM1JX&amp;target_type=answer">https://www.quora.com/What-is-boosting-in-ML/answer/Mike-West-99?ch=10&amp;oid=250818536&amp;share=4eb23b20&amp;srid=hM1JX&amp;target_type=answer</a></p>
<p>Gradient boosting is a <strong>power technique for building predictive models</strong>. Gradient Boosting is about taking a model that by itself is a <strong>weak predictive model</strong> and <strong>combining that model with other models of the same type</strong> to produce a <strong>more accurate model</strong>. The idea is to <strong>compute a sequence of simple decisions trees</strong>, where <strong>each successive tree</strong> is built for the <strong>prediction residuals of the preceding tree</strong>.</p>
<p>In gradient boosting the <strong>weak model</strong> is called a <strong>weak learner</strong>. The term used when <strong>combining various machine learning models</strong> is an <strong>ensemble</strong>. The <strong>weak learner in XGBoost is a decision tree</strong>.</p>
<p>Therefore, we need to understand <strong>how a decision tree works</strong> before we can understand boosting.</p>
<ul class="simple">
<li><p><strong>Root node</strong></p>
<ul>
<li><p>The start of the tree is called the root node.</p></li>
</ul>
</li>
<li><p><strong>Decision node</strong></p>
<ul>
<li><p>After the initial split are decision nodes.</p></li>
</ul>
</li>
<li><p><strong>leaf node</strong></p>
<ul>
<li><p>will often lead to the answer or to the predicted value</p></li>
</ul>
</li>
<li><p>The Titanic dataset</p>
<ul>
<li><p>some groups of people were more likely to survive than others.</p>
<ul>
<li><p>The target variable, the attribute we want to <strong>predict is survived</strong>.</p></li>
<li><p>Because the target variable is a 1 or a 0 this is a <strong>binary classification problem</strong>.</p></li>
</ul>
</li>
<li><p><strong>How would a decision tree be applied</strong> to this dataset</p>
<ul>
<li><p>look for the <strong>attribute that is the most important</strong> in the dataset.</p></li>
<li><p>then <strong>analyze the second most import attribute</strong> in the dataset</p></li>
<li><p>creating a tree involves <strong>deciding which features to choose</strong> and what <strong>conditions to use for splitting</strong>, along with knowing when to stop</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<section id="gbm">
<h4>GBM<a class="headerlink" href="#gbm" title="Permalink to this headline">#</a></h4>
</section>
<section id="xgboost">
<h4>XGBoost<a class="headerlink" href="#xgboost" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://logikbot.quora.com/XGBoost-The-King-of-the-Boosters-Why-should-you-be-using-XGBoost-The-simple-answer-is-that-nothing-beats-it-on-struc?ch=10&amp;oid=11410205&amp;share=956e2403&amp;srid=hM1JX&amp;target_type=post">https://logikbot.quora.com/XGBoost-The-King-of-the-Boosters-Why-should-you-be-using-XGBoost-The-simple-answer-is-that-nothing-beats-it-on-struc?ch=10&amp;oid=11410205&amp;share=956e2403&amp;srid=hM1JX&amp;target_type=post</a></p>
</section>
<section id="lightgbm">
<h4>LightGBM<a class="headerlink" href="#lightgbm" title="Permalink to this headline">#</a></h4>
</section>
<section id="catboost">
<h4>CatBoost<a class="headerlink" href="#catboost" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/component-reference">Microsoft - ML Reference</a>
<img src="https://docs.microsoft.com/en-us/azure/machine-learning/media/algorithm-cheat-sheet/machine-learning-algorithm-cheat-sheet.png#lightbox"></p>
</section>
</section>
</section>
<section id="regression-predict-a-value">
<h2>Regression - Predict a value<a class="headerlink" href="#regression-predict-a-value" title="Permalink to this headline">#</a></h2>
<section id="boosted-decision-tree-regression">
<h3>Boosted Decision Tree Regression<a class="headerlink" href="#boosted-decision-tree-regression" title="Permalink to this headline">#</a></h3>
</section>
<section id="decision-forest-regression">
<h3>Decision Forest Regression<a class="headerlink" href="#decision-forest-regression" title="Permalink to this headline">#</a></h3>
</section>
<section id="fast-forest-quantile-regression">
<h3>Fast Forest Quantile Regression<a class="headerlink" href="#fast-forest-quantile-regression" title="Permalink to this headline">#</a></h3>
</section>
<section id="id3">
<h3>Linear Regression<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h3>
</section>
<section id="local-linear-regression">
<h3>Local Linear Regression<a class="headerlink" href="#local-linear-regression" title="Permalink to this headline">#</a></h3>
</section>
<section id="locally-weighted-linear-regression-lwr">
<h3>Locally Weighted Linear Regression (LWR)<a class="headerlink" href="#locally-weighted-linear-regression-lwr" title="Permalink to this headline">#</a></h3>
<p>Any parametric model can be made local if the fitting method accommodates observation weights. This is a variant of linear regression where the weights of each training example in the cost function is defined as</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ w^{(i)}(x) = \exp \left( -\frac{(x^{(i)} - x)^{2}}{2\tau^{2}} \right) \]</div>
</div></blockquote>
</section>
<section id="neural-network-regression">
<h3>Neural Network Regression<a class="headerlink" href="#neural-network-regression" title="Permalink to this headline">#</a></h3>
</section>
<section id="poisson-regression">
<h3>Poisson Regression<a class="headerlink" href="#poisson-regression" title="Permalink to this headline">#</a></h3>
</section>
<section id="quantile-regression">
<h3>Quantile Regression<a class="headerlink" href="#quantile-regression" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="classification-predict-a-class-choose-from-binary-two-class-or-multiclass-algorithms">
<h2>Classification - Predict a class. Choose from binary (two-class) or multiclass algorithms.<a class="headerlink" href="#classification-predict-a-class-choose-from-binary-two-class-or-multiclass-algorithms" title="Permalink to this headline">#</a></h2>
<section id="multiclass-boosted-decision-tree">
<h3>Multiclass Boosted Decision Tree<a class="headerlink" href="#multiclass-boosted-decision-tree" title="Permalink to this headline">#</a></h3>
</section>
<section id="multiclass-decision-forest">
<h3>Multiclass Decision Forest<a class="headerlink" href="#multiclass-decision-forest" title="Permalink to this headline">#</a></h3>
</section>
<section id="multiclass-logistic-regression-softmax-regression">
<h3>Multiclass Logistic Regression (Softmax Regression)<a class="headerlink" href="#multiclass-logistic-regression-softmax-regression" title="Permalink to this headline">#</a></h3>
<p>Softmax Regression is a generalization of logistic regression where we want to handle multiple classes instead of two classes <span class="math notranslate nohighlight">\((y^{(i)} \in \{0,1\})\)</span></p>
<ul class="simple">
<li><p><a class="reference external" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/</a></p></li>
</ul>
</section>
<section id="multiclass-neural-network">
<h3>Multiclass Neural Network<a class="headerlink" href="#multiclass-neural-network" title="Permalink to this headline">#</a></h3>
</section>
<section id="one-vs-all-multiclass">
<h3>One vs. All Multiclass<a class="headerlink" href="#one-vs-all-multiclass" title="Permalink to this headline">#</a></h3>
</section>
<section id="one-vs-one-multiclass">
<h3>One vs. One Multiclass<a class="headerlink" href="#one-vs-one-multiclass" title="Permalink to this headline">#</a></h3>
</section>
<section id="two-class-averaged-perceptron">
<h3>Two-Class Averaged Perceptron<a class="headerlink" href="#two-class-averaged-perceptron" title="Permalink to this headline">#</a></h3>
</section>
<section id="two-class-boosted-decision-tree">
<h3>Two-Class Boosted Decision Tree<a class="headerlink" href="#two-class-boosted-decision-tree" title="Permalink to this headline">#</a></h3>
</section>
<section id="two-class-decision-forest">
<h3>Two-Class Decision Forest<a class="headerlink" href="#two-class-decision-forest" title="Permalink to this headline">#</a></h3>
</section>
<section id="two-class-logistic-regression">
<h3>Two-Class Logistic Regression<a class="headerlink" href="#two-class-logistic-regression" title="Permalink to this headline">#</a></h3>
<p><a href="#Logistic-Regression">Logistic Regression</a></p>
<p>The hypothesis looks like</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}\
\begin{aligned}\
h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}}\\
\end{aligned} \
\end{equation}\
\end{split}\]</div>
<p>and the model parameters <span class="math notranslate nohighlight">\(\theta\)</span> are trained to minimize the cost function</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}\
\begin{aligned}\
 J(\theta) &amp;= - \left [ \sum\limits_{i=0}^{n}y^{(i)}\log\sigma(\theta^{T}x^{(i)}) + (1-y^{(i)}) \log[1-\sigma(\theta^{T}x^{(i)})] \right]  \\
 &amp;= -\left [ \sum\limits_{i=0}^{n}y^{(i)}\log h_{\theta}(x^{(i)}) + (1-y^{(i)}) \log[1- h_{\theta}(x^{(i)})) ]\right] \\
\end{aligned} \
\end{equation}\
\end{split}\]</div>
</section>
<section id="two-class-neural-network">
<h3>Two-Class Neural Network<a class="headerlink" href="#two-class-neural-network" title="Permalink to this headline">#</a></h3>
</section>
<section id="two-class-support-vector-machine">
<h3>Two Class Support Vector Machine<a class="headerlink" href="#two-class-support-vector-machine" title="Permalink to this headline">#</a></h3>
</section>
<section id="binary-vs-multiclass-vs-multilabel-classification">
<h3>Binary vs Multiclass vs MultiLabel classification<a class="headerlink" href="#binary-vs-multiclass-vs-multilabel-classification" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>binary classification</p>
<ul>
<li><p>classifies observation into one of two possible outcomes</p></li>
<li><p>finds a way to separate data from two classes</p></li>
<li><p>examples</p>
<ul>
<li><p>ad will be clicked or not</p></li>
<li><p>spam or not spam</p></li>
</ul>
</li>
</ul>
</li>
<li><p>multiclass classification</p>
<ul>
<li><p>also referred as multinomial classification</p></li>
<li><p>examples</p>
<ul>
<li><p>handwritten zip codes</p></li>
</ul>
</li>
</ul>
</li>
<li><p>multilabel classification</p></li>
</ul>
</section>
</section>
<section id="id4">
<h2>Linear Regression<a class="headerlink" href="#id4" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.mathworks.com/help/stats/linear-regression.html">https://www.mathworks.com/help/stats/linear-regression.html</a></p></li>
</ul>
<section id="multiple-linear-regression">
<h3>Multiple Linear Regression<a class="headerlink" href="#multiple-linear-regression" title="Permalink to this headline">#</a></h3>
</section>
<section id="multivariate-linear-regression">
<h3>Multivariate Linear Regression<a class="headerlink" href="#multivariate-linear-regression" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="supervised-learning-classification-regression">
<h2>Supervised learning (Classification, Regression)<a class="headerlink" href="#supervised-learning-classification-regression" title="Permalink to this headline">#</a></h2>
<section id="decision-trees">
<h3>Decision trees<a class="headerlink" href="#decision-trees" title="Permalink to this headline">#</a></h3>
</section>
<section id="regression-trees">
<h3>Regression trees<a class="headerlink" href="#regression-trees" title="Permalink to this headline">#</a></h3>
</section>
<section id="ensembles">
<h3>Ensembles<a class="headerlink" href="#ensembles" title="Permalink to this headline">#</a></h3>
</section>
<section id="k-nn">
<h3>k-NN<a class="headerlink" href="#k-nn" title="Permalink to this headline">#</a></h3>
</section>
<section id="id5">
<h3>Linear regression<a class="headerlink" href="#id5" title="Permalink to this headline">#</a></h3>
</section>
<section id="id6">
<h3>Naive Bayes<a class="headerlink" href="#id6" title="Permalink to this headline">#</a></h3>
</section>
<section id="artificial-neural-networks">
<h3>Artificial neural networks<a class="headerlink" href="#artificial-neural-networks" title="Permalink to this headline">#</a></h3>
</section>
<section id="id7">
<h3>Logistic regression<a class="headerlink" href="#id7" title="Permalink to this headline">#</a></h3>
</section>
<section id="perceptron">
<h3>Perceptron<a class="headerlink" href="#perceptron" title="Permalink to this headline">#</a></h3>
<p>The perceptron is an algorithm for learning a binary classifier called a threshold function: a function that maps its input <span class="math notranslate nohighlight">\(\mathbf {x}\)</span> (a real-valued vector) to an output value <span class="math notranslate nohighlight">\(f(\mathbf {x} )\)</span> (a single binary value):</p>
<div class="math notranslate nohighlight">
\[\begin{split} f(\mathbf {x} )={\begin{cases}1&amp;{\text{if }}\ \mathbf {w} \cdot \mathbf {x} +b&gt;0,\\0&amp;{\text{otherwise}}\end{cases}}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf {w}\)</span>  is a vector of real-valued weights, <span class="math notranslate nohighlight">\( \mathbf {w} \cdot \mathbf {x} \)</span> is the dot product <span class="math notranslate nohighlight">\( \sum _{i=1}^{m}w_{i}x_{i}\)</span>, where m is the number of inputs to the perceptron, and b is the bias. The bias shifts the decision boundary away from the origin and does not depend on any input value.</p>
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/1000px-Perceptron_example.svg.png" width=500 height=500>
<p><span class="math notranslate nohighlight">\(\tiny\text{Wikipedia}\)</span></p>
</section>
<section id="relevance-vector-machine-rvm">
<h3>Relevance vector machine (RVM)<a class="headerlink" href="#relevance-vector-machine-rvm" title="Permalink to this headline">#</a></h3>
</section>
<section id="support-vector-machine-svm">
<h3>Support vector machine (SVM)<a class="headerlink" href="#support-vector-machine-svm" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="id8">
<h2>Unsupervised learning<a class="headerlink" href="#id8" title="Permalink to this headline">#</a></h2>
<section id="k-means-clustering">
<h3>K-means clustering<a class="headerlink" href="#k-means-clustering" title="Permalink to this headline">#</a></h3>
</section>
<section id="knn-k-nearest-neighbors">
<h3>KNN (k-nearest neighbors)<a class="headerlink" href="#knn-k-nearest-neighbors" title="Permalink to this headline">#</a></h3>
</section>
<section id="hierarchal-clustering">
<h3>Hierarchal clustering<a class="headerlink" href="#hierarchal-clustering" title="Permalink to this headline">#</a></h3>
</section>
<section id="anomaly-detection">
<h3>Anomaly detection<a class="headerlink" href="#anomaly-detection" title="Permalink to this headline">#</a></h3>
</section>
<section id="neural-networks">
<h3>Neural Networks<a class="headerlink" href="#neural-networks" title="Permalink to this headline">#</a></h3>
</section>
<section id="principle-component-analysis">
<h3>Principle Component Analysis<a class="headerlink" href="#principle-component-analysis" title="Permalink to this headline">#</a></h3>
</section>
<section id="independent-component-analysis">
<h3>Independent Component Analysis<a class="headerlink" href="#independent-component-analysis" title="Permalink to this headline">#</a></h3>
</section>
<section id="apriori-algorithm">
<h3>Apriori algorithm<a class="headerlink" href="#apriori-algorithm" title="Permalink to this headline">#</a></h3>
</section>
<section id="singular-value-decomposition">
<h3>Singular value decomposition<a class="headerlink" href="#singular-value-decomposition" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="clustering">
<h2>Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">#</a></h2>
<section id="birch">
<h3>BIRCH<a class="headerlink" href="#birch" title="Permalink to this headline">#</a></h3>
</section>
<section id="cure">
<h3>CURE<a class="headerlink" href="#cure" title="Permalink to this headline">#</a></h3>
</section>
<section id="hierarchical">
<h3>Hierarchical<a class="headerlink" href="#hierarchical" title="Permalink to this headline">#</a></h3>
</section>
<section id="id9">
<h3>k-means<a class="headerlink" href="#id9" title="Permalink to this headline">#</a></h3>
</section>
<section id="expectationmaximization-em">
<h3>Expectation–maximization (EM)<a class="headerlink" href="#expectationmaximization-em" title="Permalink to this headline">#</a></h3>
</section>
<section id="dbscan">
<h3>DBSCAN<a class="headerlink" href="#dbscan" title="Permalink to this headline">#</a></h3>
</section>
<section id="optics">
<h3>OPTICS<a class="headerlink" href="#optics" title="Permalink to this headline">#</a></h3>
</section>
<section id="mean-shift">
<h3>Mean shift<a class="headerlink" href="#mean-shift" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="dimensionality-reduction">
<h2>Dimensionality Reduction<a class="headerlink" href="#dimensionality-reduction" title="Permalink to this headline">#</a></h2>
<section id="factor-analysis">
<h3>Factor analysis<a class="headerlink" href="#factor-analysis" title="Permalink to this headline">#</a></h3>
</section>
<section id="cca">
<h3>CCA<a class="headerlink" href="#cca" title="Permalink to this headline">#</a></h3>
</section>
<section id="ica">
<h3>ICA<a class="headerlink" href="#ica" title="Permalink to this headline">#</a></h3>
</section>
<section id="lda">
<h3>LDA<a class="headerlink" href="#lda" title="Permalink to this headline">#</a></h3>
</section>
<section id="nmf">
<h3>NMF<a class="headerlink" href="#nmf" title="Permalink to this headline">#</a></h3>
</section>
<section id="pca">
<h3>PCA<a class="headerlink" href="#pca" title="Permalink to this headline">#</a></h3>
</section>
<section id="pgd">
<h3>PGD<a class="headerlink" href="#pgd" title="Permalink to this headline">#</a></h3>
</section>
<section id="t-sne">
<h3>t-SNE<a class="headerlink" href="#t-sne" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="ensemble-methods">
<h2>Ensemble Methods<a class="headerlink" href="#ensemble-methods" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>An ensemble of classifiers is a classifier build upon some combination of weak learners</p></li>
<li><p>strategy is to learn many weak classifiers and combine them in some way, instead of trying to learn a single strong classifier</p></li>
<li><p>rationale is, it is easy to train several simple classifiers and combine them into a more complex classifier than to learn a single complex classifier</p></li>
</ul>
<section id="bootstrap-aggregating-bagging">
<h3>Bootstrap aggregating (Bagging)<a class="headerlink" href="#bootstrap-aggregating-bagging" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>sample with replacement from the dataset</p></li>
<li><p>diversity in ensemble is ensured by the variation within bootstrap set</p></li>
<li><p>best suited for small available training datasets</p></li>
<li><p>reduces variance for those algorithms that have high variance, eg CART</p></li>
</ul>
</section>
<section id="bagging-vs-boosting">
<h3>Bagging vs Boosting<a class="headerlink" href="#bagging-vs-boosting" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>In bagging,</p>
<ul>
<li><p>instances selected to train individual classifiers are bootstrapped replicas of the training data,</p>
<ul>
<li><p>which means that each instance has equal chance of being in each training dataset.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>In boosting,</p>
<ul>
<li><p>the training dataset for each subsequent classifier increasingly focuses on instances misclassified by previously generated classifiers.</p></li>
</ul>
</li>
</ul>
</section>
<section id="boosting">
<h3>Boosting<a class="headerlink" href="#boosting" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Boosting, in binary class problems, creates sets of three weak classifiers at a time:</p>
<ul>
<li><p>the first classifier (or hypothesis) h1 is trained on a random subset of the available training data, similar to bagging.</p></li>
<li><p>te second classifier, h2, is trained on a different subset of the original dataset, precisely half of which is correctly identified by h1, and the other half is misclassified. Such a training subset is said to be the “most informative,” given the decision of h1.</p></li>
<li><p>the third classifier h3 is then trained with instances on which h1 and h2 disagree.</p></li>
</ul>
</li>
<li><p>These three classifiers are then combined through a three-way majority vote.</p></li>
<li><p>Schapire proved that the training error of this three-classifier ensemble is bounded above by <span class="math notranslate nohighlight">\(g(\epsilon) &lt; 3\epsilon^2 - 2\epsilon^3\)</span>, where <span class="math notranslate nohighlight">\(\epsilon\)</span> is the error of any of the three classifiers, provided that each classifier has an error rate <span class="math notranslate nohighlight">\(\epsilon \lt 0.5\)</span>, the least we can expect from a classifier on a binary classification problem.</p></li>
</ul>
<section id="adaboost">
<h4>AdaBoost<a class="headerlink" href="#adaboost" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>AdaBoost(Adaptive Boosting) - Example of Boosting</p>
<ul>
<li><p>extended the original boosting algorithm to multiple classes</p>
<ul>
<li><p>multiple classes (AdaBoost.M1, AdaBost.M2)</p></li>
<li><p>regression problems (AdaBoost.R)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>2 fundamental differences</p>
<ul>
<li><p>instances are drawn into subsequent datasets from an iteratively updated sample distribution of the training data</p></li>
<li><p>classifiers are combined through weighted majority voting</p>
<ul>
<li><p>voting weights are based on classifiers training errors</p></li>
<li><p>the distribution weights of instances correctly classified by the current hypothesis are reduced by a factor</p></li>
<li><p>whereas weights of misclassified instances are left unchanged</p></li>
</ul>
</li>
<li><p>minimizes the exponential loss function that make algorithm sensitive to outlier</p></li>
</ul>
</li>
</ul>
<img src="./images/adaBoost_algo.png">
</section>
<section id="gradient-tree-boosting">
<h4>Gradient Tree Boosting<a class="headerlink" href="#gradient-tree-boosting" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>Gradient Tree Boosting or Gradient Boosted Decision Tree (GBDT)</p></li>
<li><p>3 main components</p>
<ul>
<li><p>additive model</p></li>
<li><p>loss function</p>
<ul>
<li><p>solves differentiable loss function</p></li>
<li><p>can be used both for classification and regression problem</p></li>
</ul>
</li>
<li><p>weak learner</p></li>
</ul>
</li>
<li><p>at m-th step, fits a decision tree <span class="math notranslate nohighlight">\(h_m(x)\)</span> to residuals</p></li>
</ul>
</section>
<section id="id10">
<h4>XGBoost<a class="headerlink" href="#id10" title="Permalink to this headline">#</a></h4>
</section>
</section>
<section id="id11">
<h3>Random Forest<a class="headerlink" href="#id11" title="Permalink to this headline">#</a></h3>
</section>
<section id="stacking">
<h3>Stacking<a class="headerlink" href="#stacking" title="Permalink to this headline">#</a></h3>
</section>
<section id="bayes-optimal-classifier">
<h3>Bayes optimal classifier<a class="headerlink" href="#bayes-optimal-classifier" title="Permalink to this headline">#</a></h3>
</section>
<section id="bayesian-model-averaging-bma">
<h3>Bayesian model averaging (BMA)<a class="headerlink" href="#bayesian-model-averaging-bma" title="Permalink to this headline">#</a></h3>
</section>
<section id="bayesian-model-combination-bmc">
<h3>Bayesian model combination (BMC)<a class="headerlink" href="#bayesian-model-combination-bmc" title="Permalink to this headline">#</a></h3>
</section>
<section id="bucket-of-models">
<h3>Bucket of models<a class="headerlink" href="#bucket-of-models" title="Permalink to this headline">#</a></h3>
</section>
<section id="voting-complement-of-bagging">
<h3>Voting (complement of Bagging)<a class="headerlink" href="#voting-complement-of-bagging" title="Permalink to this headline">#</a></h3>
</section>
<section id="blending-subtype-of-stacking">
<h3>Blending (subtype of Stacking)<a class="headerlink" href="#blending-subtype-of-stacking" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="ensemble-learning-techniques">
<h2>Ensemble Learning Techniques<a class="headerlink" href="#ensemble-learning-techniques" title="Permalink to this headline">#</a></h2>
<section id="type-of-models-used">
<h3>Type of models used<a class="headerlink" href="#type-of-models-used" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Homogeneous models</p></li>
<li><p>Heterogeneous models</p></li>
</ul>
</section>
<section id="data-sampling">
<h3>Data Sampling<a class="headerlink" href="#data-sampling" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>With replacement</p></li>
<li><p>Without replacement</p></li>
<li><p>k-fold</p></li>
</ul>
</section>
<section id="decision-function">
<h3>Decision Function<a class="headerlink" href="#decision-function" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Voting</p></li>
<li><p>Average</p></li>
<li><p>Meta Model</p></li>
</ul>
</section>
<section id="types-of-learners">
<h3>Types of learners<a class="headerlink" href="#types-of-learners" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Weak learners</p>
<ul>
<li><p>slightly better than random guess</p></li>
</ul>
</li>
<li><p>Strong learners</p>
<ul>
<li><p>very accurate predictions</p></li>
</ul>
</li>
</ul>
</section>
<section id="types-of-ensemble-methods">
<h3>Types of ensemble methods<a class="headerlink" href="#types-of-ensemble-methods" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Decrease Variance - Bagging</p></li>
<li><p>Decrease Bias - Boosting</p></li>
<li><p>Improve Prediction - Stacking</p></li>
</ul>
</section>
<section id="types-of-ensemble-learners">
<h3>Types of ensemble learners<a class="headerlink" href="#types-of-ensemble-learners" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Sequential Learners</p>
<ul>
<li><p>mistakes of previous models are learned by their successors</p>
<ul>
<li><p>e.g., AdaBoost</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Parallel Learners</p>
<ul>
<li><p>exploits independence between models by averaging out their mistakes</p>
<ul>
<li><p>e.g., Random Forest</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="structured-prediction-graphical-models">
<h2>Structured Prediction - Graphical models<a class="headerlink" href="#structured-prediction-graphical-models" title="Permalink to this headline">#</a></h2>
<section id="bayes-net">
<h3>Bayes net<a class="headerlink" href="#bayes-net" title="Permalink to this headline">#</a></h3>
</section>
<section id="conditional-random-field">
<h3>Conditional random field<a class="headerlink" href="#conditional-random-field" title="Permalink to this headline">#</a></h3>
</section>
<section id="hidden-markov">
<h3>Hidden Markov<a class="headerlink" href="#hidden-markov" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="id12">
<h2>Anomaly detection<a class="headerlink" href="#id12" title="Permalink to this headline">#</a></h2>
<section id="id13">
<h3>k-NN<a class="headerlink" href="#id13" title="Permalink to this headline">#</a></h3>
</section>
<section id="local-outlier-factor">
<h3>Local outlier factor<a class="headerlink" href="#local-outlier-factor" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="artificial-neural-network">
<h2>Artificial neural network<a class="headerlink" href="#artificial-neural-network" title="Permalink to this headline">#</a></h2>
<section id="autoencoder">
<h3>Autoencoder<a class="headerlink" href="#autoencoder" title="Permalink to this headline">#</a></h3>
</section>
<section id="cognitive-computing">
<h3>Cognitive computing<a class="headerlink" href="#cognitive-computing" title="Permalink to this headline">#</a></h3>
</section>
<section id="deep-learning">
<h3>Deep learning<a class="headerlink" href="#deep-learning" title="Permalink to this headline">#</a></h3>
</section>
<section id="deepdream">
<h3>DeepDream<a class="headerlink" href="#deepdream" title="Permalink to this headline">#</a></h3>
</section>
<section id="multilayer-perceptron">
<h3>Multilayer perceptron<a class="headerlink" href="#multilayer-perceptron" title="Permalink to this headline">#</a></h3>
<p>A <strong>perceptron</strong> was the name given to a model having one single linear layer and, if it has multiple layers, it is  called a <strong>multi-layer perceptron (MLP)</strong>. Note that the input and the output layers are visible from outside, while all the other layers in the middle are hidden – hence the name hidden layers. In this context, a single layer is simply a linear function and the MLP is therefore obtained by stacking multiple single layers one after the other:</p>
<img src="https://www.researchgate.net/profile/Hassan-Afzaal/publication/338103191/figure/fig2/AS:838599264174093@1576949053675/The-multilayer-perceptron-MLP-model-for-various-input-variable-combinations.jpg" width=500 height=500>
</section>
<section id="rnn">
<h3>RNN<a class="headerlink" href="#rnn" title="Permalink to this headline">#</a></h3>
<section id="lstm">
<h4>LSTM<a class="headerlink" href="#lstm" title="Permalink to this headline">#</a></h4>
</section>
<section id="gru">
<h4>GRU<a class="headerlink" href="#gru" title="Permalink to this headline">#</a></h4>
</section>
<section id="esn">
<h4>ESN<a class="headerlink" href="#esn" title="Permalink to this headline">#</a></h4>
</section>
</section>
<section id="restricted-boltzmann-machine">
<h3>Restricted Boltzmann machine<a class="headerlink" href="#restricted-boltzmann-machine" title="Permalink to this headline">#</a></h3>
</section>
<section id="gan">
<h3>GAN<a class="headerlink" href="#gan" title="Permalink to this headline">#</a></h3>
</section>
<section id="som">
<h3>SOM<a class="headerlink" href="#som" title="Permalink to this headline">#</a></h3>
</section>
<section id="convolutional-neural-network">
<h3>Convolutional neural network<a class="headerlink" href="#convolutional-neural-network" title="Permalink to this headline">#</a></h3>
<section id="u-net">
<h4>U-Net<a class="headerlink" href="#u-net" title="Permalink to this headline">#</a></h4>
</section>
</section>
<section id="transformer-vision">
<h3>Transformer Vision<a class="headerlink" href="#transformer-vision" title="Permalink to this headline">#</a></h3>
</section>
<section id="spiking-neural-network">
<h3>Spiking neural network<a class="headerlink" href="#spiking-neural-network" title="Permalink to this headline">#</a></h3>
</section>
<section id="memtransistor">
<h3>Memtransistor<a class="headerlink" href="#memtransistor" title="Permalink to this headline">#</a></h3>
</section>
<section id="electrochemical-ram-ecram">
<h3>Electrochemical RAM (ECRAM)<a class="headerlink" href="#electrochemical-ram-ecram" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="id14">
<h2>Reinforcement Learning<a class="headerlink" href="#id14" title="Permalink to this headline">#</a></h2>
<section id="q-learning">
<h3>Q-learning<a class="headerlink" href="#q-learning" title="Permalink to this headline">#</a></h3>
</section>
<section id="sarsa">
<h3>SARSA<a class="headerlink" href="#sarsa" title="Permalink to this headline">#</a></h3>
</section>
<section id="temporal-difference-td">
<h3>Temporal difference (TD)<a class="headerlink" href="#temporal-difference-td" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="machine-learning-theory">
<h2>Machine Learning Theory<a class="headerlink" href="#machine-learning-theory" title="Permalink to this headline">#</a></h2>
<section id="kernel-machines">
<h3>Kernel machines<a class="headerlink" href="#kernel-machines" title="Permalink to this headline">#</a></h3>
</section>
<section id="biasvariance-tradeoff">
<h3>Bias–variance tradeoff<a class="headerlink" href="#biasvariance-tradeoff" title="Permalink to this headline">#</a></h3>
</section>
<section id="computational-learning-theory">
<h3>Computational learning theory<a class="headerlink" href="#computational-learning-theory" title="Permalink to this headline">#</a></h3>
</section>
<section id="empirical-risk-minimization">
<h3>Empirical risk minimization<a class="headerlink" href="#empirical-risk-minimization" title="Permalink to this headline">#</a></h3>
</section>
<section id="occam-learning">
<h3>Occam learning<a class="headerlink" href="#occam-learning" title="Permalink to this headline">#</a></h3>
</section>
<section id="pac-learning">
<h3>PAC learning<a class="headerlink" href="#pac-learning" title="Permalink to this headline">#</a></h3>
</section>
<section id="statistical-learning">
<h3>Statistical learning<a class="headerlink" href="#statistical-learning" title="Permalink to this headline">#</a></h3>
</section>
<section id="vc-theory">
<h3>VC theory<a class="headerlink" href="#vc-theory" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="recommender-system-methods-and-challenges">
<h2>Recommender System - Methods and challenges<a class="headerlink" href="#recommender-system-methods-and-challenges" title="Permalink to this headline">#</a></h2>
<section id="cold-start">
<h3>Cold start<a class="headerlink" href="#cold-start" title="Permalink to this headline">#</a></h3>
</section>
<section id="collaborative-filtering">
<h3>Collaborative filtering<a class="headerlink" href="#collaborative-filtering" title="Permalink to this headline">#</a></h3>
</section>
<section id="id15">
<h3>Dimensionality reduction<a class="headerlink" href="#id15" title="Permalink to this headline">#</a></h3>
</section>
<section id="implicit-data-collection">
<h3>Implicit data collection<a class="headerlink" href="#implicit-data-collection" title="Permalink to this headline">#</a></h3>
</section>
<section id="item-item-collaborative-filtering">
<h3>Item-item collaborative filtering<a class="headerlink" href="#item-item-collaborative-filtering" title="Permalink to this headline">#</a></h3>
</section>
<section id="matrix-factorization">
<h3>Matrix factorization<a class="headerlink" href="#matrix-factorization" title="Permalink to this headline">#</a></h3>
</section>
<section id="preference-elicitation">
<h3>Preference elicitation<a class="headerlink" href="#preference-elicitation" title="Permalink to this headline">#</a></h3>
</section>
<section id="similarity-search">
<h3>Similarity search<a class="headerlink" href="#similarity-search" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="most-popular-deep-learning-algorithms">
<h2>10 most popular deep learning algorithms<a class="headerlink" href="#most-popular-deep-learning-algorithms" title="Permalink to this headline">#</a></h2>
<p><a class="reference external" href="https://www.simplilearn.com/tutorials/deep-learning-tutorial/deep-learning-algorithm">Link</a></p>
<section id="convolutional-neural-networks-cnns">
<h3>Convolutional Neural Networks (CNNs)<a class="headerlink" href="#convolutional-neural-networks-cnns" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Computer Vision with Deep Learning has been constructed and perfected with time, primarily over one particular algorithm — a <strong>Convolutional Neural Network(ConvNet/CNN)</strong>.</p></li>
<li><p>Neural networks came to prominence in <strong>2012</strong> as machine learning expert <strong>Alex Krizhevsky</strong> utilized them to get <strong>first prize in the ImageNet competition</strong>.</p></li>
<li><p>Applications</p>
<ul>
<li><p>Facebook’s famous automatic tagging algorithm works? The answer is neural networks.</p></li>
<li><p>product recommendation you get on Amazon and several other similar platforms is because of neural networks.</p></li>
<li><p>Neural networks are the reason behind Google’s superb image searching abilities.</p></li>
<li><p>Instagram’s solid search infrastructure is possible because the social media network uses neural networks.</p></li>
</ul>
</li>
<li><p>A convolutional network <strong>ingests such images</strong> as three separate strata of color <strong>stacked one on top of the other</strong>. A normal color image is seen as a rectangular box whose width and height are measured by the number of pixels from those dimensions. The depth layers in the three layers of colours(RGB) interpreted by CNNs are referred to as channels.</p></li>
<li><p>A ConvNet is able to successfully <strong>capture the Spatial and Temporal dependencies</strong> in an image through the application of relevant filters.</p></li>
<li><p>The <strong>role of the ConvNet</strong> is to <strong>reduce the images</strong> into a form which is easier to process, <strong>without losing features</strong> which are critical for getting a good prediction.</p></li>
<li><p>The objective of the Convolution Operation is to <strong>extract the high-level features</strong> such as edges, from the input image. ConvNets need not be limited to only one Convolutional Layer. Conventionally, the <strong>first ConvLayer</strong> is responsible for <strong>capturing the Low-Level features</strong> such as edges, color, gradient orientation, etc.</p></li>
<li><p>CNN’s have a <strong>ReLU layer</strong> to <strong>perform operations on elements</strong>. The output is a rectified feature map.</p></li>
<li><p>the <strong>Pooling layer</strong> is responsible for <strong>reducing the spatial size of the Convolved Feature</strong>. This is to <strong>decrease the computational power required to process the data through dimensionality reduction</strong>. Furthermore, it is useful for <strong>extracting dominant features which are rotational and positional invariant</strong>, thus maintaining the process of effectively training of the model.</p></li>
<li><p><strong>Adding a Fully-Connected layer</strong> is a (usually) <strong>cheap way of learning non-linear combinations of the high-level features</strong> as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space.</p></li>
<li><p>There are <strong>various architectures of CNNs available</strong> which have been key in building algorithms which power and shall power AI as a whole in the foreseeable future. Some of them have been listed below:</p>
<ul>
<li><p>LeNet</p></li>
<li><p>AlexNet</p></li>
<li><p>VGGNet</p></li>
<li><p>GoogLeNet</p></li>
<li><p>ResNet</p></li>
<li><p>ZFNet</p></li>
</ul>
</li>
</ul>
<img src="https://miro.medium.com/max/1400/1*qtinjiZct2w7Dr4XoFixnA.gif">
<div class="math notranslate nohighlight">
\[\text{Convolutional Layer}\]</div>
<hr class="docutils" />
<img src="https://miro.medium.com/max/1400/1*uAeANQIOQPqWZnnuH-VEyw.jpeg">
<div class="math notranslate nohighlight">
\[\text{A CNN sequence to classify handwritten digits}\]</div>
<hr class="docutils" />
<img src="https://miro.medium.com/max/850/1*GLQjM9k0gZ14nYF0XmkRWQ.png" width=500 height=500>
<div class="math notranslate nohighlight">
\[\text{Flattened 3x3 image matrix 1 dimension}\]</div>
<hr class="docutils" />
<img src="https://miro.medium.com/max/1052/1*GcI7G-JLAQiEoCON7xFbhg.gif" width=500 height=500>
<div class="math notranslate nohighlight">
\[\text{Convolution Layer — The Kernel - Convoluting a 5x5x1 image(Green) with a 3x3x1 kernel(Yellow) to get a 3x3x1(Red) convolved feature}\]</div>
<hr class="docutils" />
<img src="https://miro.medium.com/max/1400/1*ciDgQEjViWLnCbmX-EeSrA.gif">
<div class="math notranslate nohighlight">
\[\text{Convolution operation on a MxNx3 image matrix with a 3x3x3 Kernel}\]</div>
<hr class="docutils" />
<img src="https://miro.medium.com/max/790/1*1VJDP6qDY9-ExTuQVEOlVg.gif" width=500 height=500>
<div class="math notranslate nohighlight">
\[\text{Convolution Operation with Stride Length = 2}\]</div>
<hr class="docutils" />
<img src="https://miro.medium.com/max/790/1*nYf_cUIHFEWU1JXGwnz-Ig.gif" width=500 height=500>
<div class="math notranslate nohighlight">
\[\text{SAME padding: 5x5x1 image is padded with 0s to create a 6x6x1 image}\]</div>
<hr class="docutils" />
<img src="https://miro.medium.com/max/1192/1*KQIEqhxzICU7thjaQBfPBQ.png" width=500 height=500>
<div class="math notranslate nohighlight">
\[\text{Types of Pooling}\]</div>
<hr class="docutils" />
<img src="https://miro.medium.com/max/1400/1*kToStLowjokojIQ7pY2ynQ.jpeg" width=500 height=500>
<div class="math notranslate nohighlight">
\[\text{Classification — Fully Connected Layer (FC Layer)}\]</div>
<hr class="docutils" />
<ul class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53</a></p></li>
<li><p><a class="reference external" href="https://github.com/ss-is-master-chief/MNIST-Digit.Recognizer-CNNs/blob/master/MNIST-Hand.Written.Digit.Recognition-CNN.ipynb">https://github.com/ss-is-master-chief/MNIST-Digit.Recognizer-CNNs/blob/master/MNIST-Hand.Written.Digit.Recognition-CNN.ipynb</a></p></li>
<li><p><a class="reference external" href="https://medium.datadriveninvestor.com/introduction-to-how-cnns-work-77e0e4cde99b">https://medium.datadriveninvestor.com/introduction-to-how-cnns-work-77e0e4cde99b</a></p></li>
</ul>
</section>
<section id="long-short-term-memory-networks-lstms">
<h3>Long Short Term Memory Networks (LSTMs)<a class="headerlink" href="#long-short-term-memory-networks-lstms" title="Permalink to this headline">#</a></h3>
</section>
<section id="recurrent-neural-networks-rnns">
<h3>Recurrent Neural Networks (RNNs)<a class="headerlink" href="#recurrent-neural-networks-rnns" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><strong>Recurrent Neural Networks (RNNs)</strong> are widely used for data with some kind of <strong>sequential structure</strong>. For instance, <strong>time series data has an intrinsic ordering based on time</strong>. Sentences are also sequential, “I love dogs” has a different meaning than “Dogs I love.” Simply put, if the <strong>semantics of your data is altered by random permutation, you have a sequential dataset and RNNs may be used for your problem!</strong> To help solidify the types of problems RNNs can solve, here is a list of common applications :</p></li>
<li><p><strong>Applications</strong></p>
<ul>
<li><p>Speech Recognition</p></li>
<li><p>Sentiment Classification</p></li>
<li><p>Machine Translation (i.e. Chinese to English)</p></li>
<li><p>Video Activity Recognition</p></li>
<li><p>Name Entity Recognition — (i.e. Identifying names in a sentence)</p></li>
</ul>
</li>
<li><p>What are RNNs</p>
<ul>
<li><p>RNNs are <strong>different than the classical multi-layer perceptron (MLP) networks</strong> because of two main reasons:</p>
<ul>
<li><ol class="simple">
<li><p>They take into account what happened previously and</p></li>
</ol>
</li>
<li><ol class="simple">
<li><p>they share parameters/weights.</p></li>
</ol>
</li>
</ul>
</li>
<li><p>A recurrent neural network is a neural network that is specialized for processing a sequence of data x(t)= x(1), … , x(τ) with the time step index t ranging from 1 to τ. For tasks that involve sequential inputs, such as speech and language, it is often better to use RNNs. In a NLP problem, if you want to predict the next word in a sentence it is important to know the words before it. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far.</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p><a class="reference external" href="https://pub.towardsai.net/whirlwind-tour-of-rnns-a11effb7808f">https://pub.towardsai.net/whirlwind-tour-of-rnns-a11effb7808f</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85">https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85</a></p></li>
<li><p><a class="reference external" href="https://github.com/javaidnabi31/RNN-from-scratch/blob/master/RNN_char_text%20generator.ipynb">https://github.com/javaidnabi31/RNN-from-scratch/blob/master/RNN_char_text generator.ipynb</a></p></li>
<li><p><a class="reference external" href="https://www.deeplearningbook.org/contents/rnn.html">https://www.deeplearningbook.org/contents/rnn.html</a></p></li>
<li><p><a class="reference external" href="https://gist.github.com/karpathy/d4dee566867f8291f086">https://gist.github.com/karpathy/d4dee566867f8291f086</a></p></li>
<li><p><a class="reference external" href="https://www.coursera.org/learn/nlp-sequence-models/lecture/0h7gT/why-sequence-models">https://www.coursera.org/learn/nlp-sequence-models/lecture/0h7gT/why-sequence-models</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1610.02583.pdf">A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation</a></p></li>
</ul>
</section>
<section id="generative-adversarial-networks-gans">
<h3>Generative Adversarial Networks (GANs)<a class="headerlink" href="#generative-adversarial-networks-gans" title="Permalink to this headline">#</a></h3>
</section>
<section id="radial-basis-function-networks-rbfns">
<h3>Radial Basis Function Networks (RBFNs)<a class="headerlink" href="#radial-basis-function-networks-rbfns" title="Permalink to this headline">#</a></h3>
</section>
<section id="multilayer-perceptrons-mlps">
<h3>Multilayer Perceptrons (MLPs)<a class="headerlink" href="#multilayer-perceptrons-mlps" title="Permalink to this headline">#</a></h3>
</section>
<section id="self-organizing-maps-soms">
<h3>Self Organizing Maps (SOMs)<a class="headerlink" href="#self-organizing-maps-soms" title="Permalink to this headline">#</a></h3>
</section>
<section id="deep-belief-networks-dbns">
<h3>Deep Belief Networks (DBNs)<a class="headerlink" href="#deep-belief-networks-dbns" title="Permalink to this headline">#</a></h3>
</section>
<section id="restricted-boltzmann-machines-rbms">
<h3>Restricted Boltzmann Machines( RBMs)<a class="headerlink" href="#restricted-boltzmann-machines-rbms" title="Permalink to this headline">#</a></h3>
</section>
<section id="autoencoders">
<h3>Autoencoders<a class="headerlink" href="#autoencoders" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="optimization-algorithms-deep-learning">
<h2>Optimization algorithms - Deep Learning<a class="headerlink" href="#optimization-algorithms-deep-learning" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://ruder.io/optimizing-gradient-descent/">Optimizing GD</a></p></li>
<li><p><a class="reference external" href="https://github.com/Jaewan-Yun/optimizer-visualization">Optimizer Visualization</a></p></li>
</ul>
<section id="asgd">
<h3>ASGD<a class="headerlink" href="#asgd" title="Permalink to this headline">#</a></h3>
</section>
<section id="adadelta">
<h3>Adadelta<a class="headerlink" href="#adadelta" title="Permalink to this headline">#</a></h3>
</section>
<section id="adagrad">
<h3>Adagrad<a class="headerlink" href="#adagrad" title="Permalink to this headline">#</a></h3>
</section>
<section id="adam">
<h3>Adam<a class="headerlink" href="#adam" title="Permalink to this headline">#</a></h3>
</section>
<section id="adamw">
<h3>AdamW<a class="headerlink" href="#adamw" title="Permalink to this headline">#</a></h3>
</section>
<section id="adamax">
<h3>Adamax<a class="headerlink" href="#adamax" title="Permalink to this headline">#</a></h3>
</section>
<section id="lbfgs">
<h3>LBFGS<a class="headerlink" href="#lbfgs" title="Permalink to this headline">#</a></h3>
</section>
<section id="nadam">
<h3>NAdam<a class="headerlink" href="#nadam" title="Permalink to this headline">#</a></h3>
</section>
<section id="radam">
<h3>RAdam<a class="headerlink" href="#radam" title="Permalink to this headline">#</a></h3>
</section>
<section id="rmsprop">
<h3>RMSprop<a class="headerlink" href="#rmsprop" title="Permalink to this headline">#</a></h3>
</section>
<section id="rprop">
<h3>Rprop<a class="headerlink" href="#rprop" title="Permalink to this headline">#</a></h3>
</section>
<section id="sgd">
<h3>SGD<a class="headerlink" href="#sgd" title="Permalink to this headline">#</a></h3>
</section>
<section id="sparseadam">
<h3>SparseAdam<a class="headerlink" href="#sparseadam" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="evaluation-metrics-regression">
<h2>Evaluation Metrics - Regression<a class="headerlink" href="#evaluation-metrics-regression" title="Permalink to this headline">#</a></h2>
<section id="mean-absolute-error-mae">
<h3>Mean Absolute Error (MAE)<a class="headerlink" href="#mean-absolute-error-mae" title="Permalink to this headline">#</a></h3>
</section>
<section id="mean-squared-error-mse">
<h3>Mean Squared Error (MSE)<a class="headerlink" href="#mean-squared-error-mse" title="Permalink to this headline">#</a></h3>
</section>
<section id="root-mean-squared-error-rmse">
<h3>Root Mean Squared Error (RMSE)<a class="headerlink" href="#root-mean-squared-error-rmse" title="Permalink to this headline">#</a></h3>
</section>
<section id="r-squared-coefficient-of-determination">
<h3>R-Squared (Coefficient of determination)<a class="headerlink" href="#r-squared-coefficient-of-determination" title="Permalink to this headline">#</a></h3>
</section>
<section id="adjusted-r-squared">
<h3>Adjusted R-squared<a class="headerlink" href="#adjusted-r-squared" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="estimating-errors">
<h2>Estimating Errors<a class="headerlink" href="#estimating-errors" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Error Metrics</p>
<ul>
<li><p>MAE (Mean Absolute Error)</p></li>
<li><p>MSE (Mean Squared Error)</p></li>
</ul>
</li>
<li><p>Accuracy Metrics</p>
<ul>
<li><p>Precision</p></li>
<li><p>Recall</p></li>
</ul>
</li>
<li><p>Ranking Metrics</p>
<ul>
<li><p>MAP (Mean Average Precision)</p></li>
<li><p>AUC (Area Under the Curve)</p></li>
</ul>
</li>
</ul>
</section>
<section id="evaluation-metrics-distribution-fit">
<h2>Evaluation Metrics - Distribution Fit<a class="headerlink" href="#evaluation-metrics-distribution-fit" title="Permalink to this headline">#</a></h2>
<section id="bayesian-information-criterion">
<h3>Bayesian information criterion<a class="headerlink" href="#bayesian-information-criterion" title="Permalink to this headline">#</a></h3>
</section>
<section id="kolmogorovsmirnov-test">
<h3>Kolmogorov–Smirnov test<a class="headerlink" href="#kolmogorovsmirnov-test" title="Permalink to this headline">#</a></h3>
</section>
<section id="cramervon-mises-criterion">
<h3>Cramér–von Mises criterion<a class="headerlink" href="#cramervon-mises-criterion" title="Permalink to this headline">#</a></h3>
</section>
<section id="andersondarling-test">
<h3>Anderson–Darling test<a class="headerlink" href="#andersondarling-test" title="Permalink to this headline">#</a></h3>
</section>
<section id="shapirowilk-test">
<h3>Shapiro–Wilk test<a class="headerlink" href="#shapirowilk-test" title="Permalink to this headline">#</a></h3>
</section>
<section id="chi-square-test">
<h3>Chi-square test<a class="headerlink" href="#chi-square-test" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="http://www.medicine.mcgill.ca/epidemiology/joseph/courses/EPIB-621/fit.pdf">http://www.medicine.mcgill.ca/epidemiology/joseph/courses/EPIB-621/fit.pdf</a></p></li>
<li><p><a class="reference external" href="https://stats.stackexchange.com/questions/173748/what-does-goodness-of-fit-mean-in-context-of-linear-regression">https://stats.stackexchange.com/questions/173748/what-does-goodness-of-fit-mean-in-context-of-linear-regression</a></p></li>
</ul>
</section>
<section id="akaike-information-criterion">
<h3>Akaike information criterion<a class="headerlink" href="#akaike-information-criterion" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="evaluation-metrics-classification">
<h2>Evaluation Metrics - Classification<a class="headerlink" href="#evaluation-metrics-classification" title="Permalink to this headline">#</a></h2>
<section id="classification-threshold">
<h3>Classification Threshold<a class="headerlink" href="#classification-threshold" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>to map a logistic regression value to a binary category, a classification threshold (also called the decision threshold) is defined</p></li>
<li><p>assuming that the classification threshold should always be 0.5 is not true and needs to be tuned</p>
<ul>
<li><p>AUC helps us here</p></li>
</ul>
</li>
<li><p>“Tuning” a threshold for logistic regression is different from tuning hyperparameters such as learning rate</p></li>
</ul>
<img src="./images/precisionRecallTugOfWar_ClassificationThreshold.png" width=500 height=500>
</section>
<section id="confusion-matrix">
<h3>Confusion Matrix<a class="headerlink" href="#confusion-matrix" title="Permalink to this headline">#</a></h3>
<img src="https://miro.medium.com/max/1220/1*ZRU18eG-F5Sjtcph9ru8Og.png" width=500 height=500> 
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>.</p></th>
<th class="head"><p>.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img src="https://miro.medium.com/max/658/1*GJLE_YxjlSA5o_TJ9B_h5A.png" width=500 height=300></p></td>
<td><p><img src="https://miro.medium.com/max/700/1*KPXQ6VBc1foXXWgkn1q4BA.png" width=500 height=300></p></td>
</tr>
</tbody>
</table>
</section>
<section id="precision-positive-predictive-value-ppv">
<h3>Precision/Positive Predictive Value(PPV)<a class="headerlink" href="#precision-positive-predictive-value-ppv" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[\text{Precision} = \frac{t_{p}}{t_{p} + f_{p}}\]</div>
</section>
<section id="sensitivity-recall-hit-rate-tpr">
<h3>Sensitivity/Recall/Hit Rate/TPR<a class="headerlink" href="#sensitivity-recall-hit-rate-tpr" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>This tells us what percentage of people <strong>with</strong> heart disease were actually correctly identified</p></li>
<li><p><strong>Sensitivity/Recall</strong> is same as <strong>True Positive Rate(TPR)</strong>
$<span class="math notranslate nohighlight">\(\text{True Positive Rate(TPR) = Sensitivity = Recall} = \frac{t_{p}}{t_{p} + f_{n}}\)</span>$</p></li>
</ul>
</section>
<section id="specificity-selectivity-tnr">
<h3>Specificity/Selectivity/TNR<a class="headerlink" href="#specificity-selectivity-tnr" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>This tells us what percentage of people <strong>without</strong> heart disease were actually correctly identified</p></li>
<li><p>Specificity is same as <strong>(1 - False Positive Rate)</strong>
$<span class="math notranslate nohighlight">\(\text{Specificity} = \frac{t_{n}}{t_{n} + f_{p}}\)</span>$</p></li>
</ul>
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png" width=300 height=300>
</section>
<section id="accuracy">
<h3>Accuracy<a class="headerlink" href="#accuracy" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[\text{Accuracy} = \frac{TP + TN}{P + N}\]</div>
<ul class="simple">
<li><p>Accuracy is a poor or misleading metric</p>
<ul>
<li><p>Typical case includes class imbalance, when positives or negatives are extremely rare</p></li>
</ul>
</li>
</ul>
</section>
<section id="f1">
<h3>F1<a class="headerlink" href="#f1" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>F1 is an overall measure of a model’s accuracy that combines precision and recall</p></li>
<li><p>A good F1 score means that you have low false positives and low false negatives, so you’re correctly identifying real threats and you are not disturbed by false alarms.</p></li>
<li><p>An F1 score is considered perfect when it’s 1, while the model is a total failure when it’s 0.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{F1} = \frac{2 * \text{Precision} * \text{Recall}}{\text{Precision} + \text{Recall}}\]</div>
</section>
<section id="how-to-identify-the-correct-threshold">
<h3>How to identify the correct threshold<a class="headerlink" href="#how-to-identify-the-correct-threshold" title="Permalink to this headline">#</a></h3>
<p><strong>Lower the threshold</strong></p>
<ul class="simple">
<li><p>increase in number of false positive</p></li>
<li><p>decrease in number of false negative</p></li>
</ul>
<p><strong>Recalculate the confusion matrix</strong></p>
<ul class="simple">
<li><p>increase in False positive rate</p></li>
<li><p>decrease in True positive rate</p></li>
</ul>
<img src="https://miro.medium.com/max/700/1*FdWxUcsx-sBvAvpn5sTtzg.png" width=500 height=500>
<p><strong>Raise the threshold</strong></p>
<ul class="simple">
<li><p>decrease in number of false positive</p></li>
<li><p>increase in number of false negative</p></li>
</ul>
<p><strong>Recalculate the confusion matrix</strong></p>
<ul class="simple">
<li><p>decrease in False positive rate</p></li>
<li><p>increase in True positive rate</p></li>
</ul>
<img src="https://miro.medium.com/max/700/1*6EglCqnoHkVdux5hbElQ7w.png" width=500 height=500>
</section>
<section id="roc-curve">
<h3>ROC curve<a class="headerlink" href="#roc-curve" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Receiver Operator Characteristic Curve (ROC)</p>
<ul>
<li><p>helps in deciding the best threshold value</p></li>
<li><p>plot true positive rate(y-axis)(Sensitivity) against false positive rate(x-axis)(1-Specificity)</p></li>
<li><p>summarizes the confusion matrix for each threshold without having to calculate</p></li>
</ul>
</li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>.</p></th>
<th class="head"><p>.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img src="https://miro.medium.com/max/700/1*HTVotuY5L-cZjXGAW2A6_A.png" width=500 height=500></p></td>
<td><p><img src="https://miro.medium.com/max/700/1*s4tdBI_DQ7xxMmUQUu5wqw.png" width=500 height=500></p></td>
</tr>
<tr class="row-odd"><td><p><img src="https://miro.medium.com/max/693/1*6CxyxXxn9KhX_u-ZqxjbAA.png" width=500 height=500></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><img src="https://miro.medium.com/max/700/1*5-1RHdhFO4TniI-U7-nKeA.png" width=500 height=500></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<img src="https://miro.medium.com/max/700/1*-78-LvCmqxPyAmBIGJaoLw.png" width=500 height=500>
</section>
<section id="roc-curve-for-scenarios">
<h3>ROC curve for scenarios<a class="headerlink" href="#roc-curve-for-scenarios" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>Best-case ROC curve</p></th>
<th class="text-align:center head"><p>ROC curve with no predictive power</p></th>
<th class="text-align:center head"><p>Worst-case ROC curve</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>AUC = 1</p></td>
<td class="text-align:center"><p>AUC = 0.5</p></td>
<td class="text-align:center"><p>AUC = 0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p><img src="https://www.graphpad.com/guides/prism/latest/curve-fitting/images/hmfile_hash_a3639962.png" width=300 height=300></p></td>
<td class="text-align:center"><p><img src="https://www.graphpad.com/guides/prism/latest/curve-fitting/images/hmfile_hash_f3d108a2.png" width=300 height=300></p></td>
<td class="text-align:center"><p><img src="https://www.graphpad.com/guides/prism/latest/curve-fitting/images/hmfile_hash_90cc150b.png" width=300 height=300></p></td>
</tr>
</tbody>
</table>
</section>
<section id="area-under-the-curve-auc">
<h3>Area under the curve (AUC)<a class="headerlink" href="#area-under-the-curve-auc" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><strong>Intuition:</strong> gives an aggregate measure of performance aggregated across all possible classification thresholds</p></li>
<li><p>AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).</p></li>
<li><p>gives the rate of successful classification by logistic model</p></li>
<li><p>higher the AUC, better the model is at predicting classes</p></li>
<li><p>helps compare the ROC curve of different models</p></li>
<li><p>is the probability that the classifier will be able to tell which one is which</p></li>
<li><p>AUC = 1, implies an excellent model, good measure of separability</p></li>
<li><p>AUC = 0, implies a poor model, worst measure of separability</p></li>
<li><p>AUC = 0.5, implies an no class separation capacity, a random classifier</p></li>
</ul>
<p>AUC is desirable for the following two reasons:</p>
<ul class="simple">
<li><p>AUC is scale-invariant.</p>
<ul>
<li><p>It measures how well predictions are ranked, rather than their absolute values.</p></li>
<li><p>AUC is based on the relative predictions, so any transformation of the predictions that preserves the relative ranking has no effect on AUC</p></li>
</ul>
</li>
<li><p>AUC is classification-threshold-invariant.</p>
<ul>
<li><p>multiplying all of the predictions from a given model by 2.0 - No impact on AUC</p>
<ul>
<li><p>It measures the quality of the model’s predictions irrespective of what classification threshold is chosen.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>.</p></th>
<th class="head"><p>.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img src="https://www.ismiletechnologies.com/wp-content/uploads/2021/10/image-9.png" width=300 height=300></p></td>
<td><p><img src="https://miro.medium.com/max/700/1*U3KzhfUE4FLy4CL_U-ygpg.png" width=500 height=500></p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>.</p></th>
<th class="head"><p>.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img src="https://miro.medium.com/max/1056/1*Uu-t4pOotRQFoyrfqEvIEg.png" width=300 height=300></p></td>
<td><p><img src="https://miro.medium.com/max/1014/1*yF8hvKR9eNfqqej2JnVKzg.png" width=300 height=300></p></td>
</tr>
<tr class="row-odd"><td><p><img src="https://miro.medium.com/max/860/1*iLW_BrJZRI0UZSflfMrmZQ.png" width=300 height=300></p></td>
<td><p><img src="https://miro.medium.com/max/1112/1*aUZ7H-Lw74KSucoLlj1pgw.png" width=300 height=300></p></td>
</tr>
</tbody>
</table>
</section>
<section id="diagnostic-testing-diagram">
<h3>Diagnostic Testing Diagram<a class="headerlink" href="#diagnostic-testing-diagram" title="Permalink to this headline">#</a></h3>
<img src="./images/confusionMatrixTests3.png">
</section>
<section id="gini-coefficient">
<h3>Gini Coefficient<a class="headerlink" href="#gini-coefficient" title="Permalink to this headline">#</a></h3>
</section>
<section id="log-loss-or-cross-entropy-loss">
<h3>Log Loss or Cross Entropy Loss<a class="headerlink" href="#log-loss-or-cross-entropy-loss" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The loss function for linear regression is squared loss.</p></li>
<li><p>The loss function for logistic regression is Log Loss, which is defined as follows</p></li>
</ul>
<img src="./images/logisticRegressionCostFun.png" width=500 height=500>
<ul class="simple">
<li><p>The cost function is defined as the sum across all the training examples as:</p></li>
</ul>
<img src="https://miro.medium.com/max/1400/1*s5AmzAfKxh06ymdw1zkNkA.png" width=500 height=500>
<p><span class="math notranslate nohighlight">\(\tiny{\text{towardsdatascience.com - Parul Pandey}}\)</span><br />
<span class="math notranslate nohighlight">\(\tiny{\text{towardsdatascience.com - Sarang Narkhede}}\)</span><br />
<span class="math notranslate nohighlight">\(\tiny{\text{towardsdatascience.com - Shuyu Luo}}\)</span></p>
</section>
<section id="prediction-bias">
<h3>Prediction Bias<a class="headerlink" href="#prediction-bias" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>quantity that measures how far apart those two averages are.</p></li>
</ul>
<blockquote>
<div><p>Prediction Bias = Average of Predictions - Average of labels in dataset<br />
“average of predictions” should <span class="math notranslate nohighlight">\(\approx\)</span> “average of observations”</p>
</div></blockquote>
<p>Possible root causes of prediction bias are:</p>
<ul class="simple">
<li><p>Incomplete feature set</p></li>
<li><p>Noisy data set</p></li>
<li><p>Buggy pipeline</p></li>
<li><p>Biased training sample</p></li>
<li><p>Overly strong regularization</p></li>
</ul>
<p>To examine prediction bias</p>
<ul class="simple">
<li><p>examine the prediction bias on a “bucket” of examples.</p>
<ul>
<li><p>Linearly breaking up the target predictions.</p></li>
<li><p>Forming quantiles</p></li>
</ul>
</li>
<li><p>Plot Prediction bias curve</p>
<ul>
<li><p>prediction vs label</p></li>
<li><p>some subsets of the data set are noisier than others</p></li>
</ul>
</li>
</ul>
</section>
<section id="optimal-predicted-probability-cutoff">
<h3>Optimal Predicted Probability Cutoff<a class="headerlink" href="#optimal-predicted-probability-cutoff" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>maximum vertical distance between ROC curve and diagonal line</p></li>
<li><p>called as Youden’s J index</p></li>
<li><p>maximize the difference between True Positive and False Positive</p></li>
</ul>
</section>
</section>
<section id="generative-models">
<h2>Generative Models<a class="headerlink" href="#generative-models" title="Permalink to this headline">#</a></h2>
</section>
<section id="discriminative-models">
<h2>Discriminative Models<a class="headerlink" href="#discriminative-models" title="Permalink to this headline">#</a></h2>
</section>
<section id="types-of-classifiers">
<h2>Types of classifiers<a class="headerlink" href="#types-of-classifiers" title="Permalink to this headline">#</a></h2>
</section>
<section id="estimation-methods">
<h2>Estimation methods<a class="headerlink" href="#estimation-methods" title="Permalink to this headline">#</a></h2>
<p><a class="reference external" href="https://towardsdatascience.com/essential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0">Estimation Techniques</a></p>
<section id="maximum-likelihood-ml-estimation">
<h3>Maximum Likelihood (ML) Estimation<a class="headerlink" href="#maximum-likelihood-ml-estimation" title="Permalink to this headline">#</a></h3>
<p>A maximum likelihood estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is the solution of maximization problem. <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is the parameter that maximizes the likelihood of the sample <span class="math notranslate nohighlight">\(\zeta\)</span></p>
<div class="math notranslate nohighlight">
\[\hat{\theta} = arg \max\limits_{\theta}L(\theta; \zeta)\]</div>
<section id="iterative-procedures">
<h4>Iterative Procedures<a class="headerlink" href="#iterative-procedures" title="Permalink to this headline">#</a></h4>
<p>To calculate the likelihood equations, which generally cannot be solved explicitly for an estimator <span class="math notranslate nohighlight">\(\hat{\theta} = \hat{\theta}(y)\)</span>, except for special cases.
<span class="math notranslate nohighlight">\(\frac{\partial L(\theta; y)}{\partial \theta} = 0\)</span></p>
<p>It is solved iteratively, starting iteratively from an initial guess of <span class="math notranslate nohighlight">\(\theta\)</span>, seeking a convergent sequence
<span class="math notranslate nohighlight">\(\hat{\theta}_{r+1} = \hat{\theta}_{r} + \eta_{r}d_{r}(\hat{\theta})\)</span>, where <span class="math notranslate nohighlight">\(d_{r}(\hat{\theta})\)</span> indicates the descent direection of <span class="math notranslate nohighlight">\(r^{th}\)</span> step and <span class="math notranslate nohighlight">\(\eta_{r}\)</span> is the learning rate</p>
<p>Few of the iterative methods are:</p>
<ul class="simple">
<li><p>Gradient descent method</p></li>
<li><p>Newton-Raphson method</p></li>
</ul>
</section>
</section>
<section id="maximum-a-posteriori-map-estimation">
<h3>Maximum a Posteriori (MAP) Estimation<a class="headerlink" href="#maximum-a-posteriori-map-estimation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>It is an estimate of posterior distribution</p></li>
<li><p>It is seen as a regularization of MLE</p></li>
<li><p>It tends to look like MLE asymptotically as <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span></p>
<ul>
<li><p><strong><em>Prior probability</em></strong> represents what is originally believed before new evidence is introduced</p></li>
<li><p><strong><em>Posterior probability</em></strong> takes this new information into account.</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\theta_{MAP} = arg \max\limits_{\theta}L(\theta; D)\]</div>
<div class="math notranslate nohighlight">
\[\theta_{MLE} = arg \max\limits_{\theta}L(D; \theta)\]</div>
<div class="math notranslate nohighlight">
\[ \text{Posterior} \propto \text{Likelihood  x  Prior} \]</div>
</section>
<section id="minimum-mean-square-error-mmse-estimation">
<h3>Minimum Mean Square Error (MMSE) Estimation<a class="headerlink" href="#minimum-mean-square-error-mmse-estimation" title="Permalink to this headline">#</a></h3>
</section>
<section id="expectation-maximization-em-algorithm">
<h3>Expectation-Maximization(EM) algorithm<a class="headerlink" href="#expectation-maximization-em-algorithm" title="Permalink to this headline">#</a></h3>
<p>The EM algorithm helps us find the MLE for model parameters when the data has missing data points or unobserved (hidden) latent variables. It is an iterative method to approximate maximum likelihood function.</p>
<section id="gaussian-density">
<h4>Gaussian Density<a class="headerlink" href="#gaussian-density" title="Permalink to this headline">#</a></h4>
</section>
<section id="mle-vs-em">
<h4>MLE vs EM<a class="headerlink" href="#mle-vs-em" title="Permalink to this headline">#</a></h4>
</section>
<section id="applications-of-em">
<h4>Applications of EM<a class="headerlink" href="#applications-of-em" title="Permalink to this headline">#</a></h4>
</section>
</section>
<section id="least-square-ls-estimation">
<h3>Least Square (LS) Estimation<a class="headerlink" href="#least-square-ls-estimation" title="Permalink to this headline">#</a></h3>
<p>The best fit estimation line in least-square minimizes the sum of squared residuals</p>
<ul class="simple">
<li><p>it has a closed form solution</p></li>
</ul>
</section>
<section id="linear-least-squares">
<h3>Linear Least squares<a class="headerlink" href="#linear-least-squares" title="Permalink to this headline">#</a></h3>
<blockquote>
<div><p>Types of Linear least squares formulation:</p>
<ul class="simple">
<li><p>Ordinary Least squares (OLS)</p>
<ul>
<li><p>The OLS method minimizes the sum of squared residuals, and leads to a closed-form expression for the estimated value of the unknown parameter vector</p></li>
<li><p>it has the assumption that the error terms have finite variance and are homoscedastic</p></li>
</ul>
</li>
<li><p>Weighted Least squares (WLS)</p>
<ul>
<li><p>it has the assumption that the error terms are heteroscedasticity</p></li>
</ul>
</li>
<li><p>Generalized Least squares (GLS)</p>
<ul>
<li><p>it has the assumption that the error terms are either heteroscedasticity or correlations or both are present among the error terms of the model</p></li>
</ul>
</li>
</ul>
</div></blockquote>
</section>
<section id="non-linear">
<h3>Non-linear<a class="headerlink" href="#non-linear" title="Permalink to this headline">#</a></h3>
<p>The nonlinear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases.</p>
</section>
<section id="ordinary">
<h3>Ordinary<a class="headerlink" href="#ordinary" title="Permalink to this headline">#</a></h3>
</section>
<section id="weighted">
<h3>Weighted<a class="headerlink" href="#weighted" title="Permalink to this headline">#</a></h3>
</section>
<section id="generalized">
<h3>Generalized<a class="headerlink" href="#generalized" title="Permalink to this headline">#</a></h3>
</section>
<section id="partial">
<h3>Partial<a class="headerlink" href="#partial" title="Permalink to this headline">#</a></h3>
</section>
<section id="total">
<h3>Total<a class="headerlink" href="#total" title="Permalink to this headline">#</a></h3>
</section>
<section id="non-negative">
<h3>Non-negative<a class="headerlink" href="#non-negative" title="Permalink to this headline">#</a></h3>
</section>
<section id="ridge-regression">
<h3>Ridge regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">#</a></h3>
</section>
<section id="regularized">
<h3>Regularized<a class="headerlink" href="#regularized" title="Permalink to this headline">#</a></h3>
</section>
<section id="least-absolute-deviations">
<h3>Least absolute deviations<a class="headerlink" href="#least-absolute-deviations" title="Permalink to this headline">#</a></h3>
</section>
<section id="iteratively-reweighted">
<h3>Iteratively reweighted<a class="headerlink" href="#iteratively-reweighted" title="Permalink to this headline">#</a></h3>
</section>
<section id="bayesian">
<h3>Bayesian<a class="headerlink" href="#bayesian" title="Permalink to this headline">#</a></h3>
</section>
<section id="bayesian-multivariate">
<h3>Bayesian multivariate<a class="headerlink" href="#bayesian-multivariate" title="Permalink to this headline">#</a></h3>
</section>
<section id="bayes-estimator">
<h3>Bayes Estimator<a class="headerlink" href="#bayes-estimator" title="Permalink to this headline">#</a></h3>
</section>
<section id="probit">
<h3>Probit<a class="headerlink" href="#probit" title="Permalink to this headline">#</a></h3>
</section>
<section id="logit">
<h3>Logit<a class="headerlink" href="#logit" title="Permalink to this headline">#</a></h3>
</section>
<section id="kernel-density-estimation-kde">
<h3>Kernel density estimation (KDE)<a class="headerlink" href="#kernel-density-estimation-kde" title="Permalink to this headline">#</a></h3>
<p>KDE is a non-parametric way to estimate the probability density function of a random variable</p>
</section>
</section>
<section id="frequentists-vs-bayesian-approach">
<h2>Frequentists vs Bayesian Approach<a class="headerlink" href="#frequentists-vs-bayesian-approach" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Frequentists define probability as a relative frequency of an event in the long run</p></li>
<li><p>Bayesians define probability as a measure of uncertainty and belief for any event.</p></li>
</ul>
<blockquote>
<div><p>TODO: READ</p>
</div></blockquote>
</section>
<section id="parametric-vs-nonparametric-method">
<h2>Parametric vs NonParametric Method<a class="headerlink" href="#parametric-vs-nonparametric-method" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Parametric methods makes assumption in regards to the form of the function <span class="math notranslate nohighlight">\(f(X) = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + ... + \beta_{p}X_{p}\)</span>, where <span class="math notranslate nohighlight">\(f(X)\)</span> is the unknown function to be estimated, <span class="math notranslate nohighlight">\(\beta\)</span> are the coefficients to learn, <span class="math notranslate nohighlight">\(X\)</span>’s are the corresponding inputs and <span class="math notranslate nohighlight">\(p\)</span> is the number of independent variables.</p>
<ul>
<li><p>These assumptions may or may not be correct</p></li>
<li><p>these methods are quite fast</p></li>
<li><p>they require significantly less data</p></li>
<li><p>they are more interpretable</p></li>
<li><p>Examples</p>
<ul>
<li><p>Linear Regression</p></li>
<li><p>Naive Bayes</p></li>
<li><p>Perceptron</p></li>
</ul>
</li>
</ul>
</li>
<li><p>NonParametric methods donot make any underlying assumption wrt to the form of the function to be estimated.</p>
<ul>
<li><p>they tend to be more accurate</p></li>
<li><p>they require lots of data</p></li>
<li><p>Examples:</p>
<ul>
<li><p>Support Vector Machines</p></li>
<li><p>K-Nearest Neighbors</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="prediction-vs-inference">
<h2>Prediction vs Inference<a class="headerlink" href="#prediction-vs-inference" title="Permalink to this headline">#</a></h2>
</section>
<section id="mathematical-functions">
<h2>Mathematical functions<a class="headerlink" href="#mathematical-functions" title="Permalink to this headline">#</a></h2>
<section id="logistic-sigmoid-function">
<h3>Logistic/Sigmoid function<a class="headerlink" href="#logistic-sigmoid-function" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[ f(x) = \frac{L}{1+e^{-k(x-x_{0})}} \]</div>
<blockquote>
<div><p>L - the curve’s maximum value<br />
<span class="math notranslate nohighlight">\(x_{0}\)</span> - the sigmoid’s midpoint<br />
k - the logistic growth rate or steepness of curve</p>
</div></blockquote>
<ul class="simple">
<li><p>they are asympotes, never reaches 0</p></li>
<li><p>output always fall between 0 and 1</p></li>
</ul>
</section>
<section id="standard-logistic-sigmoid-function">
<h3>Standard logistic sigmoid function<a class="headerlink" href="#standard-logistic-sigmoid-function" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[ \sigma(x) = \frac{1}{1+e^{-x}} \]</div>
<blockquote>
<div><p>L = 1 - the curve’s maximum value<br />
<span class="math notranslate nohighlight">\(x_{0}\)</span> = 0 - the sigmoid’s midpoint<br />
k = 1 - the logistic growth rate or steepness of curve</p>
</div></blockquote>
</section>
<section id="logit-function">
<h3>Logit function<a class="headerlink" href="#logit-function" title="Permalink to this headline">#</a></h3>
<p>The logit function is the logarithm of the odds ratio (log-odds)</p>
<div class="math notranslate nohighlight">
\[ \text{logit(p)} = \ln\frac{p}{1-p}\]</div>
</section>
<section id="probit-function">
<h3>Probit function<a class="headerlink" href="#probit-function" title="Permalink to this headline">#</a></h3>
</section>
<section id="softplus-function">
<h3>Softplus function<a class="headerlink" href="#softplus-function" title="Permalink to this headline">#</a></h3>
</section>
<section id="weighted-sum">
<h3>Weighted sum<a class="headerlink" href="#weighted-sum" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[ \theta^{T}x = \sum\limits_{i=1}^{n}\theta_{i}x_{i} = \theta_{1}x_{1} + \theta_{2}x_{2} + ... + \theta_{n}x_{n} \]</div>
</section>
<section id="likelihood-function">
<h3>Likelihood function<a class="headerlink" href="#likelihood-function" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The likelihood talks about the optimal value of mean or standard deviation or parameters for a distribution given a bunch of observed measurements.</p></li>
<li><p>The likelihood function defines the joint probability of observed data as a function of parameters of the model.</p></li>
<li><p>It gives us a probabilistic prediction to the observed data.</p></li>
<li><p>In terms of hypothesis testing, the likelihood function gives us the probability of varying outcomes given a set of parameters defined in the null hypothesis as <span class="math notranslate nohighlight">\(f(\theta \mid x)\)</span> is to inference (finding the likely parameters given a specific outcome).</p></li>
</ul>
<div class="math notranslate nohighlight">
\[L(\theta | D) = P(D | \theta) = \prod\limits_{i}P(x_{i}| \theta) \]</div>
<img src="./images/mle_vs_prob.png" width=400 height=300>
<ul class="simple">
<li><p>The way of expressing how good a model is using the likelihood of the data given the model, which can be expressed as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[L(\theta) = \prod\limits_{i}f(x_{i}|\theta)\]</div>
<ul class="simple">
<li><p>It contains the product of probability densities of all of the samples, an aggregate measure of how probable the dataset is given the choice of specific model. The best model is expressed as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\theta^{*} = \text{argmax} L(\theta) = \text{argmax} \prod\limits_{i} f(x_{i}|\theta)\]</div>
</section>
<section id="log-likelihood-function">
<h3>Log-Likelihood function<a class="headerlink" href="#log-likelihood-function" title="Permalink to this headline">#</a></h3>
<p>The log likelihood for logistic regression is given by</p>
<div class="math notranslate nohighlight">
\[ LL(\theta) = - \sum\limits_{i=0}^{n}\left[y^{(i)}\log\sigma(\theta^{T}x^{(i)}) + (1-y^{(i)}) \log[1-\sigma(\theta^{T}x^{(i)})]\right]  \]</div>
</section>
<section id="gradient-of-log-likelihood-function">
<h3>Gradient of log likelihood function<a class="headerlink" href="#gradient-of-log-likelihood-function" title="Permalink to this headline">#</a></h3>
<p>It tries to choose values of <span class="math notranslate nohighlight">\(\theta\)</span> that maximizes the function</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial LL(\theta)}{\partial \theta_{j}} = \sum\limits_{i=0}^{n}\left[ y^{(i)} - \sigma(\theta^{T}x^{(i)}) \right]x_{j}^{(i)} \]</div>
</section>
<section id="negative-log-likelihood-function">
<h3>Negative log likelihood function<a class="headerlink" href="#negative-log-likelihood-function" title="Permalink to this headline">#</a></h3>
</section>
<section id="likelihood-vs-log-likelihood-function">
<h3>Likelihood vs Log-Likelihood function<a class="headerlink" href="#likelihood-vs-log-likelihood-function" title="Permalink to this headline">#</a></h3>
<img src="./images/GMM_Likelihood.png"></section>
</section>
<section id="additive-models">
<h2>Additive Models<a class="headerlink" href="#additive-models" title="Permalink to this headline">#</a></h2>
</section>
<section id="machine-learning-vs-rule-based-systems">
<h2>Machine Learning vs Rule-based Systems<a class="headerlink" href="#machine-learning-vs-rule-based-systems" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>rule-based systems</p>
<ul>
<li><p>humans manually program rule-based systems</p></li>
<li><p>rule based system explicitly need to be told what to do by humans</p></li>
</ul>
</li>
<li><p>self-learning systems</p>
<ul>
<li><p>machines automatically train self-learning systems</p></li>
<li><p>self-learning systems learn from experience</p></li>
</ul>
</li>
</ul>
</section>
<section id="model-selection-validation">
<h2>Model Selection/Validation<a class="headerlink" href="#model-selection-validation" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>typical split - train(50%), validate(25%), test(25%)</p></li>
</ul>
<p><strong>How to choose one among a set of candidate models</strong></p>
<ul class="simple">
<li><p>analytical - probabilistic statistical measure</p>
<ul>
<li><p>benefit</p>
<ul>
<li><p>test dataset is not required</p></li>
<li><p>entire dataset is used to fit the model</p></li>
<li><p>final model based on score prediction</p></li>
</ul>
</li>
<li><p>limitation</p>
<ul>
<li><p>cannot apply to compare different models</p></li>
<li><p>metric derived for each model is different</p></li>
<li><p>do not take inaccount uncertainty of the model, favour simple models</p></li>
</ul>
</li>
<li><p>types</p>
<ul>
<li><p>AIC - Akaike Information Criterion</p>
<ul>
<li><p>scoring based on its log-likelihood and complexity</p></li>
<li><p>derived from frequentist probability</p></li>
<li><p>AIC deals with trade-off between goodness of fit of model and  simplicity of the model</p></li>
<li><p>AIC deals with both the risk of overfitting and the risk of underfitting</p></li>
<li><p>AIC estimates the relative amount of information lost by a given model: the <strong>less information a model loses, the higher the quality of that model</strong></p></li>
</ul>
</li>
<li><p>BIC - Bayesian Information Criterion</p>
<ul>
<li><p>scoring based on its log-likelihood and complexity</p></li>
<li><p>derived from Bayesian probability</p></li>
<li><p>the penalty is different from AIC</p></li>
</ul>
</li>
<li><p>MDL - Minimum Description Length</p>
<ul>
<li><p>comes from information theory</p></li>
</ul>
</li>
<li><p>SRM</p></li>
</ul>
</li>
</ul>
</li>
<li><p>sampling method</p>
<ul>
<li><p>types</p>
<ul>
<li><p>cross validation</p>
<ul>
<li><p>LOOCV - Leave-One-Out CV</p></li>
<li><p>Stratified</p></li>
<li><p>Repeated</p></li>
<li><p>Nested</p></li>
</ul>
</li>
<li><p>bootstrap</p></li>
<li><p>jackknife</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[AIC = - 2\frac{LL}{N} + 2\frac{d}{N} \]</div>
<ul class="simple">
<li><p>where</p>
<ul>
<li><p>N - number of examples in training dataset</p></li>
<li><p>LL - log-likelihood of the model on the training dataset</p></li>
<li><p>d - number of parameters in the model</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[BIC = -2 LL + d\ln{N} \]</div>
<ul class="simple">
<li><p>hold out</p>
<ul>
<li><p>split data into train and test set</p></li>
<li><p>dependent on how the data is split</p></li>
</ul>
</li>
<li><p>cross validation or k-fold</p>
<ul>
<li><p>dataset is randomly split into k-groups</p></li>
<li><p>one is used as test set, rest as training set</p></li>
<li><p>gives model opportunity to train on multiple train-test split</p></li>
<li><p>requirees more computational power and time</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://robjhyndman.com/hyndsight/crossvalidation/">https://robjhyndman.com/hyndsight/crossvalidation/</a></p></li>
<li><p><a class="reference external" href="https://robjhyndman.com/hyndsight/aic/">https://robjhyndman.com/hyndsight/aic/</a></p></li>
</ul>
</section>
<section id="activation-functions">
<h2>Activation Functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">#</a></h2>
<section id="sigmoid-function">
<h3>Sigmoid function<a class="headerlink" href="#sigmoid-function" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Features</p>
<ul>
<li><p>Sigmoid function gives an S-shaped curve</p></li>
<li><p>used to map predicted values to probabilities</p></li>
<li><p>function maps any real value into another value between 0 and 1</p></li>
</ul>
</li>
<li><p>Function</p>
<ul>
<li><p>Equation:</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[ f(x) = s = \frac{1}{1+e^{-x}}\]</div>
<ul class="simple">
<li><p>Derivative:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ f’(x) = s*(1-s) \]</div>
<ul class="simple">
<li><p>Range: (0,1)</p></li>
<li><p>Adv</p>
<ul>
<li><p>function is differentiable. can find the slope of the sigmoid curve at any two points</p></li>
<li><p>Output values bound between 0 and 1, which normalizes output of each neuron</p></li>
</ul>
</li>
<li><p>DisAdv</p>
<ul>
<li><p>Vanishing gradient — for very high or very low values of X, there is almost no change to the prediction</p></li>
<li><p>due to vanishing gradient problem, sigmoids have slow convergence</p></li>
<li><p>outputs are not zero centered</p></li>
<li><p>computationally expensive</p></li>
</ul>
</li>
</ul>
</section>
<section id="softmax">
<h3>Softmax<a class="headerlink" href="#softmax" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Features</p>
<ul>
<li><p>calculates probability distribution of an event over ’n’ different events</p></li>
</ul>
</li>
<li><p>Function</p>
<ul>
<li><p>Equation:</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[ f(x) = \frac{e^{x_{i}}}{\sum_{j=1}^{n} e^{x_{i}}} \]</div>
<ul class="simple">
<li><p>Probabilistic interpretation:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ S_{j} = P(y=j|x) \]</div>
<ul class="simple">
<li><p>Range: (0, 1)</p></li>
<li><p>Adv</p>
<ul>
<li><p>Able to handle multiple classes only one class in other activation functions</p>
<ul>
<li><p>normalizes the outputs for each class between 0 and 1</p></li>
</ul>
</li>
<li><p>useful for output neurons to classify inputs as probability of multiple categories.</p></li>
</ul>
</li>
</ul>
</section>
<section id="tanh-function">
<h3>tanh function<a class="headerlink" href="#tanh-function" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Function</p>
<ul>
<li><p>Equation:</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[ f(x) = a = \tanh(x) =\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \]</div>
<ul class="simple">
<li><p>Derivative:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[f’(x) = (1- a^{2})\]</div>
<ul class="simple">
<li><p>Range: (-1, 1)</p></li>
<li><p>Adv</p>
<ul>
<li><p>zero centered</p>
<ul>
<li><p>good for model that have strongly negative, neutral, and strongly positive values</p></li>
</ul>
</li>
<li><p>function and its derivative both are monotonic</p></li>
<li><p>works better than sigmoid function</p></li>
</ul>
</li>
<li><p>DisAdv</p>
<ul>
<li><p>vanishing gradient problem and hence slow convergence</p></li>
</ul>
</li>
</ul>
</section>
<section id="relu-rectified-linear-unit-function">
<h3>ReLU (REctified Linear Unit) function<a class="headerlink" href="#relu-rectified-linear-unit-function" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Features</p>
<ul>
<li><p>a simple non-linear function that chops off at non-zero</p></li>
<li><p>the function is zero for negative values and it grows linearly for positive values.</p></li>
<li><p>is very simple to implement (generally, three instructions are enough), while the sigmoid is a few orders of magnitude more.</p>
<ul>
<li><p>This helped to squeeze the neural networks onto an early GPU</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Function</p>
<ul>
<li><p>Equation:</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[ f(x) = \max(x, 0) \]</div>
<ul class="simple">
<li><p>Derivative:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ f'(x) = 1 \text{ if } x \gt 0 \text{ otherwise } 0 \]</div>
<ul class="simple">
<li><p>Range:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[[0, +\infty)\]</div>
<ul class="simple">
<li><p>Adv</p>
<ul>
<li><p>Computationally efficient</p>
<ul>
<li><p>allows the network to converge very quickly</p></li>
</ul>
</li>
<li><p>Non-linear</p>
<ul>
<li><p>although it looks like a linear function, ReLU has a derivative function and allows for back-propagation</p></li>
</ul>
</li>
</ul>
</li>
<li><p>DisAdv</p>
<ul>
<li><p>Dying ReLU problem</p>
<ul>
<li><p>when inputs approach zero, or are negative, gradient of function becomes zero, network cannot perform back-propagation and cannot learn</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="elu">
<h3>ELU<a class="headerlink" href="#elu" title="Permalink to this headline">#</a></h3>
</section>
<section id="leakyrelu">
<h3>LeakyReLU<a class="headerlink" href="#leakyrelu" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Features</p>
<ul>
<li><p>variant of ReLU</p></li>
</ul>
</li>
<li><p>Function</p>
<ul>
<li><p>Equation:</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[f(x)= a = \max(0.01x, x)\]</div>
<ul class="simple">
<li><p>Derivative:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ f'(x) = 1 \text{ if } x \gt 0 \text{ otherwise } 0.01 \]</div>
<ul class="simple">
<li><p>Range: (0.01, +<span class="math notranslate nohighlight">\(\infty\)</span>)</p></li>
<li><p>Adv</p>
<ul>
<li><p>Prevents dying ReLU problem</p>
<ul>
<li><p>this variation of ReLU has a small positive slope in the negative area, so it does enable back-propagation, even for negative input values</p></li>
</ul>
</li>
</ul>
</li>
<li><p>DisAdv</p>
<ul>
<li><p>Results not consistent</p>
<ul>
<li><p>leaky ReLU does not provide consistent predictions for negative input values.</p></li>
<li><p>During the front propagation if the learning rate is set very high it will overshoot killing the neuron.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="feature-engineering">
<h2>Feature Engineering<a class="headerlink" href="#feature-engineering" title="Permalink to this headline">#</a></h2>
<section id="mapping-rawdata-to-features">
<h3>Mapping RawData to Features<a class="headerlink" href="#mapping-rawdata-to-features" title="Permalink to this headline">#</a></h3>
</section>
<section id="has-good-feature-property">
<h3>Has Good Feature property<a class="headerlink" href="#has-good-feature-property" title="Permalink to this headline">#</a></h3>
</section>
<section id="one-hot-encoding">
<h3>One-Hot Encoding<a class="headerlink" href="#one-hot-encoding" title="Permalink to this headline">#</a></h3>
</section>
<section id="scaling-feature-values">
<h3>Scaling Feature values<a class="headerlink" href="#scaling-feature-values" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>with mean and std deviation</p></li>
<li><p>as z-values</p></li>
</ul>
</section>
<section id="outlier-treatment">
<h3>Outlier Treatment<a class="headerlink" href="#outlier-treatment" title="Permalink to this headline">#</a></h3>
</section>
<section id="binning-by-values">
<h3>Binning by values<a class="headerlink" href="#binning-by-values" title="Permalink to this headline">#</a></h3>
</section>
<section id="binning-by-quantiles">
<h3>Binning by quantiles<a class="headerlink" href="#binning-by-quantiles" title="Permalink to this headline">#</a></h3>
</section>
<section id="scrubing">
<h3>Scrubing<a class="headerlink" href="#scrubing" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Data reliability</p></li>
</ul>
</section>
</section>
<section id="feature-crosses">
<h2>Feature Crosses<a class="headerlink" href="#feature-crosses" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>a synthetic feature formed by multiplying (crossing) two or more feature</p>
<ul>
<li><p>[A x B]: a feature cross formed by multiplying the values of two features.</p></li>
<li><p>[A x B x C x D x E]: a feature cross formed by multiplying the values of five features.</p></li>
<li><p>[A x A]: a feature cross formed by squaring a single feature.</p></li>
</ul>
</li>
<li><p>logical conjunctions</p>
<ul>
<li><p>country:usa AND language:spanish</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(x_{1}x_{2}\)</span></p></li>
</ul>
</section>
<section id="regularization-simplicity">
<h2>Regularization - Simplicity<a class="headerlink" href="#regularization-simplicity" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Not trusting your data too much</p></li>
</ul>
<section id="early-stopping">
<h3>Early stopping<a class="headerlink" href="#early-stopping" title="Permalink to this headline">#</a></h3>
</section>
<section id="penalize-model-complexity">
<h3>Penalize model complexity<a class="headerlink" href="#penalize-model-complexity" title="Permalink to this headline">#</a></h3>
</section>
<section id="id16">
<h3>Empirical Risk Minimization<a class="headerlink" href="#id16" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>aim for low training error</p></li>
</ul>
<blockquote>
<div><p>minimize: Loss(Data|Model)</p>
</div></blockquote>
</section>
<section id="structural-risk-minimization">
<h3>Structural Risk Minimization<a class="headerlink" href="#structural-risk-minimization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>aim for low training error</p></li>
<li><p>while balance against complexity</p></li>
</ul>
<blockquote>
<div><p>minimize: Loss(Data|Model) + Complexity(Model)</p>
</div></blockquote>
</section>
<section id="how-to-define-model-complexity">
<h3>How to define model complexity<a class="headerlink" href="#how-to-define-model-complexity" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>prefer smaller weights</p></li>
</ul>
</section>
<section id="loss-function-with-l2-regularization">
<h3>Loss Function with L2 Regularization<a class="headerlink" href="#loss-function-with-l2-regularization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>the sum of the squares of all the feature weights</p></li>
<li><p>this does not depend upon the data, but rely on weights</p></li>
<li><p>will encourage many of the non-informative weights to be nearly (but not exactly) 0.0.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[L_{2} = ||w||_{2}^{2} = w_{1}^{2} + w_{2}^{2} + w_{3}^{2} + w_{4}^{2}\]</div>
</section>
<section id="regularization-rate">
<h3>Regularization Rate<a class="headerlink" href="#regularization-rate" title="Permalink to this headline">#</a></h3>
<blockquote>
<div><p>minimize: Loss(Data|Model) + <span class="math notranslate nohighlight">\(\lambda\)</span> Complexity(Model)</p>
</div></blockquote>
</section>
</section>
<section id="regularization-sparsity">
<h2>Regularization - Sparsity<a class="headerlink" href="#regularization-sparsity" title="Permalink to this headline">#</a></h2>
<section id="sparse-cross-feature">
<h3>Sparse cross-feature<a class="headerlink" href="#sparse-cross-feature" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Model size will be huge</p></li>
<li><p>Noisy coefficients causing overfitting</p></li>
</ul>
</section>
<section id="explicitly-zero-out-weights">
<h3>Explicitly zero out weights<a class="headerlink" href="#explicitly-zero-out-weights" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L_{2}\)</span> regularization encourages weights to be small, but doesn’t force them to exactly 0.0</p></li>
<li><p><span class="math notranslate nohighlight">\(L_{0}\)</span> regularization</p>
<ul>
<li><p>penalize for non-zero weights</p></li>
<li><p>but this is not convex and hard to optimize</p></li>
</ul>
</li>
<li><p>Relax to <span class="math notranslate nohighlight">\(L_{1}\)</span> regularization</p>
<ul>
<li><p>Penalize sum of abs(weights)</p></li>
<li><p>Convex problem</p></li>
<li><p>Encourage sparsity unlike <span class="math notranslate nohighlight">\(L_{2}\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="l1-vs-l2-regularization">
<h3>L1 vs L2 regularization<a class="headerlink" href="#l1-vs-l2-regularization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L_{2}\)</span> and <span class="math notranslate nohighlight">\(L_{1}\)</span> penalize weights differently:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(L_{2}\)</span> penalizes <span class="math notranslate nohighlight">\(weight^{2}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(L_{1}\)</span> penalizes |weight|</p></li>
</ul>
</li>
<li><p>Both <span class="math notranslate nohighlight">\(L_{2}\)</span> and <span class="math notranslate nohighlight">\(L_{1}\)</span> have different derivatives</p>
<ul>
<li><p>derivative of <span class="math notranslate nohighlight">\(L_{2}\)</span> is 2 * weight.</p>
<ul>
<li><p>derivative of <span class="math notranslate nohighlight">\(L_{2}\)</span> is a force that removes x% of the weight every time</p></li>
</ul>
</li>
<li><p>derivative of <span class="math notranslate nohighlight">\(L_{1}\)</span> is k (a constant, whose value is independent of weight)</p>
<ul>
<li><p>derivative of <span class="math notranslate nohighlight">\(L_{1}\)</span> is a force that subtracts some constant from the weight every time</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="l1-regularization">
<h3>L1 regularization<a class="headerlink" href="#l1-regularization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>will encourage most of the non-informative weights to be exactly 0.0.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(L_{1}\)</span> regularization of sufficient lambda tends to encourage non-informative weights to become exactly 0.0.</p></li>
<li><p>By doing so, these non-informative features leave the model.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(L_{1}\)</span> regularization may cause informative features to get a weight of exactly 0.0.</p>
<ul>
<li><p><strong>Be careful</strong></p>
<ul>
<li><p>L1 regularization may cause the following kinds of features to be given weights of exactly 0:</p>
<ul>
<li><p>Weakly informative features.</p></li>
<li><p>Strongly informative features on different scales.</p></li>
<li><p>Informative features strongly correlated with other similarly informative features.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="elastic-nets">
<h3>Elastic Nets<a class="headerlink" href="#elastic-nets" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>is an extension of linear regression that adds regularization penalties to the loss function during training</p></li>
<li><p>the estimates from the elastic net method is defined as
$<span class="math notranslate nohighlight">\( \hat{\beta} = \text{argmin}_{\beta}(\| y - X\beta \|^{2} + \lambda_{2}\| \beta\|^{2} + \lambda_{1}\| \beta\|_{1}) \)</span>$</p></li>
<li><p>the benefit of elastic net is that it allows balance of both the penalties as a weighted sum
$<span class="math notranslate nohighlight">\(\lambda_{1} + \lambda_{2} = 1\)</span>$</p></li>
</ul>
</section>
</section>
<section id="neural-network">
<h2>Neural Network<a class="headerlink" href="#neural-network" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Linear combination of linear function is still linear (Left below)</p></li>
<li><p>To make NN non-linear, non-linear transformation needs to be introduced (Right below)</p></li>
<li><p>neural networks generally use non-linear activation functions</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>.</p></th>
<th class="head"><p>.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img src="./images/nn_linearNN.png" width=400></p></td>
<td><p><img style="float: right;" src="./images/nn_nonlinearNN.png" width=400></p></td>
</tr>
</tbody>
</table>
<section id="linear-regression-model-vs-nn">
<h3>Linear Regression Model vs NN<a class="headerlink" href="#linear-regression-model-vs-nn" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A neural network without an activation function is just a linear regression model.</p></li>
<li><p>Linear combination of linear function is still linear</p></li>
<li><p>To make NN non-linear, non-linear transformation needs to be introduced</p></li>
</ul>
</section>
<section id="why-use-activation-function">
<h3>Why use activation function<a class="headerlink" href="#why-use-activation-function" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>without activation function the weights and bias would simply do a linear transformation</p></li>
<li><p>A linear equation is simple to solve but is limited in its capacity to solve complex problems and have less power to learn complex functional mappings from data.</p></li>
<li><p>A neural network without an activation function is just a linear regression model.</p></li>
<li><p>neural networks use non-linear activation functions</p></li>
</ul>
</section>
<section id="types-of-activation-function">
<h3>Types of Activation Function<a class="headerlink" href="#types-of-activation-function" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Linear or Identity Activation Function</p></li>
<li><p>Non-linear Activation Functions</p></li>
</ul>
</section>
<section id="linear-activation-function">
<h3>Linear Activation function<a class="headerlink" href="#linear-activation-function" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Activation function should be differentiable</p></li>
<li><p>problem in using linear activation function</p>
<ul>
<li><p>Back-propagation is not possible</p>
<ul>
<li><p>The derivative of linear function is a constant, and has no relation to the input, X.</p></li>
<li><p>It’s not possible to go back and understand which weights in the input neurons can provide a better prediction.</p></li>
</ul>
</li>
<li><p>All layers of the neural network collapse into one</p>
<ul>
<li><p>with linear activation functions, no matter how many layers in the neural network, the last layer will be a linear function of the first layer</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="non-linear-activation-function">
<h3>Non-Linear Activation function<a class="headerlink" href="#non-linear-activation-function" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>allow model to create complex mappings between the network’s inputs and outputs</p></li>
<li><p>suited fir non-linear or high dimensionality models.</p></li>
<li><p>almost any functional computation in a neural network is possible</p></li>
<li><p>allow back-propagation because they have a derivative function which is related to the inputs.</p></li>
<li><p>allow “stacking” of multiple layers of neurons to create a deep neural network</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "reco"
        },
        kernelOptions: {
            kernelName: "reco",
            path: "./ml_examples"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'reco'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../unix/my_zshrc.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">My .zshrc script</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../codes/python_faqs.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Python FAQs</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Chandra<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>